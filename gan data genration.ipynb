{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN-to generate data for BTC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU, ReLU\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, file_name=None):\n",
    "    file_name = file_name if file_name is not None else 'gen_data'\n",
    "    #path = 'gan/gen-data/'\n",
    "    #file_name = path + file_name\n",
    "    \n",
    "    np.save(file_name, data)\n",
    "    print(\"Data successfully saved to {}\".format(file_name))\n",
    "    \n",
    "def load_data(file_name):\n",
    "    #path = 'gan/gen-data/'\n",
    "    file_name = file_name + '.npy'\n",
    "    \n",
    "    gen_data = np.load(file_name, allow_pickle=True)\n",
    "    return gen_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Numerical Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mu_x1 = 0\n",
    "sd_x1 = 1\n",
    "\n",
    "x1 = np.random.normal(mu_x1, sd_x1, 10000)\n",
    "\n",
    "print(np.mean(x1))\n",
    "print(np.std(x1))\n",
    "\n",
    "mu_x2 = 5\n",
    "sd_x2 = 2\n",
    "\n",
    "x2 = np.random.normal(mu_x2, sd_x2, 10000)\n",
    "\n",
    "print(np.mean(x2))\n",
    "print(np.std(x2))\n",
    "\n",
    "x_train = np.stack((x1, x2))\n",
    "x_train = np.transpose(x_train)\n",
    "\n",
    "# save_data(x_train, file_name='numerical-original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.45801204,  6.09506574],\n",
       "       [-0.46655784,  5.80114231],\n",
       "       [ 0.4887693 ,  5.71306665],\n",
       "       ...,\n",
       "       [-0.85748491,  6.41407976],\n",
       "       [ 1.71603195,  5.89972094],\n",
       "       [ 2.61741192,  8.90431553]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x637188ba8>,\n",
       " <matplotlib.lines.Line2D at 0x637188cf8>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN for Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANNumerical():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_shape = (5,)\n",
    "        self.noise_shape = (10,)\n",
    "\n",
    "        # Manually tune down learning rate to avoid oscillation\n",
    "        optimizer = Adam()#lr=0.0002, beta_1=0.5\n",
    "\n",
    "        # -------------\n",
    "        # Discriminator\n",
    "        # -------------\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mean_squared_error',\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "        # ---------\n",
    "        # Generator\n",
    "        # ---------\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='mean_squared_error',\n",
    "                               optimizer=optimizer)\n",
    "        # --------\n",
    "        # Combined\n",
    "        # --------\n",
    "        # The combined model is created by stacking generator and discriminator.\n",
    "        # Noise ---Generator--> Generated Data ---Discriminator--> Validity\n",
    "        \n",
    "        z = Input(shape=self.noise_shape)\n",
    "        data = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        validity = self.discriminator(data)\n",
    "\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='mean_squared_error',\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "        \n",
    "    def save_model(self, version=None):\n",
    "        if version is None:\n",
    "            self.discriminator.save('gan/models/toy-numerical-discriminatorAV.h5')\n",
    "            self.generator.save('gan/models/toy-numerical-generatorAV.h5')\n",
    "            self.combined.save('gan/models/toy-numerical-combinedAV.h5')\n",
    "        else:\n",
    "            self.discriminator.save('gan/models/toy-numerical-discriminatorAV-{}.h5'.format(version))\n",
    "            self.generator.save('gan/models/toy-numerical-generatorAV-{}.h5'.format(version))\n",
    "            self.combined.save('gan/models/toy-numerical-combinedAV-{}.h5'.format(version))\n",
    "        \n",
    "        \n",
    "    def load_model(self, version=None): \n",
    "        if version is None:\n",
    "            self.discriminator = load_model('gan/models/toy-numerical-discriminatorAV.h5')\n",
    "            self.generator = load_model('gan/models/toy-numerical-generatorAV.h5')\n",
    "            self.combined = load_model('gan/models/toy-numerical-combinedAV.h5')\n",
    "        else:\n",
    "            self.discriminator = load_model('gan/models/toy-numerical-discriminatorAV-{}.h5'.format(version))\n",
    "            self.generator = load_model('gan/models/toy-numerical-generatorAV-{}.h5'.format(version))\n",
    "            self.combined = load_model('gan/models/toy-numerical-combinedAV-{}.h5'.format(version))\n",
    "            \n",
    "        \n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(8, input_shape=self.data_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(16))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        # Discriminator takes an image as an input and outputs its validity\n",
    "        data = Input(shape=self.data_shape)\n",
    "        validity = model(data)\n",
    "\n",
    "        return Model(data, validity)\n",
    "\n",
    "        \n",
    "    def build_generator(self):\n",
    "        # BatchNormalization maintains the mean activation close to 0\n",
    "        # and the activation standard deviation close to 1\n",
    "        model = Sequential()\n",
    "        model.add(Dense(16, input_shape=self.noise_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(32))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(5))\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        # Generator takes noise as an input and outputs an image\n",
    "        noise = Input(shape=self.noise_shape)\n",
    "        data = model(noise)\n",
    "\n",
    "        return Model(noise, data)\n",
    "    \n",
    "    \n",
    "    def train(self, x_train, epochs=50000, batch_size=128, save_model_interval=5000):\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # -------------------\n",
    "            # Train Discriminator\n",
    "            # -------------------\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, x_train.shape[0], half_batch)\n",
    "            data = x_train[idx]\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            noise = np.random.normal(0, 1, (half_batch, 10))\n",
    "            gen_data = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(data, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_data, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------\n",
    "            # Train Generator\n",
    "            # ---------------\n",
    "            noise = np.random.normal(0, 1, (batch_size, 10))\n",
    "\n",
    "            # The generator wants to fool the discriminator, hence trained with valid label (1)\n",
    "            # valid_y = np.array([1] * batch_size)\n",
    "            valid_y = np.ones((batch_size, 1))\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "\n",
    "            # Print progress\n",
    "            print (\"{:5d} [D loss: {}, acc_real: {:2f}, acc_fake: {:2f}] [G loss: {}]\".format(epoch, d_loss[0], 100*d_loss_real[1], 100*d_loss_fake[1], g_loss))\n",
    "\n",
    "            with open('gan/logs/toy-numerical-gan.log', 'a') as log_file:\n",
    "                log_file.write('{},{}\\n'.format(d_loss[0], g_loss))\n",
    "            \n",
    "            \n",
    "            # Save models at save_interval\n",
    "            if epoch != 0 and epoch % save_model_interval == 0:\n",
    "                self.save_model(version=str(epoch))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data1 = pd.read_csv('data.csv')\n",
    "for i in range(data1.shape[0]):\n",
    "    print(i)\n",
    "    try:\n",
    "        data1.iloc[i,4]=float(data1.iloc[i,4].split('K')[0])*1000\n",
    "    except:\n",
    "        data1.iloc[i,4]=float(0.00)\n",
    "for i in range(4):\n",
    "    for j in range(data1.shape[0]):\n",
    "        data1.iloc[j,i] = data1.iloc[j,i].replace(',','')\n",
    "        \n",
    "data1 = data1.astype('float64')\n",
    "#save_data(data1,'datanp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_data(file_name='datanp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3630.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/anilvyas/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 48        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 209\n",
      "Trainable params: 209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,077\n",
      "Trainable params: 981\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/anilvyas/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anilvyas/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 [D loss: 0.13325126469135284, acc_real: 100.000000, acc_fake: 32.812500] [G loss: 0.22704005241394043]\n",
      "    1 [D loss: 0.14745569229125977, acc_real: 100.000000, acc_fake: 37.500000] [G loss: 0.22057689726352692]\n",
      "    2 [D loss: 0.1525687575340271, acc_real: 100.000000, acc_fake: 21.875000] [G loss: 0.22805525362491608]\n",
      "    3 [D loss: 0.14524170756340027, acc_real: 100.000000, acc_fake: 37.500000] [G loss: 0.22649890184402466]\n",
      "    4 [D loss: 0.14733801782131195, acc_real: 100.000000, acc_fake: 42.187500] [G loss: 0.23213744163513184]\n",
      "    5 [D loss: 0.14246675372123718, acc_real: 100.000000, acc_fake: 35.937500] [G loss: 0.2299073189496994]\n",
      "    6 [D loss: 0.13368502259254456, acc_real: 100.000000, acc_fake: 45.312500] [G loss: 0.23513177037239075]\n",
      "    7 [D loss: 0.13281741738319397, acc_real: 100.000000, acc_fake: 50.000000] [G loss: 0.23605553805828094]\n",
      "    8 [D loss: 0.14137092232704163, acc_real: 100.000000, acc_fake: 40.625000] [G loss: 0.24343708157539368]\n",
      "    9 [D loss: 0.1448119580745697, acc_real: 100.000000, acc_fake: 42.187500] [G loss: 0.24453704059123993]\n",
      "   10 [D loss: 0.12620410323143005, acc_real: 100.000000, acc_fake: 67.187500] [G loss: 0.24122454226016998]\n",
      "   11 [D loss: 0.12675608694553375, acc_real: 100.000000, acc_fake: 60.937500] [G loss: 0.24801677465438843]\n",
      "   12 [D loss: 0.1306813359260559, acc_real: 100.000000, acc_fake: 65.625000] [G loss: 0.24614039063453674]\n",
      "   13 [D loss: 0.1296958178281784, acc_real: 100.000000, acc_fake: 67.187500] [G loss: 0.2501869797706604]\n",
      "   14 [D loss: 0.13313964009284973, acc_real: 100.000000, acc_fake: 57.812500] [G loss: 0.24926623702049255]\n",
      "   15 [D loss: 0.12608027458190918, acc_real: 100.000000, acc_fake: 62.500000] [G loss: 0.25489965081214905]\n",
      "   16 [D loss: 0.12914445996284485, acc_real: 100.000000, acc_fake: 57.812500] [G loss: 0.25950688123703003]\n",
      "   17 [D loss: 0.12007391452789307, acc_real: 100.000000, acc_fake: 62.500000] [G loss: 0.261817067861557]\n",
      "   18 [D loss: 0.13622573018074036, acc_real: 100.000000, acc_fake: 53.125000] [G loss: 0.26950812339782715]\n",
      "   19 [D loss: 0.12533050775527954, acc_real: 100.000000, acc_fake: 59.375000] [G loss: 0.2721063494682312]\n",
      "   20 [D loss: 0.1175520271062851, acc_real: 100.000000, acc_fake: 60.937500] [G loss: 0.27706101536750793]\n",
      "   21 [D loss: 0.11885415017604828, acc_real: 100.000000, acc_fake: 70.312500] [G loss: 0.27965617179870605]\n",
      "   22 [D loss: 0.1284169703722, acc_real: 96.875000, acc_fake: 70.312500] [G loss: 0.28556376695632935]\n",
      "   23 [D loss: 0.15438085794448853, acc_real: 90.625000, acc_fake: 71.875000] [G loss: 0.291013240814209]\n",
      "   24 [D loss: 0.17256873846054077, acc_real: 87.500000, acc_fake: 71.875000] [G loss: 0.2950912117958069]\n",
      "   25 [D loss: 0.22227679193019867, acc_real: 75.000000, acc_fake: 82.812500] [G loss: 0.3027520775794983]\n",
      "   26 [D loss: 0.16155824065208435, acc_real: 89.062500, acc_fake: 75.000000] [G loss: 0.30704107880592346]\n",
      "   27 [D loss: 0.15800873935222626, acc_real: 87.500000, acc_fake: 85.937500] [G loss: 0.3145015835762024]\n",
      "   28 [D loss: 0.19361600279808044, acc_real: 81.250000, acc_fake: 79.687500] [G loss: 0.31450510025024414]\n",
      "   29 [D loss: 0.1818709522485733, acc_real: 82.812500, acc_fake: 81.250000] [G loss: 0.3174739480018616]\n",
      "   30 [D loss: 0.1404397338628769, acc_real: 92.187500, acc_fake: 81.250000] [G loss: 0.316150426864624]\n",
      "   31 [D loss: 0.15500667691230774, acc_real: 90.625000, acc_fake: 67.187500] [G loss: 0.3212110996246338]\n",
      "   32 [D loss: 0.12074851989746094, acc_real: 95.312500, acc_fake: 76.562500] [G loss: 0.32519084215164185]\n",
      "   33 [D loss: 0.09180742502212524, acc_real: 100.000000, acc_fake: 79.687500] [G loss: 0.3287007212638855]\n",
      "   34 [D loss: 0.10202407091856003, acc_real: 100.000000, acc_fake: 76.562500] [G loss: 0.32758355140686035]\n",
      "   35 [D loss: 0.10491880774497986, acc_real: 100.000000, acc_fake: 68.750000] [G loss: 0.33130037784576416]\n",
      "   36 [D loss: 0.09881381690502167, acc_real: 100.000000, acc_fake: 78.125000] [G loss: 0.330765038728714]\n",
      "   37 [D loss: 0.09293518960475922, acc_real: 100.000000, acc_fake: 79.687500] [G loss: 0.336093544960022]\n",
      "   38 [D loss: 0.09700971096754074, acc_real: 100.000000, acc_fake: 76.562500] [G loss: 0.33613693714141846]\n",
      "   39 [D loss: 0.10971923917531967, acc_real: 98.437500, acc_fake: 75.000000] [G loss: 0.34226298332214355]\n",
      "   40 [D loss: 0.09692388772964478, acc_real: 98.437500, acc_fake: 78.125000] [G loss: 0.34460917115211487]\n",
      "   41 [D loss: 0.09435179084539413, acc_real: 98.437500, acc_fake: 78.125000] [G loss: 0.3398144841194153]\n",
      "   42 [D loss: 0.094075046479702, acc_real: 100.000000, acc_fake: 82.812500] [G loss: 0.3427644968032837]\n",
      "   43 [D loss: 0.08440897613763809, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.34056127071380615]\n",
      "   44 [D loss: 0.09365183115005493, acc_real: 100.000000, acc_fake: 81.250000] [G loss: 0.3391590416431427]\n",
      "   45 [D loss: 0.08903364092111588, acc_real: 100.000000, acc_fake: 85.937500] [G loss: 0.3439614772796631]\n",
      "   46 [D loss: 0.09931308031082153, acc_real: 100.000000, acc_fake: 82.812500] [G loss: 0.3387033939361572]\n",
      "   47 [D loss: 0.10670468211174011, acc_real: 100.000000, acc_fake: 81.250000] [G loss: 0.34575188159942627]\n",
      "   48 [D loss: 0.09578054398298264, acc_real: 100.000000, acc_fake: 84.375000] [G loss: 0.34379664063453674]\n",
      "   49 [D loss: 0.08728717267513275, acc_real: 100.000000, acc_fake: 89.062500] [G loss: 0.34149378538131714]\n",
      "   50 [D loss: 0.09553852677345276, acc_real: 100.000000, acc_fake: 85.937500] [G loss: 0.3435038924217224]\n",
      "   51 [D loss: 0.08968223631381989, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.3437376022338867]\n",
      "   52 [D loss: 0.08541128039360046, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.349168598651886]\n",
      "   53 [D loss: 0.08471950143575668, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.3493348956108093]\n",
      "   54 [D loss: 0.08986902236938477, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.3483623266220093]\n",
      "   55 [D loss: 0.08874538540840149, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.3483772277832031]\n",
      "   56 [D loss: 0.08469535410404205, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.35168299078941345]\n",
      "   57 [D loss: 0.08127079904079437, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.35309189558029175]\n",
      "   58 [D loss: 0.08583690226078033, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.3580085337162018]\n",
      "   59 [D loss: 0.08320144563913345, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.3563895523548126]\n",
      "   60 [D loss: 0.08221578598022461, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.3603479564189911]\n",
      "   61 [D loss: 0.08358179032802582, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.36250197887420654]\n",
      "   62 [D loss: 0.08034339547157288, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.365669846534729]\n",
      "   63 [D loss: 0.07999350130558014, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.3693995475769043]\n",
      "   64 [D loss: 0.07916443049907684, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.3643246293067932]\n",
      "   65 [D loss: 0.07833661884069443, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.371387779712677]\n",
      "   66 [D loss: 0.08174798637628555, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.3678426146507263]\n",
      "   67 [D loss: 0.07912572473287582, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.37386268377304077]\n",
      "   68 [D loss: 0.07591426372528076, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.37308555841445923]\n",
      "   69 [D loss: 0.07361941784620285, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.3746761083602905]\n",
      "   70 [D loss: 0.07503096014261246, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.37567979097366333]\n",
      "   71 [D loss: 0.07623209059238434, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.38209378719329834]\n",
      "   72 [D loss: 0.07476978003978729, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.38539421558380127]\n",
      "   73 [D loss: 0.07350203394889832, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.387043833732605]\n",
      "   74 [D loss: 0.07115492224693298, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.38947710394859314]\n",
      "   75 [D loss: 0.07222481071949005, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.38697385787963867]\n",
      "   76 [D loss: 0.07015888392925262, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.388660728931427]\n",
      "   77 [D loss: 0.07832035422325134, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.3918447494506836]\n",
      "   78 [D loss: 0.06983844935894012, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.39233076572418213]\n",
      "   79 [D loss: 0.07078944891691208, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.39505642652511597]\n",
      "   80 [D loss: 0.07238961011171341, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.39287126064300537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   81 [D loss: 0.07009893655776978, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.39385664463043213]\n",
      "   82 [D loss: 0.06912453472614288, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.3946772813796997]\n",
      "   83 [D loss: 0.06716099381446838, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.39449581503868103]\n",
      "   84 [D loss: 0.06758897751569748, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4003865122795105]\n",
      "   85 [D loss: 0.06816013902425766, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.39864110946655273]\n",
      "   86 [D loss: 0.06827177107334137, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4019213318824768]\n",
      "   87 [D loss: 0.06572592258453369, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.40425336360931396]\n",
      "   88 [D loss: 0.06492095440626144, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.40429046750068665]\n",
      "   89 [D loss: 0.06622583419084549, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4109276533126831]\n",
      "   90 [D loss: 0.06471701711416245, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4129490256309509]\n",
      "   91 [D loss: 0.06533712148666382, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.41443532705307007]\n",
      "   92 [D loss: 0.06301740556955338, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.41511276364326477]\n",
      "   93 [D loss: 0.06243159994482994, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.41742515563964844]\n",
      "   94 [D loss: 0.06245638057589531, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.42301538586616516]\n",
      "   95 [D loss: 0.061133403331041336, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4262956380844116]\n",
      "   96 [D loss: 0.06720670312643051, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.4303474724292755]\n",
      "   97 [D loss: 0.09092406183481216, acc_real: 93.750000, acc_fake: 100.000000] [G loss: 0.4308179020881653]\n",
      "   98 [D loss: 0.1539275050163269, acc_real: 81.250000, acc_fake: 100.000000] [G loss: 0.4336414933204651]\n",
      "   99 [D loss: 0.057553090155124664, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4333030879497528]\n",
      "  100 [D loss: 0.05850830301642418, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.43384575843811035]\n",
      "  101 [D loss: 0.05608510226011276, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4331764578819275]\n",
      "  102 [D loss: 0.058047953993082047, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.44000428915023804]\n",
      "  103 [D loss: 0.05642784759402275, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.43982839584350586]\n",
      "  104 [D loss: 0.05580238997936249, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.44424164295196533]\n",
      "  105 [D loss: 0.05601878464221954, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4472883641719818]\n",
      "  106 [D loss: 0.05732498690485954, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.45163190364837646]\n",
      "  107 [D loss: 0.05592220276594162, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.45391181111335754]\n",
      "  108 [D loss: 0.055543288588523865, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.45491325855255127]\n",
      "  109 [D loss: 0.057298384606838226, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4570421576499939]\n",
      "  110 [D loss: 0.05210139602422714, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.461283415555954]\n",
      "  111 [D loss: 0.051508828997612, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4661830961704254]\n",
      "  112 [D loss: 0.04968912899494171, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4696287512779236]\n",
      "  113 [D loss: 0.04971186816692352, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4721566438674927]\n",
      "  114 [D loss: 0.048011988401412964, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.47824370861053467]\n",
      "  115 [D loss: 0.04808828979730606, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4775688648223877]\n",
      "  116 [D loss: 0.04735748469829559, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.48306867480278015]\n",
      "  117 [D loss: 0.045317672193050385, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4870203137397766]\n",
      "  118 [D loss: 0.04494677111506462, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4902842044830322]\n",
      "  119 [D loss: 0.05234730616211891, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.49809983372688293]\n",
      "  120 [D loss: 0.15265634655952454, acc_real: 78.125000, acc_fake: 100.000000] [G loss: 0.49037736654281616]\n",
      "  121 [D loss: 0.04478539526462555, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.48513972759246826]\n",
      "  122 [D loss: 0.045040879398584366, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.48591428995132446]\n",
      "  123 [D loss: 0.046049803495407104, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4835812449455261]\n",
      "  124 [D loss: 0.04875672608613968, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.48217839002609253]\n",
      "  125 [D loss: 0.05043788254261017, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.48116201162338257]\n",
      "  126 [D loss: 0.05213562399148941, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.47777193784713745]\n",
      "  127 [D loss: 0.053807683289051056, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.486051470041275]\n",
      "  128 [D loss: 0.048216141760349274, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4821433424949646]\n",
      "  129 [D loss: 0.05394035577774048, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.4838477075099945]\n",
      "  130 [D loss: 0.04569145292043686, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.4885810613632202]\n",
      "  131 [D loss: 0.054201770573854446, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4846044182777405]\n",
      "  132 [D loss: 0.04836222529411316, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.48223578929901123]\n",
      "  133 [D loss: 0.052452631294727325, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4881661534309387]\n",
      "  134 [D loss: 0.0525544099509716, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.48544198274612427]\n",
      "  135 [D loss: 0.042243003845214844, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4851573407649994]\n",
      "  136 [D loss: 0.05118659883737564, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.4885558485984802]\n",
      "  137 [D loss: 0.0555390790104866, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.48538559675216675]\n",
      "  138 [D loss: 0.049539610743522644, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.4929288327693939]\n",
      "  139 [D loss: 0.06013884022831917, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.4938632547855377]\n",
      "  140 [D loss: 0.04650573804974556, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.49150511622428894]\n",
      "  141 [D loss: 0.048946313560009, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.4955127239227295]\n",
      "  142 [D loss: 0.04563451558351517, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.49401211738586426]\n",
      "  143 [D loss: 0.04992527514696121, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.49378064274787903]\n",
      "  144 [D loss: 0.05801559239625931, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.4979136884212494]\n",
      "  145 [D loss: 0.053112924098968506, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.49360737204551697]\n",
      "  146 [D loss: 0.057398997247219086, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.4976300001144409]\n",
      "  147 [D loss: 0.04887193813920021, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.4985911250114441]\n",
      "  148 [D loss: 0.04645942151546478, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5042876601219177]\n",
      "  149 [D loss: 0.04899432510137558, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.5018145442008972]\n",
      "  150 [D loss: 0.050458744168281555, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.5065973997116089]\n",
      "  151 [D loss: 0.04970350116491318, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.5044719576835632]\n",
      "  152 [D loss: 0.048840053379535675, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5037314891815186]\n",
      "  153 [D loss: 0.049436938017606735, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.506462037563324]\n",
      "  154 [D loss: 0.051552146673202515, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.5072214603424072]\n",
      "  155 [D loss: 0.04649196192622185, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.5106016993522644]\n",
      "  156 [D loss: 0.04398209601640701, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5071667432785034]\n",
      "  157 [D loss: 0.048315227031707764, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.5097001791000366]\n",
      "  158 [D loss: 0.05176795274019241, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.5099966526031494]\n",
      "  159 [D loss: 0.042195066809654236, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5067018270492554]\n",
      "  160 [D loss: 0.04290338605642319, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5117918252944946]\n",
      "  161 [D loss: 0.0434177927672863, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5122653245925903]\n",
      "  162 [D loss: 0.04904729127883911, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5192299485206604]\n",
      "  163 [D loss: 0.04276757687330246, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5175924301147461]\n",
      "  164 [D loss: 0.04453711956739426, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.5184637308120728]\n",
      "  165 [D loss: 0.0421203076839447, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.5247277021408081]\n",
      "  166 [D loss: 0.04464245215058327, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.521487295627594]\n",
      "  167 [D loss: 0.05333373323082924, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.5240713953971863]\n",
      "  168 [D loss: 0.057088665664196014, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.5280458927154541]\n",
      "  169 [D loss: 0.0412789024412632, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5305250883102417]\n",
      "  170 [D loss: 0.04010966792702675, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.5312142968177795]\n",
      "  171 [D loss: 0.04490138590335846, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5299734473228455]\n",
      "  172 [D loss: 0.04277113080024719, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5341630578041077]\n",
      "  173 [D loss: 0.03367479145526886, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5362493395805359]\n",
      "  174 [D loss: 0.04961429536342621, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5353960990905762]\n",
      "  175 [D loss: 0.043678998947143555, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5383772850036621]\n",
      "  176 [D loss: 0.03746810182929039, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5449057817459106]\n",
      "  177 [D loss: 0.03927864506840706, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5428503751754761]\n",
      "  178 [D loss: 0.037658002227544785, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5430129766464233]\n",
      "  179 [D loss: 0.039752595126628876, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5467129349708557]\n",
      "  180 [D loss: 0.042221661657094955, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.5495226383209229]\n",
      "  181 [D loss: 0.04538973420858383, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.5479030609130859]\n",
      "  182 [D loss: 0.041410624980926514, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.550906777381897]\n",
      "  183 [D loss: 0.03784593939781189, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5553090572357178]\n",
      "  184 [D loss: 0.03707054629921913, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5611201524734497]\n",
      "  185 [D loss: 0.038557734340429306, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5604629516601562]\n",
      "  186 [D loss: 0.039087094366550446, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5652071237564087]\n",
      "  187 [D loss: 0.034513816237449646, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5685946941375732]\n",
      "  188 [D loss: 0.033784978091716766, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5708023905754089]\n",
      "  189 [D loss: 0.035443007946014404, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5719132423400879]\n",
      "  190 [D loss: 0.035469163209199905, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5771979689598083]\n",
      "  191 [D loss: 0.035106174647808075, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5775038003921509]\n",
      "  192 [D loss: 0.02935301885008812, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5821003913879395]\n",
      "  193 [D loss: 0.03621315956115723, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5853738784790039]\n",
      "  194 [D loss: 0.031133579090237617, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5875738859176636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  195 [D loss: 0.030279699712991714, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5869312286376953]\n",
      "  196 [D loss: 0.03747802972793579, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5978775024414062]\n",
      "  197 [D loss: 0.03028777986764908, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5916570425033569]\n",
      "  198 [D loss: 0.02763943187892437, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5932309627532959]\n",
      "  199 [D loss: 0.030181750655174255, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5989455580711365]\n",
      "  200 [D loss: 0.031236877664923668, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6044825911521912]\n",
      "  201 [D loss: 0.0276922769844532, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6080288887023926]\n",
      "  202 [D loss: 0.028257492929697037, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.6122532486915588]\n",
      "  203 [D loss: 0.028869889676570892, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6086024045944214]\n",
      "  204 [D loss: 0.02749473601579666, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6159219741821289]\n",
      "  205 [D loss: 0.027469133958220482, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.620984673500061]\n",
      "  206 [D loss: 0.028079602867364883, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6261703372001648]\n",
      "  207 [D loss: 0.02217395044863224, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6259182691574097]\n",
      "  208 [D loss: 0.02640889212489128, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6255396604537964]\n",
      "  209 [D loss: 0.024569598957896233, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6325852870941162]\n",
      "  210 [D loss: 0.02677149325609207, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6293453574180603]\n",
      "  211 [D loss: 0.022554125636816025, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6361068487167358]\n",
      "  212 [D loss: 0.02338258922100067, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6389870643615723]\n",
      "  213 [D loss: 0.025487426668405533, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6441766619682312]\n",
      "  214 [D loss: 0.023114515468478203, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6486508846282959]\n",
      "  215 [D loss: 0.021475287154316902, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6543703079223633]\n",
      "  216 [D loss: 0.017226645722985268, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6526225805282593]\n",
      "  217 [D loss: 0.02530868537724018, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6555763483047485]\n",
      "  218 [D loss: 0.021387986838817596, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6617486476898193]\n",
      "  219 [D loss: 0.02046629786491394, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6602782011032104]\n",
      "  220 [D loss: 0.022101879119873047, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6705284118652344]\n",
      "  221 [D loss: 0.016800187528133392, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6687365174293518]\n",
      "  222 [D loss: 0.029322141781449318, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.6655251383781433]\n",
      "  223 [D loss: 0.019159678369760513, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6624456644058228]\n",
      "  224 [D loss: 0.021696055307984352, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6658324003219604]\n",
      "  225 [D loss: 0.020889688283205032, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6629984378814697]\n",
      "  226 [D loss: 0.02523370273411274, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6589381098747253]\n",
      "  227 [D loss: 0.020267104730010033, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6593877673149109]\n",
      "  228 [D loss: 0.01922866888344288, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6594896912574768]\n",
      "  229 [D loss: 0.02349258027970791, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6598162651062012]\n",
      "  230 [D loss: 0.020269079133868217, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.66376131772995]\n",
      "  231 [D loss: 0.021559521555900574, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6657154560089111]\n",
      "  232 [D loss: 0.020493803545832634, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.66780686378479]\n",
      "  233 [D loss: 0.020895671099424362, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6675048470497131]\n",
      "  234 [D loss: 0.02008609101176262, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.6738722324371338]\n",
      "  235 [D loss: 0.018734807148575783, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.673560380935669]\n",
      "  236 [D loss: 0.018077068030834198, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6749472618103027]\n",
      "  237 [D loss: 0.0177093967795372, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6732726097106934]\n",
      "  238 [D loss: 0.01961403340101242, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.676115870475769]\n",
      "  239 [D loss: 0.015486770309507847, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6832098960876465]\n",
      "  240 [D loss: 0.020459819585084915, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6791046857833862]\n",
      "  241 [D loss: 0.01606987603008747, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6781763434410095]\n",
      "  242 [D loss: 0.018559321761131287, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6814723014831543]\n",
      "  243 [D loss: 0.016356397420167923, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6829112768173218]\n",
      "  244 [D loss: 0.01875324547290802, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6873584985733032]\n",
      "  245 [D loss: 0.016293853521347046, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6847995519638062]\n",
      "  246 [D loss: 0.018530361354351044, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6875544786453247]\n",
      "  247 [D loss: 0.01760723441839218, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6920211911201477]\n",
      "  248 [D loss: 0.015142516233026981, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6901099681854248]\n",
      "  249 [D loss: 0.014833210967481136, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6960649490356445]\n",
      "  250 [D loss: 0.014901809394359589, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.704363226890564]\n",
      "  251 [D loss: 0.014962585642933846, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7015296220779419]\n",
      "  252 [D loss: 0.01618381217122078, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7015042304992676]\n",
      "  253 [D loss: 0.021848347038030624, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.7058796882629395]\n",
      "  254 [D loss: 0.014242756180465221, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7024550437927246]\n",
      "  255 [D loss: 0.014906054362654686, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7027088403701782]\n",
      "  256 [D loss: 0.012648159638047218, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7048130035400391]\n",
      "  257 [D loss: 0.014039309695363045, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7098613977432251]\n",
      "  258 [D loss: 0.014713605865836143, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7104434967041016]\n",
      "  259 [D loss: 0.014639751985669136, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7152853012084961]\n",
      "  260 [D loss: 0.015206906944513321, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7107619047164917]\n",
      "  261 [D loss: 0.016068099066615105, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7065465450286865]\n",
      "  262 [D loss: 0.015577532351016998, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7052278518676758]\n",
      "  263 [D loss: 0.013626908883452415, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7040265202522278]\n",
      "  264 [D loss: 0.01549875270575285, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6968968510627747]\n",
      "  265 [D loss: 0.014382297173142433, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6969122886657715]\n",
      "  266 [D loss: 0.014476078562438488, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7003716230392456]\n",
      "  267 [D loss: 0.014539632014930248, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.701662540435791]\n",
      "  268 [D loss: 0.014658274129033089, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7014274001121521]\n",
      "  269 [D loss: 0.01468170341104269, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7082408666610718]\n",
      "  270 [D loss: 0.01343442127108574, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7062606811523438]\n",
      "  271 [D loss: 0.0124211385846138, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7066713571548462]\n",
      "  272 [D loss: 0.014089930802583694, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7093995809555054]\n",
      "  273 [D loss: 0.01521455030888319, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7118732929229736]\n",
      "  274 [D loss: 0.015889078378677368, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7143818736076355]\n",
      "  275 [D loss: 0.012866973876953125, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7187541127204895]\n",
      "  276 [D loss: 0.014205049723386765, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7180341482162476]\n",
      "  277 [D loss: 0.012425608932971954, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7228865027427673]\n",
      "  278 [D loss: 0.012396673671901226, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7251091599464417]\n",
      "  279 [D loss: 0.012911425903439522, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7290258407592773]\n",
      "  280 [D loss: 0.01268219854682684, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7329506874084473]\n",
      "  281 [D loss: 0.011229344643652439, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7341586351394653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  282 [D loss: 0.012787813320755959, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7343835830688477]\n",
      "  283 [D loss: 0.017438028007745743, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.7397430539131165]\n",
      "  284 [D loss: 0.010897818952798843, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7360783815383911]\n",
      "  285 [D loss: 0.011578746140003204, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7361925840377808]\n",
      "  286 [D loss: 0.009876232594251633, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.738838255405426]\n",
      "  287 [D loss: 0.012382722459733486, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7384259700775146]\n",
      "  288 [D loss: 0.011652566492557526, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7402031421661377]\n",
      "  289 [D loss: 0.010447870939970016, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7375386953353882]\n",
      "  290 [D loss: 0.011233681812882423, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7362463474273682]\n",
      "  291 [D loss: 0.012059404514729977, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7383442521095276]\n",
      "  292 [D loss: 0.010141576640307903, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7404248118400574]\n",
      "  293 [D loss: 0.011414721608161926, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7402864694595337]\n",
      "  294 [D loss: 0.013535554520785809, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.7386037111282349]\n",
      "  295 [D loss: 0.011407079175114632, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7420163154602051]\n",
      "  296 [D loss: 0.01236008107662201, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7394484281539917]\n",
      "  297 [D loss: 0.011433222331106663, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7384288311004639]\n",
      "  298 [D loss: 0.010701270774006844, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7414730191230774]\n",
      "  299 [D loss: 0.010052632540464401, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7429405450820923]\n",
      "  300 [D loss: 0.01085959654301405, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7399536371231079]\n",
      "  301 [D loss: 0.010180793702602386, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.742396354675293]\n",
      "  302 [D loss: 0.00950945820659399, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7422356605529785]\n",
      "  303 [D loss: 0.01083236001431942, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7469179630279541]\n",
      "  304 [D loss: 0.008709192276000977, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7505871057510376]\n",
      "  305 [D loss: 0.009380104020237923, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7497832775115967]\n",
      "  306 [D loss: 0.01029559038579464, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7544958591461182]\n",
      "  307 [D loss: 0.010327065363526344, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7520180940628052]\n",
      "  308 [D loss: 0.009367642924189568, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7550169825553894]\n",
      "  309 [D loss: 0.010115709155797958, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7559794187545776]\n",
      "  310 [D loss: 0.013708706013858318, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.7506835460662842]\n",
      "  311 [D loss: 0.01048475131392479, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7493269443511963]\n",
      "  312 [D loss: 0.008268803358078003, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7485231161117554]\n",
      "  313 [D loss: 0.009360299445688725, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7492581605911255]\n",
      "  314 [D loss: 0.00989779457449913, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7464169263839722]\n",
      "  315 [D loss: 0.012071453034877777, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7499386668205261]\n",
      "  316 [D loss: 0.009359854273498058, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7504652738571167]\n",
      "  317 [D loss: 0.008761171251535416, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7483478784561157]\n",
      "  318 [D loss: 0.010330760851502419, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7493597865104675]\n",
      "  319 [D loss: 0.010744224302470684, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7498420476913452]\n",
      "  320 [D loss: 0.009944817051291466, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7498811483383179]\n",
      "  321 [D loss: 0.010111206211149693, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7558627128601074]\n",
      "  322 [D loss: 0.008999047800898552, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7529312968254089]\n",
      "  323 [D loss: 0.011262602172791958, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7553824186325073]\n",
      "  324 [D loss: 0.009447267279028893, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.758206844329834]\n",
      "  325 [D loss: 0.008588161319494247, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7597406506538391]\n",
      "  326 [D loss: 0.009504096582531929, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7609750628471375]\n",
      "  327 [D loss: 0.009466791525483131, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7612082958221436]\n",
      "  328 [D loss: 0.009268495254218578, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7613687515258789]\n",
      "  329 [D loss: 0.008822659961879253, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7697781920433044]\n",
      "  330 [D loss: 0.01005183719098568, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7631604075431824]\n",
      "  331 [D loss: 0.00983018521219492, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7619131803512573]\n",
      "  332 [D loss: 0.009995854459702969, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7622249126434326]\n",
      "  333 [D loss: 0.00991484709084034, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7652860879898071]\n",
      "  334 [D loss: 0.007821099832654, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7650539875030518]\n",
      "  335 [D loss: 0.00873463973402977, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7665090560913086]\n",
      "  336 [D loss: 0.008792711421847343, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7738623023033142]\n",
      "  337 [D loss: 0.009372367523610592, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7696245908737183]\n",
      "  338 [D loss: 0.009244355373084545, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7730197906494141]\n",
      "  339 [D loss: 0.008565152995288372, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7751293182373047]\n",
      "  340 [D loss: 0.009689724072813988, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7771738767623901]\n",
      "  341 [D loss: 0.01015390083193779, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7765237092971802]\n",
      "  342 [D loss: 0.01728871278464794, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.7762250900268555]\n",
      "  343 [D loss: 0.014115139842033386, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7303406596183777]\n",
      "  344 [D loss: 0.0181739442050457, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.685271143913269]\n",
      "  345 [D loss: 0.020284201949834824, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6521486043930054]\n",
      "  346 [D loss: 0.02715330198407173, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6237879991531372]\n",
      "  347 [D loss: 0.02723867818713188, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6034620404243469]\n",
      "  348 [D loss: 0.03214077651500702, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.580521285533905]\n",
      "  349 [D loss: 0.03823059797286987, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.5629675984382629]\n",
      "  350 [D loss: 0.03613109514117241, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5544230341911316]\n",
      "  351 [D loss: 0.04662356898188591, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5476417541503906]\n",
      "  352 [D loss: 0.05085865035653114, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5287690162658691]\n",
      "  353 [D loss: 0.04552841931581497, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5267727971076965]\n",
      "  354 [D loss: 0.05328318476676941, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.5133928060531616]\n",
      "  355 [D loss: 0.051708903163671494, acc_real: 100.000000, acc_fake: 89.062500] [G loss: 0.516818106174469]\n",
      "  356 [D loss: 0.04873589053750038, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.5184029340744019]\n",
      "  357 [D loss: 0.051808133721351624, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.5122022032737732]\n",
      "  358 [D loss: 0.055402226746082306, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.5074023008346558]\n",
      "  359 [D loss: 0.060663722455501556, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.5100681781768799]\n",
      "  360 [D loss: 0.05149321258068085, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.5118744373321533]\n",
      "  361 [D loss: 0.05040010064840317, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5162408947944641]\n",
      "  362 [D loss: 0.05456603690981865, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.4996700882911682]\n",
      "  363 [D loss: 0.05590564012527466, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.5118041038513184]\n",
      "  364 [D loss: 0.05957377701997757, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.5134720802307129]\n",
      "  365 [D loss: 0.05057937651872635, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5113600492477417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  366 [D loss: 0.061146143823862076, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.5112848877906799]\n",
      "  367 [D loss: 0.05886274576187134, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.5107765793800354]\n",
      "  368 [D loss: 0.06220565363764763, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.5113869905471802]\n",
      "  369 [D loss: 0.0689338892698288, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.5147407054901123]\n",
      "  370 [D loss: 0.051065072417259216, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.523224413394928]\n",
      "  371 [D loss: 0.059753358364105225, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.5184689164161682]\n",
      "  372 [D loss: 0.04590888321399689, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.5344215631484985]\n",
      "  373 [D loss: 0.05431540310382843, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.5321462154388428]\n",
      "  374 [D loss: 0.05442126467823982, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.539267361164093]\n",
      "  375 [D loss: 0.05207482725381851, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.547203540802002]\n",
      "  376 [D loss: 0.05402005836367607, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.5400716066360474]\n",
      "  377 [D loss: 0.06093280762434006, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.5582341551780701]\n",
      "  378 [D loss: 0.03907207027077675, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.5589081048965454]\n",
      "  379 [D loss: 0.043647605925798416, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.5613440275192261]\n",
      "  380 [D loss: 0.046197086572647095, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.5659164190292358]\n",
      "  381 [D loss: 0.04251903295516968, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.5795251727104187]\n",
      "  382 [D loss: 0.043079473078250885, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.58446204662323]\n",
      "  383 [D loss: 0.04298042505979538, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.5838395357131958]\n",
      "  384 [D loss: 0.04068879783153534, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.5981444716453552]\n",
      "  385 [D loss: 0.04099389910697937, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.6022483110427856]\n",
      "  386 [D loss: 0.03703692555427551, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.6059318780899048]\n",
      "  387 [D loss: 0.031809765845537186, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.606489360332489]\n",
      "  388 [D loss: 0.03916827216744423, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.6183993816375732]\n",
      "  389 [D loss: 0.03104293905198574, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.620119571685791]\n",
      "  390 [D loss: 0.036035314202308655, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.6289669871330261]\n",
      "  391 [D loss: 0.03433855623006821, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.6296248435974121]\n",
      "  392 [D loss: 0.025615574792027473, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.6424957513809204]\n",
      "  393 [D loss: 0.027988553047180176, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6465547680854797]\n",
      "  394 [D loss: 0.024054158478975296, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6540285348892212]\n",
      "  395 [D loss: 0.023879844695329666, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6581507325172424]\n",
      "  396 [D loss: 0.027030522003769875, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6650530099868774]\n",
      "  397 [D loss: 0.025051511824131012, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.6769753098487854]\n",
      "  398 [D loss: 0.022376762703061104, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.6830191612243652]\n",
      "  399 [D loss: 0.0216965451836586, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6845403909683228]\n",
      "  400 [D loss: 0.02145170420408249, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6866029500961304]\n",
      "  401 [D loss: 0.02050437033176422, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6884893178939819]\n",
      "  402 [D loss: 0.020237401127815247, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7067680954933167]\n",
      "  403 [D loss: 0.01720685511827469, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7049797773361206]\n",
      "  404 [D loss: 0.017876461148262024, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7117716073989868]\n",
      "  405 [D loss: 0.016112715005874634, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7237739562988281]\n",
      "  406 [D loss: 0.017654750496149063, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7222300171852112]\n",
      "  407 [D loss: 0.01611213944852352, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7217731475830078]\n",
      "  408 [D loss: 0.018639886751770973, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7297439575195312]\n",
      "  409 [D loss: 0.013938454911112785, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7380126714706421]\n",
      "  410 [D loss: 0.014835384674370289, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7423802614212036]\n",
      "  411 [D loss: 0.014556724578142166, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7468534111976624]\n",
      "  412 [D loss: 0.009866878390312195, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7534955739974976]\n",
      "  413 [D loss: 0.011671975255012512, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7523328065872192]\n",
      "  414 [D loss: 0.01301513146609068, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7558804750442505]\n",
      "  415 [D loss: 0.010425429791212082, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7570021152496338]\n",
      "  416 [D loss: 0.009601743891835213, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7693296074867249]\n",
      "  417 [D loss: 0.011326344683766365, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7712449431419373]\n",
      "  418 [D loss: 0.01055541355162859, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7760668992996216]\n",
      "  419 [D loss: 0.008634554222226143, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7699634432792664]\n",
      "  420 [D loss: 0.008424406871199608, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7776896357536316]\n",
      "  421 [D loss: 0.009225651621818542, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7719323635101318]\n",
      "  422 [D loss: 0.006822704337537289, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7789753675460815]\n",
      "  423 [D loss: 0.007751983590424061, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7854146957397461]\n",
      "  424 [D loss: 0.0076033445075154305, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7846215963363647]\n",
      "  425 [D loss: 0.007613103371113539, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7897030115127563]\n",
      "  426 [D loss: 0.008246512152254581, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7883253693580627]\n",
      "  427 [D loss: 0.006863716058433056, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7918601036071777]\n",
      "  428 [D loss: 0.009173143655061722, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7949646711349487]\n",
      "  429 [D loss: 0.00804375484585762, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.787992000579834]\n",
      "  430 [D loss: 0.010462743230164051, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.795234203338623]\n",
      "  431 [D loss: 0.008896807208657265, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.795582115650177]\n",
      "  432 [D loss: 0.00802154466509819, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7986494898796082]\n",
      "  433 [D loss: 0.0069152130745351315, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7926764488220215]\n",
      "  434 [D loss: 0.008840275928378105, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8006526827812195]\n",
      "  435 [D loss: 0.0058921752497553825, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7945878505706787]\n",
      "  436 [D loss: 0.007053867448121309, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8016976714134216]\n",
      "  437 [D loss: 0.008246082812547684, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7982418537139893]\n",
      "  438 [D loss: 0.007631426677107811, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7995493412017822]\n",
      "  439 [D loss: 0.0082847960293293, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8020966649055481]\n",
      "  440 [D loss: 0.008956056088209152, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8044677376747131]\n",
      "  441 [D loss: 0.0058501302264630795, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8039900660514832]\n",
      "  442 [D loss: 0.006672214716672897, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8026272058486938]\n",
      "  443 [D loss: 0.006399206351488829, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8087635636329651]\n",
      "  444 [D loss: 0.005546151660382748, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.809736967086792]\n",
      "  445 [D loss: 0.006350134499371052, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8111425638198853]\n",
      "  446 [D loss: 0.007194490171968937, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8145170211791992]\n",
      "  447 [D loss: 0.0055176494643092155, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.814582109451294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  448 [D loss: 0.006952292285859585, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8150579929351807]\n",
      "  449 [D loss: 0.006314587779343128, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8145133256912231]\n",
      "  450 [D loss: 0.007819419726729393, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8152663707733154]\n",
      "  451 [D loss: 0.007928709499537945, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8101212978363037]\n",
      "  452 [D loss: 0.00621894933283329, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8131192922592163]\n",
      "  453 [D loss: 0.0059374733828008175, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8121337294578552]\n",
      "  454 [D loss: 0.013905888423323631, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.8137718439102173]\n",
      "  455 [D loss: 0.005779474042356014, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.814886748790741]\n",
      "  456 [D loss: 0.004763536620885134, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8112376928329468]\n",
      "  457 [D loss: 0.007611657492816448, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8101526498794556]\n",
      "  458 [D loss: 0.005647324491292238, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8157800436019897]\n",
      "  459 [D loss: 0.005599012598395348, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8112884163856506]\n",
      "  460 [D loss: 0.007428889628499746, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8145653605461121]\n",
      "  461 [D loss: 0.00583244301378727, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8193137049674988]\n",
      "  462 [D loss: 0.007002033293247223, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8176197409629822]\n",
      "  463 [D loss: 0.005596780218183994, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8176083564758301]\n",
      "  464 [D loss: 0.00685889134183526, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8110362887382507]\n",
      "  465 [D loss: 0.006022353656589985, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8224241137504578]\n",
      "  466 [D loss: 0.005775990895926952, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8200146555900574]\n",
      "  467 [D loss: 0.004740814678370953, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8210150599479675]\n",
      "  468 [D loss: 0.005604449659585953, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8279819488525391]\n",
      "  469 [D loss: 0.005284386686980724, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8207690119743347]\n",
      "  470 [D loss: 0.00545716006308794, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8257519602775574]\n",
      "  471 [D loss: 0.00639773067086935, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8275052905082703]\n",
      "  472 [D loss: 0.008526010438799858, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.8240991830825806]\n",
      "  473 [D loss: 0.0050199287943542, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8248386383056641]\n",
      "  474 [D loss: 0.005937142297625542, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8228490948677063]\n",
      "  475 [D loss: 0.004513708408921957, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8237451314926147]\n",
      "  476 [D loss: 0.005886262282729149, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8231680393218994]\n",
      "  477 [D loss: 0.005172584671527147, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8259701728820801]\n",
      "  478 [D loss: 0.005325079895555973, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.822433590888977]\n",
      "  479 [D loss: 0.004747967701405287, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8297882676124573]\n",
      "  480 [D loss: 0.005188854411244392, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8235408663749695]\n",
      "  481 [D loss: 0.006254693027585745, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8257062435150146]\n",
      "  482 [D loss: 0.007083576172590256, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8209481239318848]\n",
      "  483 [D loss: 0.0063933501951396465, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8189083337783813]\n",
      "  484 [D loss: 0.005624482408165932, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8218954801559448]\n",
      "  485 [D loss: 0.005342480260878801, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8227900266647339]\n",
      "  486 [D loss: 0.006078096106648445, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8215845823287964]\n",
      "  487 [D loss: 0.0051782941445708275, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8224635124206543]\n",
      "  488 [D loss: 0.005975634790956974, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8236780166625977]\n",
      "  489 [D loss: 0.005775455851107836, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8216756582260132]\n",
      "  490 [D loss: 0.006358542013913393, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8240695595741272]\n",
      "  491 [D loss: 0.004978830460458994, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8278485536575317]\n",
      "  492 [D loss: 0.005060587543994188, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8312118053436279]\n",
      "  493 [D loss: 0.0043136244639754295, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8229285478591919]\n",
      "  494 [D loss: 0.006411542184650898, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.820344865322113]\n",
      "  495 [D loss: 0.005649242550134659, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8140906095504761]\n",
      "  496 [D loss: 0.007230174262076616, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8103006482124329]\n",
      "  497 [D loss: 0.007029295898973942, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8097901344299316]\n",
      "  498 [D loss: 0.01260604802519083, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.8063533306121826]\n",
      "  499 [D loss: 0.005987029522657394, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8043645620346069]\n",
      "  500 [D loss: 0.007982119917869568, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7969497442245483]\n",
      "  501 [D loss: 0.008061710745096207, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7923386693000793]\n",
      "  502 [D loss: 0.006640725303441286, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7933914661407471]\n",
      "  503 [D loss: 0.0078086042776703835, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7944813966751099]\n",
      "  504 [D loss: 0.006876728497445583, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7960245609283447]\n",
      "  505 [D loss: 0.008198392577469349, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8053916692733765]\n",
      "  506 [D loss: 0.0065714591182768345, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7959504723548889]\n",
      "  507 [D loss: 0.007125832140445709, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8056906461715698]\n",
      "  508 [D loss: 0.008624739944934845, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7995932102203369]\n",
      "  509 [D loss: 0.0061275591142475605, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8017129898071289]\n",
      "  510 [D loss: 0.007805737666785717, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.803878903388977]\n",
      "  511 [D loss: 0.006486182101070881, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8077722787857056]\n",
      "  512 [D loss: 0.00650631682947278, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8057816624641418]\n",
      "  513 [D loss: 0.006804408971220255, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8122146129608154]\n",
      "  514 [D loss: 0.006950125098228455, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8100389242172241]\n",
      "  515 [D loss: 0.00767684169113636, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.8131660223007202]\n",
      "  516 [D loss: 0.015617432072758675, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.8176952600479126]\n",
      "  517 [D loss: 0.007214708253741264, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.813768744468689]\n",
      "  518 [D loss: 0.006222286261618137, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8088657855987549]\n",
      "  519 [D loss: 0.005850021727383137, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8079654574394226]\n",
      "  520 [D loss: 0.005162485875189304, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8133824467658997]\n",
      "  521 [D loss: 0.007931474596261978, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8144577741622925]\n",
      "  522 [D loss: 0.005671282298862934, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8126697540283203]\n",
      "  523 [D loss: 0.005560973659157753, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8099552392959595]\n",
      "  524 [D loss: 0.0063001010566949844, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8183391094207764]\n",
      "  525 [D loss: 0.006157515570521355, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.815410852432251]\n",
      "  526 [D loss: 0.005571902263909578, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8197786808013916]\n",
      "  527 [D loss: 0.006307183299213648, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.821784257888794]\n",
      "  528 [D loss: 0.005766239948570728, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8205658197402954]\n",
      "  529 [D loss: 0.00443272152915597, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8210893273353577]\n",
      "  530 [D loss: 0.005467656068503857, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.821179986000061]\n",
      "  531 [D loss: 0.00606487225741148, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8256230354309082]\n",
      "  532 [D loss: 0.0066844988614320755, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8264994621276855]\n",
      "  533 [D loss: 0.005325265694409609, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8292335867881775]\n",
      "  534 [D loss: 0.005140456836670637, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8282386660575867]\n",
      "  535 [D loss: 0.005138088017702103, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8276824951171875]\n",
      "  536 [D loss: 0.006854712963104248, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.807306170463562]\n",
      "  537 [D loss: 0.00574223417788744, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7999337911605835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  538 [D loss: 0.008103545755147934, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7746037244796753]\n",
      "  539 [D loss: 0.010301074013113976, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7667100429534912]\n",
      "  540 [D loss: 0.011264258995652199, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7557910680770874]\n",
      "  541 [D loss: 0.01077786460518837, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7532616853713989]\n",
      "  542 [D loss: 0.009511268697679043, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7473487257957458]\n",
      "  543 [D loss: 0.013384554535150528, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7557560205459595]\n",
      "  544 [D loss: 0.013219822198152542, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7410498261451721]\n",
      "  545 [D loss: 0.013618430122733116, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7374866008758545]\n",
      "  546 [D loss: 0.012096873484551907, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7386284470558167]\n",
      "  547 [D loss: 0.014234360307455063, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7442643642425537]\n",
      "  548 [D loss: 0.015098235569894314, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7468845844268799]\n",
      "  549 [D loss: 0.011793594807386398, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7473337054252625]\n",
      "  550 [D loss: 0.012845710851252079, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7501040697097778]\n",
      "  551 [D loss: 0.009192591533064842, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7581601738929749]\n",
      "  552 [D loss: 0.011455672793090343, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7581377625465393]\n",
      "  553 [D loss: 0.015080641955137253, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7619763612747192]\n",
      "  554 [D loss: 0.012479839846491814, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.770262598991394]\n",
      "  555 [D loss: 0.008849000558257103, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7702012062072754]\n",
      "  556 [D loss: 0.017013048753142357, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7732939720153809]\n",
      "  557 [D loss: 0.009659064933657646, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7717588543891907]\n",
      "  558 [D loss: 0.00785369798541069, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7873550653457642]\n",
      "  559 [D loss: 0.007029691711068153, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7868648767471313]\n",
      "  560 [D loss: 0.00981903076171875, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7906113862991333]\n",
      "  561 [D loss: 0.008869740180671215, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7933199405670166]\n",
      "  562 [D loss: 0.007422222755849361, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7926698327064514]\n",
      "  563 [D loss: 0.01754113659262657, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.7955260276794434]\n",
      "  564 [D loss: 0.00776726845651865, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8005077838897705]\n",
      "  565 [D loss: 0.009725615382194519, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8108290433883667]\n",
      "  566 [D loss: 0.007831226103007793, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8064337372779846]\n",
      "  567 [D loss: 0.0069249351508915424, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8074885606765747]\n",
      "  568 [D loss: 0.009915372356772423, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8108134269714355]\n",
      "  569 [D loss: 0.015313688665628433, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.812453031539917]\n",
      "  570 [D loss: 0.007235911674797535, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8127429485321045]\n",
      "  571 [D loss: 0.012695180252194405, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.8128044009208679]\n",
      "  572 [D loss: 0.008680008351802826, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8122764825820923]\n",
      "  573 [D loss: 0.005791211500763893, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8133405447006226]\n",
      "  574 [D loss: 0.007808689959347248, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8120920658111572]\n",
      "  575 [D loss: 0.006532000377774239, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8131991028785706]\n",
      "  576 [D loss: 0.006389990448951721, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8126556873321533]\n",
      "  577 [D loss: 0.006579333916306496, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8212686777114868]\n",
      "  578 [D loss: 0.006119081750512123, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8181841373443604]\n",
      "  579 [D loss: 0.006571503356099129, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8264142870903015]\n",
      "  580 [D loss: 0.0053671905770897865, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.82497638463974]\n",
      "  581 [D loss: 0.0056921979412436485, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8269960284233093]\n",
      "  582 [D loss: 0.005531465169042349, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8280858397483826]\n",
      "  583 [D loss: 0.007762458175420761, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.8293466567993164]\n",
      "  584 [D loss: 0.007845671847462654, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8253048658370972]\n",
      "  585 [D loss: 0.00579601526260376, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8224567770957947]\n",
      "  586 [D loss: 0.006000299006700516, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8263052105903625]\n",
      "  587 [D loss: 0.004908394068479538, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8251147866249084]\n",
      "  588 [D loss: 0.004640254192054272, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8240648508071899]\n",
      "  589 [D loss: 0.011090312153100967, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.8289898633956909]\n",
      "  590 [D loss: 0.004281800240278244, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8234434723854065]\n",
      "  591 [D loss: 0.0046385787427425385, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.826187252998352]\n",
      "  592 [D loss: 0.004912855103611946, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8275797367095947]\n",
      "  593 [D loss: 0.005232217255979776, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8250649571418762]\n",
      "  594 [D loss: 0.0047285836189985275, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8307807445526123]\n",
      "  595 [D loss: 0.005405946634709835, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8291888236999512]\n",
      "  596 [D loss: 0.0052593424916267395, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8305835723876953]\n",
      "  597 [D loss: 0.006193564273416996, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8354384303092957]\n",
      "  598 [D loss: 0.004669825080782175, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8416513204574585]\n",
      "  599 [D loss: 0.004330315627157688, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8409832715988159]\n",
      "  600 [D loss: 0.003685462288558483, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8419780731201172]\n",
      "  601 [D loss: 0.0037634691689163446, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.846500039100647]\n",
      "  602 [D loss: 0.0038042892701923847, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8480286598205566]\n",
      "  603 [D loss: 0.0031907018274068832, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8488835692405701]\n",
      "  604 [D loss: 0.004482715390622616, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.84348464012146]\n",
      "  605 [D loss: 0.0038745140191167593, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8394880294799805]\n",
      "  606 [D loss: 0.004123845137655735, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8398488759994507]\n",
      "  607 [D loss: 0.00432234350591898, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8315406441688538]\n",
      "  608 [D loss: 0.0050948928110301495, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8312276601791382]\n",
      "  609 [D loss: 0.00475666020065546, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8346138000488281]\n",
      "  610 [D loss: 0.0054303128272295, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8303206562995911]\n",
      "  611 [D loss: 0.004043056629598141, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8303238153457642]\n",
      "  612 [D loss: 0.004052308388054371, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.83107990026474]\n",
      "  613 [D loss: 0.005012491252273321, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8363897204399109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  614 [D loss: 0.004724654834717512, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8360501527786255]\n",
      "  615 [D loss: 0.005307572893798351, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8368955850601196]\n",
      "  616 [D loss: 0.005288870073854923, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.838417112827301]\n",
      "  617 [D loss: 0.005034741945564747, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.845157265663147]\n",
      "  618 [D loss: 0.005120116285979748, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8467891216278076]\n",
      "  619 [D loss: 0.003910666797310114, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8553372621536255]\n",
      "  620 [D loss: 0.005893530324101448, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8485753536224365]\n",
      "  621 [D loss: 0.004504019394516945, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8416545987129211]\n",
      "  622 [D loss: 0.0037969332188367844, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8445168733596802]\n",
      "  623 [D loss: 0.004311541095376015, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.84250408411026]\n",
      "  624 [D loss: 0.004405590705573559, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8488854169845581]\n",
      "  625 [D loss: 0.004019820597022772, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8493556380271912]\n",
      "  626 [D loss: 0.004983259830623865, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8519042730331421]\n",
      "  627 [D loss: 0.004035875201225281, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8534741401672363]\n",
      "  628 [D loss: 0.004150031600147486, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8499664068222046]\n",
      "  629 [D loss: 0.0030582973267883062, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8506298661231995]\n",
      "  630 [D loss: 0.003031812608242035, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8513622283935547]\n",
      "  631 [D loss: 0.004344537388533354, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8528989553451538]\n",
      "  632 [D loss: 0.0034471084363758564, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8552412390708923]\n",
      "  633 [D loss: 0.0033114159014075994, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8593735694885254]\n",
      "  634 [D loss: 0.0033070312347263098, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8578336238861084]\n",
      "  635 [D loss: 0.0037785449530929327, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8607231378555298]\n",
      "  636 [D loss: 0.0032403506338596344, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8665218353271484]\n",
      "  637 [D loss: 0.0031105850357562304, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8643444776535034]\n",
      "  638 [D loss: 0.009000255726277828, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.8603594303131104]\n",
      "  639 [D loss: 0.007324778474867344, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.8567660450935364]\n",
      "  640 [D loss: 0.003664137562736869, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8551841378211975]\n",
      "  641 [D loss: 0.004058586899191141, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8536558151245117]\n",
      "  642 [D loss: 0.0033766592387109995, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8515937328338623]\n",
      "  643 [D loss: 0.003801095997914672, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8582157492637634]\n",
      "  644 [D loss: 0.003790765069425106, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8488691449165344]\n",
      "  645 [D loss: 0.004120131954550743, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8489125967025757]\n",
      "  646 [D loss: 0.00357425631955266, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8509315252304077]\n",
      "  647 [D loss: 0.00430701207369566, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8506188988685608]\n",
      "  648 [D loss: 0.01947743073105812, acc_real: 96.875000, acc_fake: 100.000000] [G loss: 0.7542167901992798]\n",
      "  649 [D loss: 0.01783912256360054, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.6695294976234436]\n",
      "  650 [D loss: 0.06344825029373169, acc_real: 100.000000, acc_fake: 87.500000] [G loss: 0.5998958349227905]\n",
      "  651 [D loss: 0.0611407607793808, acc_real: 100.000000, acc_fake: 87.500000] [G loss: 0.5453527569770813]\n",
      "  652 [D loss: 0.08222996443510056, acc_real: 100.000000, acc_fake: 79.687500] [G loss: 0.515789270401001]\n",
      "  653 [D loss: 0.08060234785079956, acc_real: 100.000000, acc_fake: 82.812500] [G loss: 0.486914724111557]\n",
      "  654 [D loss: 0.12030242383480072, acc_real: 100.000000, acc_fake: 62.500000] [G loss: 0.4629141092300415]\n",
      "  655 [D loss: 0.11171294748783112, acc_real: 100.000000, acc_fake: 57.812500] [G loss: 0.4471072554588318]\n",
      "  656 [D loss: 0.10001489520072937, acc_real: 100.000000, acc_fake: 62.500000] [G loss: 0.45104414224624634]\n",
      "  657 [D loss: 0.0946432426571846, acc_real: 100.000000, acc_fake: 75.000000] [G loss: 0.43204227089881897]\n",
      "  658 [D loss: 0.12082001566886902, acc_real: 100.000000, acc_fake: 65.625000] [G loss: 0.4362933039665222]\n",
      "  659 [D loss: 0.09246601164340973, acc_real: 100.000000, acc_fake: 71.875000] [G loss: 0.44305694103240967]\n",
      "  660 [D loss: 0.09118342399597168, acc_real: 100.000000, acc_fake: 68.750000] [G loss: 0.44054466485977173]\n",
      "  661 [D loss: 0.10024788230657578, acc_real: 100.000000, acc_fake: 67.187500] [G loss: 0.4563213884830475]\n",
      "  662 [D loss: 0.10652029514312744, acc_real: 100.000000, acc_fake: 62.500000] [G loss: 0.45708000659942627]\n",
      "  663 [D loss: 0.07885166257619858, acc_real: 100.000000, acc_fake: 76.562500] [G loss: 0.4816628098487854]\n",
      "  664 [D loss: 0.08742892742156982, acc_real: 100.000000, acc_fake: 70.312500] [G loss: 0.4987620711326599]\n",
      "  665 [D loss: 0.07109960168600082, acc_real: 100.000000, acc_fake: 75.000000] [G loss: 0.5110188722610474]\n",
      "  666 [D loss: 0.09251472353935242, acc_real: 100.000000, acc_fake: 71.875000] [G loss: 0.5229717493057251]\n",
      "  667 [D loss: 0.07204342633485794, acc_real: 100.000000, acc_fake: 75.000000] [G loss: 0.5418494939804077]\n",
      "  668 [D loss: 0.056735776364803314, acc_real: 100.000000, acc_fake: 82.812500] [G loss: 0.5522931814193726]\n",
      "  669 [D loss: 0.07129362225532532, acc_real: 100.000000, acc_fake: 78.125000] [G loss: 0.5553510189056396]\n",
      "  670 [D loss: 0.08274578303098679, acc_real: 98.437500, acc_fake: 73.437500] [G loss: 0.5732144117355347]\n",
      "  671 [D loss: 0.07824051380157471, acc_real: 100.000000, acc_fake: 73.437500] [G loss: 0.5756611824035645]\n",
      "  672 [D loss: 0.09570974111557007, acc_real: 98.437500, acc_fake: 67.187500] [G loss: 0.5853421688079834]\n",
      "  673 [D loss: 0.08107492327690125, acc_real: 95.312500, acc_fake: 82.812500] [G loss: 0.6144915819168091]\n",
      "  674 [D loss: 0.0978323295712471, acc_real: 95.312500, acc_fake: 73.437500] [G loss: 0.5952754020690918]\n",
      "  675 [D loss: 0.09037166088819504, acc_real: 93.750000, acc_fake: 81.250000] [G loss: 0.6236281394958496]\n",
      "  676 [D loss: 0.10936427861452103, acc_real: 90.625000, acc_fake: 78.125000] [G loss: 0.592083215713501]\n",
      "  677 [D loss: 0.1143224835395813, acc_real: 92.187500, acc_fake: 75.000000] [G loss: 0.6118227243423462]\n",
      "  678 [D loss: 0.10052335262298584, acc_real: 90.625000, acc_fake: 82.812500] [G loss: 0.6210318207740784]\n",
      "  679 [D loss: 0.07392427325248718, acc_real: 95.312500, acc_fake: 79.687500] [G loss: 0.6125257015228271]\n",
      "  680 [D loss: 0.12969523668289185, acc_real: 84.375000, acc_fake: 87.500000] [G loss: 0.5303546190261841]\n",
      "  681 [D loss: 0.08879023790359497, acc_real: 100.000000, acc_fake: 68.750000] [G loss: 0.4726812243461609]\n",
      "  682 [D loss: 0.10198328644037247, acc_real: 100.000000, acc_fake: 71.875000] [G loss: 0.4280816614627838]\n",
      "  683 [D loss: 0.1576647162437439, acc_real: 100.000000, acc_fake: 46.875000] [G loss: 0.39635640382766724]\n",
      "  684 [D loss: 0.1375669538974762, acc_real: 100.000000, acc_fake: 59.375000] [G loss: 0.36386632919311523]\n",
      "  685 [D loss: 0.13385002315044403, acc_real: 100.000000, acc_fake: 51.562500] [G loss: 0.3372575640678406]\n",
      "  686 [D loss: 0.15049369633197784, acc_real: 100.000000, acc_fake: 59.375000] [G loss: 0.33934485912323]\n",
      "  687 [D loss: 0.1603376567363739, acc_real: 100.000000, acc_fake: 42.187500] [G loss: 0.340945839881897]\n",
      "  688 [D loss: 0.1447361707687378, acc_real: 100.000000, acc_fake: 56.250000] [G loss: 0.3543502688407898]\n",
      "  689 [D loss: 0.1581852287054062, acc_real: 100.000000, acc_fake: 50.000000] [G loss: 0.35140860080718994]\n",
      "  690 [D loss: 0.10939227789640427, acc_real: 100.000000, acc_fake: 64.062500] [G loss: 0.37960284948349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  691 [D loss: 0.08397458493709564, acc_real: 100.000000, acc_fake: 78.125000] [G loss: 0.3770774006843567]\n",
      "  692 [D loss: 0.11570118367671967, acc_real: 100.000000, acc_fake: 67.187500] [G loss: 0.4108588695526123]\n",
      "  693 [D loss: 0.08994552493095398, acc_real: 100.000000, acc_fake: 75.000000] [G loss: 0.4272991418838501]\n",
      "  694 [D loss: 0.07516621798276901, acc_real: 100.000000, acc_fake: 84.375000] [G loss: 0.44260960817337036]\n",
      "  695 [D loss: 0.07618041336536407, acc_real: 100.000000, acc_fake: 85.937500] [G loss: 0.47213059663772583]\n",
      "  696 [D loss: 0.06427088379859924, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.5196859836578369]\n",
      "  697 [D loss: 0.062288425862789154, acc_real: 100.000000, acc_fake: 89.062500] [G loss: 0.5509902238845825]\n",
      "  698 [D loss: 0.03555001690983772, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.5855807065963745]\n",
      "  699 [D loss: 0.033621858805418015, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.6091079711914062]\n",
      "  700 [D loss: 0.030014673247933388, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.64728844165802]\n",
      "  701 [D loss: 0.02395770512521267, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6648129224777222]\n",
      "  702 [D loss: 0.016676001250743866, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.6842062473297119]\n",
      "  703 [D loss: 0.017748501151800156, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.713106632232666]\n",
      "  704 [D loss: 0.016574520617723465, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7181395292282104]\n",
      "  705 [D loss: 0.012841431424021721, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7320738434791565]\n",
      "  706 [D loss: 0.01699230633676052, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.758693516254425]\n",
      "  707 [D loss: 0.013977968133985996, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7546476125717163]\n",
      "  708 [D loss: 0.016168734058737755, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7696676254272461]\n",
      "  709 [D loss: 0.01185817364603281, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7639856338500977]\n",
      "  710 [D loss: 0.018557734787464142, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7650931477546692]\n",
      "  711 [D loss: 0.03476651757955551, acc_real: 98.437500, acc_fake: 96.875000] [G loss: 0.7693045139312744]\n",
      "  712 [D loss: 0.016533540561795235, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.7596176862716675]\n",
      "  713 [D loss: 0.025059673935174942, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.7626281976699829]\n",
      "  714 [D loss: 0.02673053741455078, acc_real: 98.437500, acc_fake: 98.437500] [G loss: 0.7616163492202759]\n",
      "  715 [D loss: 0.023102153092622757, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.7691706418991089]\n",
      "  716 [D loss: 0.024657538160681725, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.774267315864563]\n",
      "  717 [D loss: 0.017314739525318146, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.7738839387893677]\n",
      "  718 [D loss: 0.011905619874596596, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.768586277961731]\n",
      "  719 [D loss: 0.0228973887860775, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7753787040710449]\n",
      "  720 [D loss: 0.021113228052854538, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7801353335380554]\n",
      "  721 [D loss: 0.01849278435111046, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.7868526577949524]\n",
      "  722 [D loss: 0.01441167015582323, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.7861011624336243]\n",
      "  723 [D loss: 0.020957939326763153, acc_real: 98.437500, acc_fake: 98.437500] [G loss: 0.7937160134315491]\n",
      "  724 [D loss: 0.013755898922681808, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8044930696487427]\n",
      "  725 [D loss: 0.013786744326353073, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8018669486045837]\n",
      "  726 [D loss: 0.027486909180879593, acc_real: 96.875000, acc_fake: 100.000000] [G loss: 0.8034671545028687]\n",
      "  727 [D loss: 0.02474675141274929, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.7977110147476196]\n",
      "  728 [D loss: 0.01463234517723322, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.809555172920227]\n",
      "  729 [D loss: 0.025158122181892395, acc_real: 96.875000, acc_fake: 98.437500] [G loss: 0.8036766052246094]\n",
      "  730 [D loss: 0.019678987562656403, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.791685938835144]\n",
      "  731 [D loss: 0.01810506172478199, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7933109998703003]\n",
      "  732 [D loss: 0.02373194880783558, acc_real: 98.437500, acc_fake: 98.437500] [G loss: 0.8037046790122986]\n",
      "  733 [D loss: 0.013795962557196617, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7991479635238647]\n",
      "  734 [D loss: 0.010900304652750492, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8031352162361145]\n",
      "  735 [D loss: 0.011063219048082829, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8060088157653809]\n",
      "  736 [D loss: 0.010185950435698032, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8059072494506836]\n",
      "  737 [D loss: 0.012372898869216442, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8219306468963623]\n",
      "  738 [D loss: 0.013771284371614456, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.8137768507003784]\n",
      "  739 [D loss: 0.009769998490810394, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8131592869758606]\n",
      "  740 [D loss: 0.013758182525634766, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8117807507514954]\n",
      "  741 [D loss: 0.010900192894041538, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8130639791488647]\n",
      "  742 [D loss: 0.00876527838408947, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8165316581726074]\n",
      "  743 [D loss: 0.010036883875727654, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.8210204243659973]\n",
      "  744 [D loss: 0.0106714041903615, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8222401142120361]\n",
      "  745 [D loss: 0.011855151504278183, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.8311659097671509]\n",
      "  746 [D loss: 0.013978337869048119, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.8331355452537537]\n",
      "  747 [D loss: 0.00767056830227375, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8439648151397705]\n",
      "  748 [D loss: 0.012981705367565155, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.8426176309585571]\n",
      "  749 [D loss: 0.0074604530818760395, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8433949947357178]\n",
      "  750 [D loss: 0.006832456216216087, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8507578372955322]\n",
      "  751 [D loss: 0.016151640564203262, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.806302547454834]\n",
      "  752 [D loss: 0.014845732599496841, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.7822468280792236]\n",
      "  753 [D loss: 0.021841004490852356, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.763617217540741]\n",
      "  754 [D loss: 0.022336557507514954, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.7499977350234985]\n",
      "  755 [D loss: 0.02096208743751049, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.7624353766441345]\n",
      "  756 [D loss: 0.03324874863028526, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.7546252012252808]\n",
      "  757 [D loss: 0.01757008209824562, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.7398087382316589]\n",
      "  758 [D loss: 0.020930858328938484, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.732575535774231]\n",
      "  759 [D loss: 0.04376751556992531, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.7315784692764282]\n",
      "  760 [D loss: 0.03556709736585617, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.7367246150970459]\n",
      "  761 [D loss: 0.027678484097123146, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.7441114783287048]\n",
      "  762 [D loss: 0.026604250073432922, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.7507407665252686]\n",
      "  763 [D loss: 0.025855325162410736, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.7601600289344788]\n",
      "  764 [D loss: 0.031278517097234726, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.7665826678276062]\n",
      "  765 [D loss: 0.02139177732169628, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.7812587022781372]\n",
      "  766 [D loss: 0.023411409929394722, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.795131504535675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  767 [D loss: 0.012817955575883389, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.8014332056045532]\n",
      "  768 [D loss: 0.014355268329381943, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.8098959922790527]\n",
      "  769 [D loss: 0.025008071213960648, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.8186310529708862]\n",
      "  770 [D loss: 0.016539014875888824, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.8285491466522217]\n",
      "  771 [D loss: 0.03511718288064003, acc_real: 96.875000, acc_fake: 95.312500] [G loss: 0.8393972516059875]\n",
      "  772 [D loss: 0.020387353375554085, acc_real: 98.437500, acc_fake: 98.437500] [G loss: 0.8503599762916565]\n",
      "  773 [D loss: 0.008245273493230343, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.8514353036880493]\n",
      "  774 [D loss: 0.018954306840896606, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.8587027788162231]\n",
      "  775 [D loss: 0.015335232019424438, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.8719926476478577]\n",
      "  776 [D loss: 0.007472969125956297, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.875652551651001]\n",
      "  777 [D loss: 0.008928727358579636, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.8421586155891418]\n",
      "  778 [D loss: 0.021273022517561913, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.8075975179672241]\n",
      "  779 [D loss: 0.008951573632657528, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.7842308878898621]\n",
      "  780 [D loss: 0.04719265550374985, acc_real: 100.000000, acc_fake: 85.937500] [G loss: 0.7737014293670654]\n",
      "  781 [D loss: 0.03958196938037872, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.7643804550170898]\n",
      "  782 [D loss: 0.03590700030326843, acc_real: 100.000000, acc_fake: 89.062500] [G loss: 0.7593696117401123]\n",
      "  783 [D loss: 0.032349564135074615, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.7501007318496704]\n",
      "  784 [D loss: 0.03663451224565506, acc_real: 100.000000, acc_fake: 89.062500] [G loss: 0.7553542852401733]\n",
      "  785 [D loss: 0.020936042070388794, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.7780115008354187]\n",
      "  786 [D loss: 0.018633702769875526, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.7604133486747742]\n",
      "  787 [D loss: 0.04358783736824989, acc_real: 100.000000, acc_fake: 85.937500] [G loss: 0.7774913907051086]\n",
      "  788 [D loss: 0.02979590743780136, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.7978233098983765]\n",
      "  789 [D loss: 0.02772991918027401, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.81821608543396]\n",
      "  790 [D loss: 0.018579918891191483, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.825430691242218]\n",
      "  791 [D loss: 0.007302069570869207, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.8446670770645142]\n",
      "  792 [D loss: 0.006680096499621868, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8603324890136719]\n",
      "  793 [D loss: 0.010082671418786049, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.8732926845550537]\n",
      "  794 [D loss: 0.0027353744953870773, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8782402276992798]\n",
      "  795 [D loss: 0.0038392695132642984, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8883051872253418]\n",
      "  796 [D loss: 0.004530708305537701, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.8940129280090332]\n",
      "  797 [D loss: 0.0036037806421518326, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9003192782402039]\n",
      "  798 [D loss: 0.0012356507359072566, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8979358673095703]\n",
      "  799 [D loss: 0.002331957221031189, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8960885405540466]\n",
      "  800 [D loss: 0.00213579717092216, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9086854457855225]\n",
      "  801 [D loss: 0.002879692707210779, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9049638509750366]\n",
      "  802 [D loss: 0.0012248081620782614, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9146707653999329]\n",
      "  803 [D loss: 0.0010985115077346563, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9186037182807922]\n",
      "  804 [D loss: 0.0015514884144067764, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9247372150421143]\n",
      "  805 [D loss: 0.0011933973291888833, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9210786819458008]\n",
      "  806 [D loss: 0.0014717604499310255, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9212921857833862]\n",
      "  807 [D loss: 0.0008763468940742314, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9206913709640503]\n",
      "  808 [D loss: 0.001312492648139596, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9262603521347046]\n",
      "  809 [D loss: 0.0009006301988847554, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9232347011566162]\n",
      "  810 [D loss: 0.0016483197687193751, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9245006442070007]\n",
      "  811 [D loss: 0.001998370746150613, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.926333487033844]\n",
      "  812 [D loss: 0.0021601286716759205, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9237987995147705]\n",
      "  813 [D loss: 0.002365764928981662, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9266152381896973]\n",
      "  814 [D loss: 0.0015927322674542665, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9268335103988647]\n",
      "  815 [D loss: 0.0021855595987290144, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9289565086364746]\n",
      "  816 [D loss: 0.0006100209429860115, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9282580614089966]\n",
      "  817 [D loss: 0.0012209860142320395, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.925017774105072]\n",
      "  818 [D loss: 0.0020942343398928642, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.926498532295227]\n",
      "  819 [D loss: 0.0035883146338164806, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9261821508407593]\n",
      "  820 [D loss: 0.004830824676901102, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9254352450370789]\n",
      "  821 [D loss: 0.002991723595187068, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9234233498573303]\n",
      "  822 [D loss: 0.003128785640001297, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9267101883888245]\n",
      "  823 [D loss: 0.0009781757835298777, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9272545576095581]\n",
      "  824 [D loss: 0.0031700357794761658, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9292910695075989]\n",
      "  825 [D loss: 0.0009929101215675473, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9258545637130737]\n",
      "  826 [D loss: 0.0014842128148302436, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9295684695243835]\n",
      "  827 [D loss: 0.0013081193901598454, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.92966628074646]\n",
      "  828 [D loss: 0.001493660849519074, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9250975847244263]\n",
      "  829 [D loss: 0.0027137657161802053, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9306368231773376]\n",
      "  830 [D loss: 0.0027545196935534477, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9266881942749023]\n",
      "  831 [D loss: 0.0008013880578801036, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9276398420333862]\n",
      "  832 [D loss: 0.0017177744302898645, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9294240474700928]\n",
      "  833 [D loss: 0.0007078631897456944, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9264774322509766]\n",
      "  834 [D loss: 0.0007381020695902407, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9238623380661011]\n",
      "  835 [D loss: 0.0029043229296803474, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9270080327987671]\n",
      "  836 [D loss: 0.0024311279412359, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9294818639755249]\n",
      "  837 [D loss: 0.0015242279041558504, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9269878268241882]\n",
      "  838 [D loss: 0.002066094661131501, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9268553853034973]\n",
      "  839 [D loss: 0.0024164882488548756, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9374912977218628]\n",
      "  840 [D loss: 0.0016649400349706411, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9306545853614807]\n",
      "  841 [D loss: 0.0015995207941159606, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.928641140460968]\n",
      "  842 [D loss: 0.0016825401689857244, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9286448955535889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  843 [D loss: 0.002717970870435238, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9293065667152405]\n",
      "  844 [D loss: 0.0015271535376086831, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9322411417961121]\n",
      "  845 [D loss: 0.001686408999375999, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.932280957698822]\n",
      "  846 [D loss: 0.0018802107078954577, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9326843023300171]\n",
      "  847 [D loss: 0.005116589367389679, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9358076453208923]\n",
      "  848 [D loss: 0.003724402980878949, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9358624815940857]\n",
      "  849 [D loss: 0.0010501934448257089, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9365376830101013]\n",
      "  850 [D loss: 0.0020831269212067127, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9408667087554932]\n",
      "  851 [D loss: 0.0004026932001579553, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9403759241104126]\n",
      "  852 [D loss: 0.0005367299309000373, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.941053032875061]\n",
      "  853 [D loss: 0.0017838209168985486, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9416369199752808]\n",
      "  854 [D loss: 0.0018316707573831081, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9428846836090088]\n",
      "  855 [D loss: 0.0004003129724878818, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.943134605884552]\n",
      "  856 [D loss: 0.0015727817080914974, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9459362626075745]\n",
      "  857 [D loss: 0.003560014069080353, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9440509080886841]\n",
      "  858 [D loss: 0.0003209596616216004, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9397484660148621]\n",
      "  859 [D loss: 0.0009167003445327282, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9411563277244568]\n",
      "  860 [D loss: 0.002856319071725011, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9394947290420532]\n",
      "  861 [D loss: 0.001970223616808653, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9356465339660645]\n",
      "  862 [D loss: 0.002176757901906967, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9368740916252136]\n",
      "  863 [D loss: 0.0007646812591701746, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9377468228340149]\n",
      "  864 [D loss: 0.00032945521525107324, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9369577765464783]\n",
      "  865 [D loss: 0.0005087480531074107, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9362922310829163]\n",
      "  866 [D loss: 0.001618851674720645, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.939099907875061]\n",
      "  867 [D loss: 0.0017468743026256561, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9391782283782959]\n",
      "  868 [D loss: 0.0012741070240736008, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9378253221511841]\n",
      "  869 [D loss: 0.0024716623593121767, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9389644861221313]\n",
      "  870 [D loss: 0.0008118518744595349, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.941180408000946]\n",
      "  871 [D loss: 0.0008077443344518542, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9405885934829712]\n",
      "  872 [D loss: 0.0009232601732946932, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9417546987533569]\n",
      "  873 [D loss: 0.0005864384584128857, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9428272843360901]\n",
      "  874 [D loss: 0.0010033033322542906, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9437023401260376]\n",
      "  875 [D loss: 0.0023890265729278326, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9450216293334961]\n",
      "  876 [D loss: 0.0010853945277631283, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9462556838989258]\n",
      "  877 [D loss: 0.0006720145465806127, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9500095844268799]\n",
      "  878 [D loss: 0.0003164644294884056, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9486067891120911]\n",
      "  879 [D loss: 0.00060100550763309, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.949835479259491]\n",
      "  880 [D loss: 0.0007454763981513679, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9488916397094727]\n",
      "  881 [D loss: 0.0007596053183078766, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.949409008026123]\n",
      "  882 [D loss: 0.0011633054818958044, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9501208662986755]\n",
      "  883 [D loss: 0.00042050256161019206, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9478579759597778]\n",
      "  884 [D loss: 0.0003895641420967877, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.948830246925354]\n",
      "  885 [D loss: 0.0007072475855238736, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.946660041809082]\n",
      "  886 [D loss: 0.0003076453576795757, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9481703042984009]\n",
      "  887 [D loss: 0.0008146569598466158, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9458730816841125]\n",
      "  888 [D loss: 0.0007936699548736215, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9473682045936584]\n",
      "  889 [D loss: 0.000678060227073729, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.945879340171814]\n",
      "  890 [D loss: 0.0007508487324230373, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9461450576782227]\n",
      "  891 [D loss: 0.0006147876265458763, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9466987252235413]\n",
      "  892 [D loss: 0.0005574184469878674, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9461053013801575]\n",
      "  893 [D loss: 0.000642582424916327, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9469955563545227]\n",
      "  894 [D loss: 0.0004340826708357781, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9465911388397217]\n",
      "  895 [D loss: 0.00028415321139618754, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9475293159484863]\n",
      "  896 [D loss: 0.0006670975126326084, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9479896426200867]\n",
      "  897 [D loss: 0.0028049652464687824, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9481250643730164]\n",
      "  898 [D loss: 0.0002640967722982168, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9488762617111206]\n",
      "  899 [D loss: 0.0004963215324096382, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9497830271720886]\n",
      "  900 [D loss: 0.00043462764006108046, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9505982995033264]\n",
      "  901 [D loss: 0.00046591408317908645, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9509996771812439]\n",
      "  902 [D loss: 0.003521040314808488, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9502949714660645]\n",
      "  903 [D loss: 0.0028026876971125603, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9497814774513245]\n",
      "  904 [D loss: 0.0005569608183577657, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9502833485603333]\n",
      "  905 [D loss: 0.000510993879288435, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9498759508132935]\n",
      "  906 [D loss: 0.0003563525096978992, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9498935341835022]\n",
      "  907 [D loss: 0.00043519504833966494, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9511325359344482]\n",
      "  908 [D loss: 0.0005527477478608489, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9506672024726868]\n",
      "  909 [D loss: 0.0026877839118242264, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9491740465164185]\n",
      "  910 [D loss: 0.00047728928620927036, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9482563734054565]\n",
      "  911 [D loss: 0.0006235457258298993, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9470659494400024]\n",
      "  912 [D loss: 0.0004771968233399093, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9474803805351257]\n",
      "  913 [D loss: 0.00046499306336045265, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9463291764259338]\n",
      "  914 [D loss: 0.001537746167741716, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9470243453979492]\n",
      "  915 [D loss: 0.0005133808008395135, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9503357410430908]\n",
      "  916 [D loss: 0.0006833060760982335, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9468052983283997]\n",
      "  917 [D loss: 0.0007237214013002813, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9501060247421265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  918 [D loss: 0.0004159283125773072, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9487437605857849]\n",
      "  919 [D loss: 0.004687284119427204, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9448055624961853]\n",
      "  920 [D loss: 0.0009162419592030346, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9419386982917786]\n",
      "  921 [D loss: 0.0006772050983272493, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9399691820144653]\n",
      "  922 [D loss: 0.0006612071883864701, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9395304918289185]\n",
      "  923 [D loss: 0.0014961952110752463, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9371286630630493]\n",
      "  924 [D loss: 0.000885527057107538, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.936959445476532]\n",
      "  925 [D loss: 0.0009498216677457094, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9375244379043579]\n",
      "  926 [D loss: 0.0026005434338003397, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9369167685508728]\n",
      "  927 [D loss: 0.0008552142535336316, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9399552345275879]\n",
      "  928 [D loss: 0.0008568516932427883, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9378782510757446]\n",
      "  929 [D loss: 0.0010312668746337295, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9384267330169678]\n",
      "  930 [D loss: 0.0011737784370779991, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9389901161193848]\n",
      "  931 [D loss: 0.002888699993491173, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9401498436927795]\n",
      "  932 [D loss: 0.001176200108602643, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9447590708732605]\n",
      "  933 [D loss: 0.000591004267334938, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9441980719566345]\n",
      "  934 [D loss: 0.0006116187432780862, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9446303248405457]\n",
      "  935 [D loss: 0.0006532911793328822, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.945515513420105]\n",
      "  936 [D loss: 0.000578662904445082, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9467542767524719]\n",
      "  937 [D loss: 0.0008107607718557119, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9470871686935425]\n",
      "  938 [D loss: 0.004911299329251051, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9468567371368408]\n",
      "  939 [D loss: 0.0008264086209237576, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9483135938644409]\n",
      "  940 [D loss: 0.0006775798974558711, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9453868865966797]\n",
      "  941 [D loss: 0.0003799049009103328, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9460474848747253]\n",
      "  942 [D loss: 0.0013530373107641935, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9458645582199097]\n",
      "  943 [D loss: 0.0005296744639053941, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9465147256851196]\n",
      "  944 [D loss: 0.00039906305028125644, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.946755588054657]\n",
      "  945 [D loss: 0.00038974941708147526, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9469343423843384]\n",
      "  946 [D loss: 0.0009659518836997449, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9458903074264526]\n",
      "  947 [D loss: 0.0004920570063404739, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.946121096611023]\n",
      "  948 [D loss: 0.00044409476686269045, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9459795951843262]\n",
      "  949 [D loss: 0.002773017156869173, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9467475414276123]\n",
      "  950 [D loss: 0.0004995440831407905, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9468827247619629]\n",
      "  951 [D loss: 0.0007259926060214639, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9460921287536621]\n",
      "  952 [D loss: 0.0005271380068734288, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9464210271835327]\n",
      "  953 [D loss: 0.0005269640241749585, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9467423558235168]\n",
      "  954 [D loss: 0.00041940773371607065, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9472011923789978]\n",
      "  955 [D loss: 0.0006493822438642383, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9476423263549805]\n",
      "  956 [D loss: 0.0009774081408977509, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9479802846908569]\n",
      "  957 [D loss: 0.0006084362394176424, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9481865763664246]\n",
      "  958 [D loss: 0.0003682071983348578, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9492735862731934]\n",
      "  959 [D loss: 0.0005641595926135778, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9492967128753662]\n",
      "  960 [D loss: 0.0008882712572813034, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9494702816009521]\n",
      "  961 [D loss: 0.0003638506168499589, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9543794989585876]\n",
      "  962 [D loss: 0.0005434781778603792, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9522107839584351]\n",
      "  963 [D loss: 0.0004243343137204647, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9504858255386353]\n",
      "  964 [D loss: 0.0004269236815162003, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.951874852180481]\n",
      "  965 [D loss: 0.0007299627177417278, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9499567747116089]\n",
      "  966 [D loss: 0.0007223679567687213, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9502105712890625]\n",
      "  967 [D loss: 0.0005296898307278752, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9494572281837463]\n",
      "  968 [D loss: 0.0007463257061317563, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9483737945556641]\n",
      "  969 [D loss: 0.0005660697934217751, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9496772289276123]\n",
      "  970 [D loss: 0.0005674781277775764, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9500615000724792]\n",
      "  971 [D loss: 0.0005369068239815533, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9497894048690796]\n",
      "  972 [D loss: 0.0003826721804216504, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9487705230712891]\n",
      "  973 [D loss: 0.00047424252261407673, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9482440948486328]\n",
      "  974 [D loss: 0.0004870354023296386, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9487890601158142]\n",
      "  975 [D loss: 0.0006516387802548707, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.94819575548172]\n",
      "  976 [D loss: 0.0005707521922886372, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9508680701255798]\n",
      "  977 [D loss: 0.00046471296809613705, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.947363018989563]\n",
      "  978 [D loss: 0.0006875079707242548, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9476202130317688]\n",
      "  979 [D loss: 0.00038899440551176667, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9486929774284363]\n",
      "  980 [D loss: 0.0004231477505527437, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9485940337181091]\n",
      "  981 [D loss: 0.0004190690815448761, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.949057936668396]\n",
      "  982 [D loss: 0.0005812339368276298, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9499980807304382]\n",
      "  983 [D loss: 0.00048658999730832875, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9489196538925171]\n",
      "  984 [D loss: 0.0003895973786711693, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9489963054656982]\n",
      "  985 [D loss: 0.0004412151174619794, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9495599269866943]\n",
      "  986 [D loss: 0.0006005119066685438, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9478722810745239]\n",
      "  987 [D loss: 0.005512306001037359, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9446991682052612]\n",
      "  988 [D loss: 0.000640341080725193, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9431159496307373]\n",
      "  989 [D loss: 0.0004078022320754826, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9413542151451111]\n",
      "  990 [D loss: 0.0007780069136060774, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.939969539642334]\n",
      "  991 [D loss: 0.0006488302024081349, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9377173185348511]\n",
      "  992 [D loss: 0.0007045576348900795, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9362273216247559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  993 [D loss: 0.0006222051451914012, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9357753396034241]\n",
      "  994 [D loss: 0.0007012570858933032, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9409499168395996]\n",
      "  995 [D loss: 0.0007600147509947419, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9387877583503723]\n",
      "  996 [D loss: 0.0008933606441132724, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9355335831642151]\n",
      "  997 [D loss: 0.0008948779432103038, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9345259666442871]\n",
      "  998 [D loss: 0.0006080272141844034, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9362719655036926]\n",
      "  999 [D loss: 0.0006744143320247531, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9384579658508301]\n",
      " 1000 [D loss: 0.001012041000649333, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9353492259979248]\n",
      " 1001 [D loss: 0.0009751582983881235, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9362815618515015]\n",
      " 1002 [D loss: 0.0014434813056141138, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9361433386802673]\n",
      " 1003 [D loss: 0.0005558832199312747, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9363056421279907]\n",
      " 1004 [D loss: 0.004982195794582367, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9382385015487671]\n",
      " 1005 [D loss: 0.0014165083412081003, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9364577531814575]\n",
      " 1006 [D loss: 0.0004943364183418453, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9338138103485107]\n",
      " 1007 [D loss: 0.0008963878499343991, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9342880249023438]\n",
      " 1008 [D loss: 0.0013345853658393025, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9342091679573059]\n",
      " 1009 [D loss: 0.0009184306836687028, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9366369247436523]\n",
      " 1010 [D loss: 0.001976774772629142, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9349895715713501]\n",
      " 1011 [D loss: 0.0020603486336767673, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9375835657119751]\n",
      " 1012 [D loss: 0.003713572397828102, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9346532821655273]\n",
      " 1013 [D loss: 0.001196366036310792, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9353315830230713]\n",
      " 1014 [D loss: 0.0012624715454876423, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9350176453590393]\n",
      " 1015 [D loss: 0.0008891966426745057, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9374900460243225]\n",
      " 1016 [D loss: 0.0006317811785265803, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9351294636726379]\n",
      " 1017 [D loss: 0.0007061599753797054, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9358184337615967]\n",
      " 1018 [D loss: 0.0014963129069656134, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9355592131614685]\n",
      " 1019 [D loss: 0.0010403316700831056, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9361375570297241]\n",
      " 1020 [D loss: 0.0032224529422819614, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9376534223556519]\n",
      " 1021 [D loss: 0.0028407787904143333, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9361019134521484]\n",
      " 1022 [D loss: 0.001596999354660511, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.930760383605957]\n",
      " 1023 [D loss: 0.001327763544395566, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.930979311466217]\n",
      " 1024 [D loss: 0.0026479086373001337, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9354636669158936]\n",
      " 1025 [D loss: 0.000894956523552537, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9313166737556458]\n",
      " 1026 [D loss: 0.0009685063268989325, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9308769702911377]\n",
      " 1027 [D loss: 0.0010670899646356702, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9318282604217529]\n",
      " 1028 [D loss: 0.000996837974525988, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9334856867790222]\n",
      " 1029 [D loss: 0.000656510004773736, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9320535659790039]\n",
      " 1030 [D loss: 0.002004556590691209, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9328929781913757]\n",
      " 1031 [D loss: 0.0009394127409905195, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9334204196929932]\n",
      " 1032 [D loss: 0.0007412878330796957, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9347819685935974]\n",
      " 1033 [D loss: 0.0009285826236009598, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9368460178375244]\n",
      " 1034 [D loss: 0.0014273643027991056, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9356675744056702]\n",
      " 1035 [D loss: 0.0006923549226485193, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9361598491668701]\n",
      " 1036 [D loss: 0.0007359808660112321, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.940277099609375]\n",
      " 1037 [D loss: 0.0007066261605359614, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9373675584793091]\n",
      " 1038 [D loss: 0.00047267432091757655, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9380815625190735]\n",
      " 1039 [D loss: 0.0008085230365395546, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9428835511207581]\n",
      " 1040 [D loss: 0.0006714739720337093, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9396663904190063]\n",
      " 1041 [D loss: 0.00858313962817192, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.94000643491745]\n",
      " 1042 [D loss: 0.0010010501137003303, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9414865374565125]\n",
      " 1043 [D loss: 0.000528119970113039, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9417215585708618]\n",
      " 1044 [D loss: 0.0006596186431124806, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9424006342887878]\n",
      " 1045 [D loss: 0.004982360173016787, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9361202716827393]\n",
      " 1046 [D loss: 0.0012662718072533607, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9346134662628174]\n",
      " 1047 [D loss: 0.0009468024363741279, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9313206672668457]\n",
      " 1048 [D loss: 0.0010364435147494078, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9292343854904175]\n",
      " 1049 [D loss: 0.0014117979444563389, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9312096238136292]\n",
      " 1050 [D loss: 0.001722422195598483, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9234699010848999]\n",
      " 1051 [D loss: 0.0018520000157877803, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9284976720809937]\n",
      " 1052 [D loss: 0.001550736604258418, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9276988506317139]\n",
      " 1053 [D loss: 0.001443054061383009, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9233804941177368]\n",
      " 1054 [D loss: 0.001085990690626204, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9251495599746704]\n",
      " 1055 [D loss: 0.0008131630602292717, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.925807774066925]\n",
      " 1056 [D loss: 0.0017326251836493611, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9274676442146301]\n",
      " 1057 [D loss: 0.000690553046297282, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9271931052207947]\n",
      " 1058 [D loss: 0.0012921098386868834, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9286211133003235]\n",
      " 1059 [D loss: 0.0009434106759727001, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9285624027252197]\n",
      " 1060 [D loss: 0.0017583452863618731, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9296534061431885]\n",
      " 1061 [D loss: 0.002562758279964328, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9320167303085327]\n",
      " 1062 [D loss: 0.0013140083756297827, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9331190586090088]\n",
      " 1063 [D loss: 0.0009487847564741969, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9313424825668335]\n",
      " 1064 [D loss: 0.0010335963452234864, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9333223104476929]\n",
      " 1065 [D loss: 0.0011846034321933985, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9340513944625854]\n",
      " 1066 [D loss: 0.0010453563882037997, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9351387023925781]\n",
      " 1067 [D loss: 0.0008710807305760682, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9410513639450073]\n",
      " 1068 [D loss: 0.0016298863338306546, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9404134154319763]\n",
      " 1069 [D loss: 0.0008295160951092839, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.937353789806366]\n",
      " 1070 [D loss: 0.0011875568889081478, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9387999773025513]\n",
      " 1071 [D loss: 0.005675447639077902, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9373000264167786]\n",
      " 1072 [D loss: 0.0006585599621757865, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.941914439201355]\n",
      " 1073 [D loss: 0.0009165970841422677, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9455099105834961]\n",
      " 1074 [D loss: 0.0007363017066381872, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9408090114593506]\n",
      " 1075 [D loss: 0.000834539532661438, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9433544278144836]\n",
      " 1076 [D loss: 0.0008811665466055274, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9375683665275574]\n",
      " 1077 [D loss: 0.0009547547670081258, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9379457235336304]\n",
      " 1078 [D loss: 0.001381239271722734, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9380412697792053]\n",
      " 1079 [D loss: 0.0013395390706136823, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9384678602218628]\n",
      " 1080 [D loss: 0.0016710978234186769, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9385865926742554]\n",
      " 1081 [D loss: 0.0044115809723734856, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9315363764762878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1082 [D loss: 0.0011907049920409918, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.923414945602417]\n",
      " 1083 [D loss: 0.0012031241785734892, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9192017316818237]\n",
      " 1084 [D loss: 0.0019561154767870903, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9138921499252319]\n",
      " 1085 [D loss: 0.0017428046558052301, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9085621237754822]\n",
      " 1086 [D loss: 0.0011833923636004329, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9083178639411926]\n",
      " 1087 [D loss: 0.0023956340737640858, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9031705856323242]\n",
      " 1088 [D loss: 0.0011982907308265567, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.903691291809082]\n",
      " 1089 [D loss: 0.003155600279569626, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9037420749664307]\n",
      " 1090 [D loss: 0.006565403193235397, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9021046757698059]\n",
      " 1091 [D loss: 0.005850981455296278, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9049875736236572]\n",
      " 1092 [D loss: 0.006425755098462105, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9094350337982178]\n",
      " 1093 [D loss: 0.0012941757449880242, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9148613214492798]\n",
      " 1094 [D loss: 0.0021705448161810637, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9216744899749756]\n",
      " 1095 [D loss: 0.0019171136664226651, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9208403825759888]\n",
      " 1096 [D loss: 0.0011642588069662452, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9257810711860657]\n",
      " 1097 [D loss: 0.001368447788991034, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9272049069404602]\n",
      " 1098 [D loss: 0.001731096301227808, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9286932945251465]\n",
      " 1099 [D loss: 0.0017597554251551628, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9302717447280884]\n",
      " 1100 [D loss: 0.0007351752137765288, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9353559017181396]\n",
      " 1101 [D loss: 0.0012088625226169825, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9402585029602051]\n",
      " 1102 [D loss: 0.0010742073645815253, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9384036660194397]\n",
      " 1103 [D loss: 0.0007857385789975524, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9384993314743042]\n",
      " 1104 [D loss: 0.00048015351057983935, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9394114017486572]\n",
      " 1105 [D loss: 0.0006780194817110896, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9400219321250916]\n",
      " 1106 [D loss: 0.0007336553535424173, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9379270672798157]\n",
      " 1107 [D loss: 0.000552913174033165, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9377790689468384]\n",
      " 1108 [D loss: 0.0013819773448631167, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.942247748374939]\n",
      " 1109 [D loss: 0.0009247492998838425, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9420660734176636]\n",
      " 1110 [D loss: 0.0024054443929344416, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9341387748718262]\n",
      " 1111 [D loss: 0.0015085064806044102, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9297701120376587]\n",
      " 1112 [D loss: 0.0018976738210767508, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9227005243301392]\n",
      " 1113 [D loss: 0.0032333512790501118, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9207456707954407]\n",
      " 1114 [D loss: 0.0012469965731725097, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9174484014511108]\n",
      " 1115 [D loss: 0.0015655598836019635, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9160941243171692]\n",
      " 1116 [D loss: 0.0052635460160672665, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9166234135627747]\n",
      " 1117 [D loss: 0.002791415434330702, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9167300462722778]\n",
      " 1118 [D loss: 0.0020873236935585737, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9196246266365051]\n",
      " 1119 [D loss: 0.002216259716078639, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9200484156608582]\n",
      " 1120 [D loss: 0.0018181249033659697, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9241405725479126]\n",
      " 1121 [D loss: 0.003017027862370014, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9268779754638672]\n",
      " 1122 [D loss: 0.00147114391438663, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9277065396308899]\n",
      " 1123 [D loss: 0.002760637551546097, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9296348094940186]\n",
      " 1124 [D loss: 0.001200859434902668, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9346452951431274]\n",
      " 1125 [D loss: 0.0008169090724550188, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9331245422363281]\n",
      " 1126 [D loss: 0.001749320887029171, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9376745820045471]\n",
      " 1127 [D loss: 0.0014818005729466677, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9391865134239197]\n",
      " 1128 [D loss: 0.001023957272991538, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9401638507843018]\n",
      " 1129 [D loss: 0.0009791632182896137, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.941275954246521]\n",
      " 1130 [D loss: 0.0006238784990273416, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9433833360671997]\n",
      " 1131 [D loss: 0.0015762705588713288, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9443349838256836]\n",
      " 1132 [D loss: 0.0003701323294080794, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9431164264678955]\n",
      " 1133 [D loss: 0.008470281027257442, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9428268074989319]\n",
      " 1134 [D loss: 0.0006983148632571101, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9400308132171631]\n",
      " 1135 [D loss: 0.0008535524248145521, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9374237060546875]\n",
      " 1136 [D loss: 0.0010307953925803304, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9390707015991211]\n",
      " 1137 [D loss: 0.0012591209961101413, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9367269277572632]\n",
      " 1138 [D loss: 0.0012476996053010225, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9340526461601257]\n",
      " 1139 [D loss: 0.002163016004487872, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.931525468826294]\n",
      " 1140 [D loss: 0.0010384400375187397, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9342529773712158]\n",
      " 1141 [D loss: 0.002134411595761776, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9337150454521179]\n",
      " 1142 [D loss: 0.0022744901943951845, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9297893047332764]\n",
      " 1143 [D loss: 0.0006952129187993705, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9341769218444824]\n",
      " 1144 [D loss: 0.0013914619339630008, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9337130784988403]\n",
      " 1145 [D loss: 0.0008450446766801178, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9357843995094299]\n",
      " 1146 [D loss: 0.0011480507673695683, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9356438517570496]\n",
      " 1147 [D loss: 0.0008173717651516199, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.933597207069397]\n",
      " 1148 [D loss: 0.0006944680353626609, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9393492341041565]\n",
      " 1149 [D loss: 0.0010995900956913829, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9357333183288574]\n",
      " 1150 [D loss: 0.0019680396653711796, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9412146806716919]\n",
      " 1151 [D loss: 0.0010964235989376903, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9374299049377441]\n",
      " 1152 [D loss: 0.001039248425513506, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9416074752807617]\n",
      " 1153 [D loss: 0.0008851700695231557, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9399713277816772]\n",
      " 1154 [D loss: 0.001034264569170773, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9438027739524841]\n",
      " 1155 [D loss: 0.0013507870025932789, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9383680820465088]\n",
      " 1156 [D loss: 0.0015022826846688986, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9363024830818176]\n",
      " 1157 [D loss: 0.0011064601130783558, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9367573261260986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1158 [D loss: 0.0015726330457255244, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9346013069152832]\n",
      " 1159 [D loss: 0.0015035592950880527, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9344799518585205]\n",
      " 1160 [D loss: 0.0007235661614686251, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9330116510391235]\n",
      " 1161 [D loss: 0.0016951420111581683, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9330434799194336]\n",
      " 1162 [D loss: 0.0011422266252338886, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9324160218238831]\n",
      " 1163 [D loss: 0.002057186560705304, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9304242134094238]\n",
      " 1164 [D loss: 0.0010736570693552494, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9319871664047241]\n",
      " 1165 [D loss: 0.002268888521939516, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9335023164749146]\n",
      " 1166 [D loss: 0.0015397118404507637, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9330241680145264]\n",
      " 1167 [D loss: 0.0009448046330362558, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9376411437988281]\n",
      " 1168 [D loss: 0.0010341082233935595, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9390729069709778]\n",
      " 1169 [D loss: 0.0010183198610320687, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9349318742752075]\n",
      " 1170 [D loss: 0.000593058648519218, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9363192915916443]\n",
      " 1171 [D loss: 0.0076934462413191795, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9370018243789673]\n",
      " 1172 [D loss: 0.0016741222934797406, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9387364387512207]\n",
      " 1173 [D loss: 0.0016694397199898958, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9393799304962158]\n",
      " 1174 [D loss: 0.0009529195376671851, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9391985535621643]\n",
      " 1175 [D loss: 0.0012369059259071946, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9383618235588074]\n",
      " 1176 [D loss: 0.000525436014868319, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9372742772102356]\n",
      " 1177 [D loss: 0.0019232983468100429, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9372118711471558]\n",
      " 1178 [D loss: 0.0010029224213212729, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9366047978401184]\n",
      " 1179 [D loss: 0.0010115468176081777, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9368329048156738]\n",
      " 1180 [D loss: 0.0007871135603636503, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9375274181365967]\n",
      " 1181 [D loss: 0.001374733168631792, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9355188012123108]\n",
      " 1182 [D loss: 0.001235417672432959, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9421725273132324]\n",
      " 1183 [D loss: 0.002222325187176466, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9366459846496582]\n",
      " 1184 [D loss: 0.0020020734518766403, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9413290023803711]\n",
      " 1185 [D loss: 0.0009154328727163374, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9425786733627319]\n",
      " 1186 [D loss: 0.0014193208189681172, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9401862025260925]\n",
      " 1187 [D loss: 0.004979186225682497, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9429107308387756]\n",
      " 1188 [D loss: 0.0015125761274248362, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.937210738658905]\n",
      " 1189 [D loss: 0.0010542094241827726, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9368757009506226]\n",
      " 1190 [D loss: 0.0017655177507549524, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9342367649078369]\n",
      " 1191 [D loss: 0.0025646144058555365, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9334484338760376]\n",
      " 1192 [D loss: 0.0014167202170938253, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9346755743026733]\n",
      " 1193 [D loss: 0.0010328097268939018, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9385279417037964]\n",
      " 1194 [D loss: 0.0012293779291212559, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9369339346885681]\n",
      " 1195 [D loss: 0.003691385965794325, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9386686086654663]\n",
      " 1196 [D loss: 0.0015156363369897008, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.942977249622345]\n",
      " 1197 [D loss: 0.001453424571081996, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9476591348648071]\n",
      " 1198 [D loss: 0.0010772289242595434, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9453535676002502]\n",
      " 1199 [D loss: 0.0010769914370030165, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.949565589427948]\n",
      " 1200 [D loss: 0.004283905494958162, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9458596706390381]\n",
      " 1201 [D loss: 0.0039195227436721325, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9390503764152527]\n",
      " 1202 [D loss: 0.0012703731190413237, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9363374710083008]\n",
      " 1203 [D loss: 0.0011804394889622927, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9316312074661255]\n",
      " 1204 [D loss: 0.0007057493203319609, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9298699498176575]\n",
      " 1205 [D loss: 0.0021715660113841295, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9310343861579895]\n",
      " 1206 [D loss: 0.0012070988304913044, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9323314428329468]\n",
      " 1207 [D loss: 0.0021984896156936884, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9305369853973389]\n",
      " 1208 [D loss: 0.0028811378870159388, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9357283115386963]\n",
      " 1209 [D loss: 0.0010915803723037243, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9349960684776306]\n",
      " 1210 [D loss: 0.001293388195335865, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.936052143573761]\n",
      " 1211 [D loss: 0.002248437376692891, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9391340613365173]\n",
      " 1212 [D loss: 0.0011006201384589076, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9411530494689941]\n",
      " 1213 [D loss: 0.0005769809358753264, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9442316889762878]\n",
      " 1214 [D loss: 0.00200078496709466, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9441007375717163]\n",
      " 1215 [D loss: 0.00059613271150738, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9466415047645569]\n",
      " 1216 [D loss: 0.0012088313233107328, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9501422643661499]\n",
      " 1217 [D loss: 0.004226831719279289, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9375261068344116]\n",
      " 1218 [D loss: 0.0007470363634638488, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.927220344543457]\n",
      " 1219 [D loss: 0.0035240878351032734, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.920249879360199]\n",
      " 1220 [D loss: 0.0033147267531603575, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9126244783401489]\n",
      " 1221 [D loss: 0.00830007903277874, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.9098419547080994]\n",
      " 1222 [D loss: 0.0018065066542476416, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9106950163841248]\n",
      " 1223 [D loss: 0.0042184186168015, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9098533987998962]\n",
      " 1224 [D loss: 0.0021670740097761154, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9122211337089539]\n",
      " 1225 [D loss: 0.0071134306490421295, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9176390171051025]\n",
      " 1226 [D loss: 0.0021798103116452694, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9216010570526123]\n",
      " 1227 [D loss: 0.0010608541779220104, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9222074747085571]\n",
      " 1228 [D loss: 0.002793668769299984, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9230709671974182]\n",
      " 1229 [D loss: 0.0009028278291225433, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9310551881790161]\n",
      " 1230 [D loss: 0.002147223800420761, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9318795800209045]\n",
      " 1231 [D loss: 0.002063014544546604, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9375698566436768]\n",
      " 1232 [D loss: 0.0028217073995620012, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9372977018356323]\n",
      " 1233 [D loss: 0.002466444857418537, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9401938915252686]\n",
      " 1234 [D loss: 0.0005726715316995978, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9428830742835999]\n",
      " 1235 [D loss: 0.0006305476417765021, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.945661187171936]\n",
      " 1236 [D loss: 0.003829499240964651, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9395154714584351]\n",
      " 1237 [D loss: 0.0006956405704841018, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9349597096443176]\n",
      " 1238 [D loss: 0.0012866649776697159, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9305140376091003]\n",
      " 1239 [D loss: 0.00193795224186033, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9271731376647949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1240 [D loss: 0.005322518292814493, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.918461799621582]\n",
      " 1241 [D loss: 0.002162956167012453, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9175271391868591]\n",
      " 1242 [D loss: 0.004121585749089718, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9149131178855896]\n",
      " 1243 [D loss: 0.0061410400085151196, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9122188687324524]\n",
      " 1244 [D loss: 0.004131040070205927, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9167265892028809]\n",
      " 1245 [D loss: 0.003826017724350095, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9156339168548584]\n",
      " 1246 [D loss: 0.002014660043641925, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9230818748474121]\n",
      " 1247 [D loss: 0.001378942746669054, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9208624362945557]\n",
      " 1248 [D loss: 0.0016812952235341072, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9235619902610779]\n",
      " 1249 [D loss: 0.0013618679950013757, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9324051141738892]\n",
      " 1250 [D loss: 0.002908267080783844, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9271077513694763]\n",
      " 1251 [D loss: 0.002116808434948325, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9318392872810364]\n",
      " 1252 [D loss: 0.0006255133775994182, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9348298907279968]\n",
      " 1253 [D loss: 0.0023534793872386217, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9404522180557251]\n",
      " 1254 [D loss: 0.001408819924108684, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9507230520248413]\n",
      " 1255 [D loss: 0.0014128198381513357, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.941961944103241]\n",
      " 1256 [D loss: 0.0005264012143015862, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.945884108543396]\n",
      " 1257 [D loss: 0.0008668278460390866, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9463965892791748]\n",
      " 1258 [D loss: 0.00165711366571486, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9482235908508301]\n",
      " 1259 [D loss: 0.000564211281016469, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9450324773788452]\n",
      " 1260 [D loss: 0.0009494583937339485, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9428802132606506]\n",
      " 1261 [D loss: 0.0005436296341940761, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9451982975006104]\n",
      " 1262 [D loss: 0.0006101512117311358, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9460302591323853]\n",
      " 1263 [D loss: 0.0012547974474728107, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9457293748855591]\n",
      " 1264 [D loss: 0.002244198927655816, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9453874826431274]\n",
      " 1265 [D loss: 0.0010022663045674562, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9477812051773071]\n",
      " 1266 [D loss: 0.0005543162114918232, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9485958218574524]\n",
      " 1267 [D loss: 0.00038501154631376266, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9492769837379456]\n",
      " 1268 [D loss: 0.0010774070397019386, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9488702416419983]\n",
      " 1269 [D loss: 0.0003168658586218953, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9507735371589661]\n",
      " 1270 [D loss: 0.007195371203124523, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9511722922325134]\n",
      " 1271 [D loss: 0.0021403192076832056, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9479423761367798]\n",
      " 1272 [D loss: 0.0008403861429542303, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9548417329788208]\n",
      " 1273 [D loss: 0.0004845988587476313, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9522923827171326]\n",
      " 1274 [D loss: 0.000647540669888258, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9452918767929077]\n",
      " 1275 [D loss: 0.001035052351653576, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9462110996246338]\n",
      " 1276 [D loss: 0.0011438645888119936, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9466533660888672]\n",
      " 1277 [D loss: 0.0006133189308457077, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9466916918754578]\n",
      " 1278 [D loss: 0.0007320584263652563, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9468017816543579]\n",
      " 1279 [D loss: 0.0008014398044906557, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9473700523376465]\n",
      " 1280 [D loss: 0.0007048505358397961, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9504107236862183]\n",
      " 1281 [D loss: 0.0006710714078508317, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9502387046813965]\n",
      " 1282 [D loss: 0.004892750643193722, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9438266754150391]\n",
      " 1283 [D loss: 0.0030063113663345575, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9385514259338379]\n",
      " 1284 [D loss: 0.0010831430554389954, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9395937323570251]\n",
      " 1285 [D loss: 0.0009357621893286705, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9369379281997681]\n",
      " 1286 [D loss: 0.002299370476976037, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9340219497680664]\n",
      " 1287 [D loss: 0.0014514101203531027, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9451862573623657]\n",
      " 1288 [D loss: 0.0014505020808428526, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9325840473175049]\n",
      " 1289 [D loss: 0.0011952994391322136, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9401225447654724]\n",
      " 1290 [D loss: 0.0007154251215979457, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9400856494903564]\n",
      " 1291 [D loss: 0.000816387590020895, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9408645033836365]\n",
      " 1292 [D loss: 0.0005243308842182159, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9397387504577637]\n",
      " 1293 [D loss: 0.0019940142519772053, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9412716627120972]\n",
      " 1294 [D loss: 0.0030712110456079245, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9446398019790649]\n",
      " 1295 [D loss: 0.00038323449552990496, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9454824924468994]\n",
      " 1296 [D loss: 0.0007221985724754632, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9482479691505432]\n",
      " 1297 [D loss: 0.0015863929875195026, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9487447142601013]\n",
      " 1298 [D loss: 0.000933041563257575, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9533215761184692]\n",
      " 1299 [D loss: 0.0020818838384002447, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9532679915428162]\n",
      " 1300 [D loss: 0.0014292100677266717, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9559345841407776]\n",
      " 1301 [D loss: 0.0006206466932781041, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9597350358963013]\n",
      " 1302 [D loss: 0.0019783987663686275, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9538146257400513]\n",
      " 1303 [D loss: 0.002378789708018303, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9510692358016968]\n",
      " 1304 [D loss: 0.004410152323544025, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9406384229660034]\n",
      " 1305 [D loss: 0.0020226032938808203, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9291406869888306]\n",
      " 1306 [D loss: 0.002018112689256668, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9223469495773315]\n",
      " 1307 [D loss: 0.0020408614072948694, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9103559255599976]\n",
      " 1308 [D loss: 0.0023551606573164463, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9022772908210754]\n",
      " 1309 [D loss: 0.003524405648931861, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9115716814994812]\n",
      " 1310 [D loss: 0.0026292954571545124, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9017260074615479]\n",
      " 1311 [D loss: 0.0037669576704502106, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9023877382278442]\n",
      " 1312 [D loss: 0.008351234719157219, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.9051696062088013]\n",
      " 1313 [D loss: 0.0056212968192994595, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.9081634879112244]\n",
      " 1314 [D loss: 0.0034524109214544296, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9165897369384766]\n",
      " 1315 [D loss: 0.007505600806325674, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9336904287338257]\n",
      " 1316 [D loss: 0.0019276801031082869, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9407232999801636]\n",
      " 1317 [D loss: 0.0018500258447602391, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9422005414962769]\n",
      " 1318 [D loss: 0.0015183788491412997, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9476271867752075]\n",
      " 1319 [D loss: 0.0006638332270085812, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9494101405143738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1320 [D loss: 0.00029649975476786494, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9555497169494629]\n",
      " 1321 [D loss: 0.0004230737977195531, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9564318656921387]\n",
      " 1322 [D loss: 0.0006219478673301637, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9573246836662292]\n",
      " 1323 [D loss: 0.005388419143855572, acc_real: 98.437500, acc_fake: 98.437500] [G loss: 0.9583660960197449]\n",
      " 1324 [D loss: 0.0007914486923255026, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9571475982666016]\n",
      " 1325 [D loss: 0.0004891230491921306, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9563378095626831]\n",
      " 1326 [D loss: 0.0020731789991259575, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9564074873924255]\n",
      " 1327 [D loss: 0.00034696064540185034, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9581606984138489]\n",
      " 1328 [D loss: 0.0010461404453963041, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9539103507995605]\n",
      " 1329 [D loss: 0.0056776972487568855, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9490038156509399]\n",
      " 1330 [D loss: 0.00936972163617611, acc_real: 98.437500, acc_fake: 98.437500] [G loss: 0.9404982924461365]\n",
      " 1331 [D loss: 0.005682849790900946, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9322776794433594]\n",
      " 1332 [D loss: 0.00267976731993258, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9254539012908936]\n",
      " 1333 [D loss: 0.0028475099243223667, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9187682271003723]\n",
      " 1334 [D loss: 0.0028523539658635855, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9166074395179749]\n",
      " 1335 [D loss: 0.009125228971242905, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.9203661680221558]\n",
      " 1336 [D loss: 0.0025327829644083977, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9237244129180908]\n",
      " 1337 [D loss: 0.0013376547722145915, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9296919107437134]\n",
      " 1338 [D loss: 0.0013326066546142101, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9386468529701233]\n",
      " 1339 [D loss: 0.000894523982424289, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9428489804267883]\n",
      " 1340 [D loss: 0.0017726571531966329, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.938043475151062]\n",
      " 1341 [D loss: 0.0012022507144138217, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9443457126617432]\n",
      " 1342 [D loss: 0.0008634018595330417, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9460646510124207]\n",
      " 1343 [D loss: 0.0011955790687352419, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9475657939910889]\n",
      " 1344 [D loss: 0.00150503299664706, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9539646506309509]\n",
      " 1345 [D loss: 0.0008793392335064709, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.953345000743866]\n",
      " 1346 [D loss: 0.0005726998788304627, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9535868763923645]\n",
      " 1347 [D loss: 0.0003077518776990473, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9549992680549622]\n",
      " 1348 [D loss: 0.001847422099672258, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9539929628372192]\n",
      " 1349 [D loss: 0.0005694676074199378, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9553123116493225]\n",
      " 1350 [D loss: 0.0013396786525845528, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9544821977615356]\n",
      " 1351 [D loss: 0.002151969587430358, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9531240463256836]\n",
      " 1352 [D loss: 0.0005993526428937912, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9535082578659058]\n",
      " 1353 [D loss: 0.006854510400444269, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.953018844127655]\n",
      " 1354 [D loss: 0.0007888844702392817, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9561885595321655]\n",
      " 1355 [D loss: 0.0010841592447832227, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9496009349822998]\n",
      " 1356 [D loss: 0.002541753463447094, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9514937400817871]\n",
      " 1357 [D loss: 0.001319034257903695, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9525259137153625]\n",
      " 1358 [D loss: 0.0006621308857575059, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9540634751319885]\n",
      " 1359 [D loss: 0.0003523015766404569, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9542306661605835]\n",
      " 1360 [D loss: 0.000926180393435061, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9551949501037598]\n",
      " 1361 [D loss: 0.0007059518247842789, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9552801251411438]\n",
      " 1362 [D loss: 0.0003892974928021431, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9567073583602905]\n",
      " 1363 [D loss: 0.001857208670116961, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.956456184387207]\n",
      " 1364 [D loss: 0.000762460520491004, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9523165822029114]\n",
      " 1365 [D loss: 0.0005512314382940531, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9620129466056824]\n",
      " 1366 [D loss: 0.00042458411189727485, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9576910734176636]\n",
      " 1367 [D loss: 0.006292866542935371, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9573946595191956]\n",
      " 1368 [D loss: 0.0006707630818709731, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9539612531661987]\n",
      " 1369 [D loss: 0.005153154022991657, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9523138403892517]\n",
      " 1370 [D loss: 0.0006987109081819654, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9476290941238403]\n",
      " 1371 [D loss: 0.002867860719561577, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9478631019592285]\n",
      " 1372 [D loss: 0.003945065196603537, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9465447664260864]\n",
      " 1373 [D loss: 0.001876775175333023, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9464892745018005]\n",
      " 1374 [D loss: 0.0008790899300947785, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9497000575065613]\n",
      " 1375 [D loss: 0.0016677097883075476, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9489182233810425]\n",
      " 1376 [D loss: 0.0007439780747517943, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9532716870307922]\n",
      " 1377 [D loss: 0.0004477654292713851, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.953991711139679]\n",
      " 1378 [D loss: 0.0005126746254973114, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9569278359413147]\n",
      " 1379 [D loss: 0.0005661207833327353, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9562084078788757]\n",
      " 1380 [D loss: 0.000556415703613311, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9571474194526672]\n",
      " 1381 [D loss: 0.0003697668726090342, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9597347974777222]\n",
      " 1382 [D loss: 0.004460819531232119, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.956170916557312]\n",
      " 1383 [D loss: 0.0003882887540385127, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9565479755401611]\n",
      " 1384 [D loss: 0.003850963432341814, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9489744901657104]\n",
      " 1385 [D loss: 0.0006280102534219623, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9420814514160156]\n",
      " 1386 [D loss: 0.002319013699889183, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9324326515197754]\n",
      " 1387 [D loss: 0.0012283867690712214, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.926658570766449]\n",
      " 1388 [D loss: 0.0008544317097403109, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9263685941696167]\n",
      " 1389 [D loss: 0.007953645661473274, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9214369654655457]\n",
      " 1390 [D loss: 0.0032384064979851246, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9249016046524048]\n",
      " 1391 [D loss: 0.0018704055109992623, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9293906688690186]\n",
      " 1392 [D loss: 0.002530910074710846, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9314833283424377]\n",
      " 1393 [D loss: 0.0022979266941547394, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9326614141464233]\n",
      " 1394 [D loss: 0.002360790967941284, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9370343089103699]\n",
      " 1395 [D loss: 0.0009673840249888599, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9397221803665161]\n",
      " 1396 [D loss: 0.00114768638741225, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9440118074417114]\n",
      " 1397 [D loss: 0.00066356360912323, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.947442352771759]\n",
      " 1398 [D loss: 0.0007400377653539181, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9502936601638794]\n",
      " 1399 [D loss: 0.002044110093265772, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9505954384803772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1400 [D loss: 0.0004135376075282693, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9532475471496582]\n",
      " 1401 [D loss: 0.0028270476032048464, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9563148617744446]\n",
      " 1402 [D loss: 0.0006558711756952107, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9562238454818726]\n",
      " 1403 [D loss: 0.0005605871556326747, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.95941162109375]\n",
      " 1404 [D loss: 0.0008250189130194485, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.958387553691864]\n",
      " 1405 [D loss: 0.002753729000687599, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9586120843887329]\n",
      " 1406 [D loss: 0.00028497015591710806, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.955610990524292]\n",
      " 1407 [D loss: 0.0010949743445962667, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9562996625900269]\n",
      " 1408 [D loss: 0.0005482013220898807, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9531968832015991]\n",
      " 1409 [D loss: 0.0005495254881680012, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9519128799438477]\n",
      " 1410 [D loss: 0.0009262466337531805, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9498251676559448]\n",
      " 1411 [D loss: 0.0008217684226110578, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9513384699821472]\n",
      " 1412 [D loss: 0.0010543707758188248, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9504532217979431]\n",
      " 1413 [D loss: 0.0005101830465719104, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9524842500686646]\n",
      " 1414 [D loss: 0.000958019052632153, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9531581401824951]\n",
      " 1415 [D loss: 0.0005421368405222893, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9531110525131226]\n",
      " 1416 [D loss: 0.0015979509335011244, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9525635242462158]\n",
      " 1417 [D loss: 0.0003197822079528123, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9632842540740967]\n",
      " 1418 [D loss: 0.0011435364140197635, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9631065726280212]\n",
      " 1419 [D loss: 0.0003347182064317167, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.959875762462616]\n",
      " 1420 [D loss: 0.0004181165131740272, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9585491418838501]\n",
      " 1421 [D loss: 0.00033514140523038805, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9603002071380615]\n",
      " 1422 [D loss: 0.003181236796081066, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9620385766029358]\n",
      " 1423 [D loss: 0.0010705122258514166, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9570006728172302]\n",
      " 1424 [D loss: 0.00029472552705556154, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9577399492263794]\n",
      " 1425 [D loss: 0.0005873389891348779, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9567948579788208]\n",
      " 1426 [D loss: 0.000477195018902421, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9558984041213989]\n",
      " 1427 [D loss: 0.00041696414700709283, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9566634297370911]\n",
      " 1428 [D loss: 0.0010225450387224555, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9566288590431213]\n",
      " 1429 [D loss: 0.0009126937366090715, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9567384719848633]\n",
      " 1430 [D loss: 0.002325703389942646, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9558236598968506]\n",
      " 1431 [D loss: 0.0005049994215369225, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9533921480178833]\n",
      " 1432 [D loss: 0.000392140937037766, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9555723667144775]\n",
      " 1433 [D loss: 0.0011443969560787082, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.955197274684906]\n",
      " 1434 [D loss: 0.002220818307250738, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9538096785545349]\n",
      " 1435 [D loss: 0.0025534597225487232, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9501584768295288]\n",
      " 1436 [D loss: 0.0017370638670399785, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9455550312995911]\n",
      " 1437 [D loss: 0.0012507842620834708, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.944953441619873]\n",
      " 1438 [D loss: 0.0020888415165245533, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9415296316146851]\n",
      " 1439 [D loss: 0.0006748975138179958, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9433081746101379]\n",
      " 1440 [D loss: 0.003907366190105677, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9421623945236206]\n",
      " 1441 [D loss: 0.0009588436223566532, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9410933256149292]\n",
      " 1442 [D loss: 0.0006252540042623878, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9446333646774292]\n",
      " 1443 [D loss: 0.0020833471789956093, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9435133934020996]\n",
      " 1444 [D loss: 0.0022812553215771914, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9468017816543579]\n",
      " 1445 [D loss: 0.0006279631052166224, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9487484693527222]\n",
      " 1446 [D loss: 0.00201242184266448, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9522058367729187]\n",
      " 1447 [D loss: 0.0010967568960040808, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9565673470497131]\n",
      " 1448 [D loss: 0.000456471141660586, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9586702585220337]\n",
      " 1449 [D loss: 0.0003857032279483974, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9580613374710083]\n",
      " 1450 [D loss: 0.001928186509758234, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9573853015899658]\n",
      " 1451 [D loss: 0.0002347949775867164, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9568653702735901]\n",
      " 1452 [D loss: 0.000226024683797732, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9579036235809326]\n",
      " 1453 [D loss: 0.0014231792883947492, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9587657451629639]\n",
      " 1454 [D loss: 0.00024402541748713702, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.958321750164032]\n",
      " 1455 [D loss: 0.0016524167731404305, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9638264179229736]\n",
      " 1456 [D loss: 0.00039354414911940694, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.959822952747345]\n",
      " 1457 [D loss: 0.0071419356390833855, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9490344524383545]\n",
      " 1458 [D loss: 0.001373871462419629, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9379205703735352]\n",
      " 1459 [D loss: 0.003086003940552473, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9265596866607666]\n",
      " 1460 [D loss: 0.005464655347168446, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.9203364849090576]\n",
      " 1461 [D loss: 0.006093552801758051, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9179449081420898]\n",
      " 1462 [D loss: 0.0015280265361070633, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9154971837997437]\n",
      " 1463 [D loss: 0.00135240878444165, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.918724775314331]\n",
      " 1464 [D loss: 0.011206310242414474, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.9242679476737976]\n",
      " 1465 [D loss: 0.0011523904977366328, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9272520542144775]\n",
      " 1466 [D loss: 0.0016311778454110026, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.933357298374176]\n",
      " 1467 [D loss: 0.0039839535020291805, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9373673796653748]\n",
      " 1468 [D loss: 0.0004868896212428808, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9425573348999023]\n",
      " 1469 [D loss: 0.001824686536565423, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9440865516662598]\n",
      " 1470 [D loss: 0.00104493647813797, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9567553400993347]\n",
      " 1471 [D loss: 0.0013901983620598912, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9546400308609009]\n",
      " 1472 [D loss: 0.0009200096828863025, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9566887617111206]\n",
      " 1473 [D loss: 0.0006540280883200467, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9609845876693726]\n",
      " 1474 [D loss: 0.000753486470784992, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9621182680130005]\n",
      " 1475 [D loss: 0.0005454322090372443, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9654510021209717]\n",
      " 1476 [D loss: 0.0035677864216268063, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9623578786849976]\n",
      " 1477 [D loss: 0.0011405627010390162, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9596263766288757]\n",
      " 1478 [D loss: 0.0007130100857466459, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9527411460876465]\n",
      " 1479 [D loss: 0.0030478574335575104, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9489898681640625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1480 [D loss: 0.00020534000941552222, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9483667612075806]\n",
      " 1481 [D loss: 0.001126710674725473, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9477768540382385]\n",
      " 1482 [D loss: 0.004366219975054264, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9467704892158508]\n",
      " 1483 [D loss: 0.0022517219185829163, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.949726402759552]\n",
      " 1484 [D loss: 0.0012268314603716135, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9491105079650879]\n",
      " 1485 [D loss: 0.0009853682713583112, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9500588178634644]\n",
      " 1486 [D loss: 0.0022831405512988567, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9499205946922302]\n",
      " 1487 [D loss: 0.0006597281899303198, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9510347843170166]\n",
      " 1488 [D loss: 0.0007330476073548198, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9544014930725098]\n",
      " 1489 [D loss: 0.0009249547147192061, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9547052383422852]\n",
      " 1490 [D loss: 0.0009102644398808479, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9583476781845093]\n",
      " 1491 [D loss: 0.0006647702539339662, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9596196413040161]\n",
      " 1492 [D loss: 0.0004140759992878884, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9601203203201294]\n",
      " 1493 [D loss: 0.00019210061873309314, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9629303216934204]\n",
      " 1494 [D loss: 0.00042210760875605047, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.959893524646759]\n",
      " 1495 [D loss: 0.0001618461246835068, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9609596729278564]\n",
      " 1496 [D loss: 0.00043452251702547073, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9622381329536438]\n",
      " 1497 [D loss: 0.0005448154988698661, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.961369514465332]\n",
      " 1498 [D loss: 0.001880029565654695, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9650734663009644]\n",
      " 1499 [D loss: 0.0010747030610218644, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.959580659866333]\n",
      " 1500 [D loss: 0.00042429077439010143, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9564793109893799]\n",
      " 1501 [D loss: 0.0006253417232073843, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9597285389900208]\n",
      " 1502 [D loss: 0.00022940781491342932, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9544204473495483]\n",
      " 1503 [D loss: 0.0009692023159004748, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9558932185173035]\n",
      " 1504 [D loss: 0.0010620959801599383, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9561648964881897]\n",
      " 1505 [D loss: 0.0020433301106095314, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9552170634269714]\n",
      " 1506 [D loss: 0.0005281538469716907, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9573938250541687]\n",
      " 1507 [D loss: 0.0008466560393571854, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9574370384216309]\n",
      " 1508 [D loss: 0.00040042033651843667, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.958336353302002]\n",
      " 1509 [D loss: 0.00037757513928227127, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9586877822875977]\n",
      " 1510 [D loss: 0.0005327739054337144, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9575291872024536]\n",
      " 1511 [D loss: 0.000472521671326831, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9585208892822266]\n",
      " 1512 [D loss: 0.0010399953462183475, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9577651023864746]\n",
      " 1513 [D loss: 0.00028794072568416595, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9578027129173279]\n",
      " 1514 [D loss: 0.0014925627037882805, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.953687310218811]\n",
      " 1515 [D loss: 0.002048542257398367, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9539856910705566]\n",
      " 1516 [D loss: 0.0011078428942710161, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9553733468055725]\n",
      " 1517 [D loss: 0.0007377677829936147, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9552712440490723]\n",
      " 1518 [D loss: 0.0001371464313706383, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9575996398925781]\n",
      " 1519 [D loss: 0.0018117359140887856, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9573742151260376]\n",
      " 1520 [D loss: 0.0012344487477093935, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9626897573471069]\n",
      " 1521 [D loss: 0.0007659358670935035, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9599988460540771]\n",
      " 1522 [D loss: 0.0003222039667889476, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9624249935150146]\n",
      " 1523 [D loss: 0.0001697409024927765, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9640599489212036]\n",
      " 1524 [D loss: 0.0007260314887389541, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9633102416992188]\n",
      " 1525 [D loss: 0.0003947661607526243, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9649859666824341]\n",
      " 1526 [D loss: 0.002333157230168581, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9653573036193848]\n",
      " 1527 [D loss: 0.0004463400400709361, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9631751179695129]\n",
      " 1528 [D loss: 0.0027878559194505215, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9658724665641785]\n",
      " 1529 [D loss: 0.000577260332647711, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9589061141014099]\n",
      " 1530 [D loss: 0.0004802295588888228, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9612009525299072]\n",
      " 1531 [D loss: 0.0007208205643109977, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.955790102481842]\n",
      " 1532 [D loss: 0.00035745950299315155, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9530686736106873]\n",
      " 1533 [D loss: 0.0006214487948454916, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9544796943664551]\n",
      " 1534 [D loss: 0.0005578294512815773, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9576445817947388]\n",
      " 1535 [D loss: 0.00040399027056992054, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.954565167427063]\n",
      " 1536 [D loss: 0.0025111872237175703, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9565085172653198]\n",
      " 1537 [D loss: 0.00038561393739655614, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9562352895736694]\n",
      " 1538 [D loss: 0.0007017977768555284, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.959255576133728]\n",
      " 1539 [D loss: 0.0004148546722717583, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9595819711685181]\n",
      " 1540 [D loss: 0.00021695946634281427, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9610700607299805]\n",
      " 1541 [D loss: 0.0017559481784701347, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9628755450248718]\n",
      " 1542 [D loss: 0.0005205365596339107, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9653525948524475]\n",
      " 1543 [D loss: 0.0028584208339452744, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9589529037475586]\n",
      " 1544 [D loss: 0.00038248480996116996, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9535996913909912]\n",
      " 1545 [D loss: 0.0032022222876548767, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9537490606307983]\n",
      " 1546 [D loss: 0.0009558098390698433, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9525555372238159]\n",
      " 1547 [D loss: 0.0018585269572213292, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9517874717712402]\n",
      " 1548 [D loss: 0.0010913144797086716, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9523149728775024]\n",
      " 1549 [D loss: 0.0007796820718795061, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9520097374916077]\n",
      " 1550 [D loss: 0.0005456957733258605, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9515857696533203]\n",
      " 1551 [D loss: 0.0005597783019766212, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9515220522880554]\n",
      " 1552 [D loss: 0.000624354463070631, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9534642696380615]\n",
      " 1553 [D loss: 0.0003592403372749686, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9538114070892334]\n",
      " 1554 [D loss: 0.0008550893981009722, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9525319337844849]\n",
      " 1555 [D loss: 0.0021996991708874702, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9543613195419312]\n",
      " 1556 [D loss: 0.0017756401794031262, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9532900452613831]\n",
      " 1557 [D loss: 0.0005843149847351015, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9584448933601379]\n",
      " 1558 [D loss: 0.002976287854835391, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9587273001670837]\n",
      " 1559 [D loss: 0.0003260215453337878, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9633638858795166]\n",
      " 1560 [D loss: 0.00018111815734300762, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9634087085723877]\n",
      " 1561 [D loss: 0.0005990861682221293, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9650505185127258]\n",
      " 1562 [D loss: 0.0010306054027751088, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9670894742012024]\n",
      " 1563 [D loss: 0.0011153167579323053, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.966216504573822]\n",
      " 1564 [D loss: 0.0007928275153972208, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9643955230712891]\n",
      " 1565 [D loss: 0.0006495583802461624, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9637688398361206]\n",
      " 1566 [D loss: 0.0007643336430191994, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9641697406768799]\n",
      " 1567 [D loss: 0.000855799182318151, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9647608995437622]\n",
      " 1568 [D loss: 0.00036008437746204436, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9651980400085449]\n",
      " 1569 [D loss: 0.0005780676146969199, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9680453538894653]\n",
      " 1570 [D loss: 0.0007427746313624084, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9656902551651001]\n",
      " 1571 [D loss: 0.00017152645159512758, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9661211967468262]\n",
      " 1572 [D loss: 0.0004113862232770771, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9684052467346191]\n",
      " 1573 [D loss: 0.0008400351507589221, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9669934511184692]\n",
      " 1574 [D loss: 0.0004319532308727503, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9647464752197266]\n",
      " 1575 [D loss: 0.0006252691964618862, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.966814398765564]\n",
      " 1576 [D loss: 0.00012174521543784067, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9675942659378052]\n",
      " 1577 [D loss: 0.0006402662256732583, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9697675108909607]\n",
      " 1578 [D loss: 0.0006363848224282265, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9663063883781433]\n",
      " 1579 [D loss: 0.0002319067862117663, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9678109884262085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1580 [D loss: 0.00798280630260706, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9164013266563416]\n",
      " 1581 [D loss: 0.008605381473898888, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.8562889099121094]\n",
      " 1582 [D loss: 0.0331743061542511, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.8034218549728394]\n",
      " 1583 [D loss: 0.03762933611869812, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.780699610710144]\n",
      " 1584 [D loss: 0.05205445736646652, acc_real: 100.000000, acc_fake: 87.500000] [G loss: 0.762513279914856]\n",
      " 1585 [D loss: 0.06192723289132118, acc_real: 100.000000, acc_fake: 81.250000] [G loss: 0.7386929988861084]\n",
      " 1586 [D loss: 0.08052682131528854, acc_real: 100.000000, acc_fake: 78.125000] [G loss: 0.7224845886230469]\n",
      " 1587 [D loss: 0.08756425976753235, acc_real: 100.000000, acc_fake: 76.562500] [G loss: 0.7269408702850342]\n",
      " 1588 [D loss: 0.08078477531671524, acc_real: 100.000000, acc_fake: 79.687500] [G loss: 0.727852463722229]\n",
      " 1589 [D loss: 0.041681770235300064, acc_real: 100.000000, acc_fake: 89.062500] [G loss: 0.7409477233886719]\n",
      " 1590 [D loss: 0.04562745243310928, acc_real: 100.000000, acc_fake: 89.062500] [G loss: 0.7413670420646667]\n",
      " 1591 [D loss: 0.05224155634641647, acc_real: 100.000000, acc_fake: 87.500000] [G loss: 0.7575762271881104]\n",
      " 1592 [D loss: 0.04404117166996002, acc_real: 98.437500, acc_fake: 87.500000] [G loss: 0.771358847618103]\n",
      " 1593 [D loss: 0.08201271295547485, acc_real: 100.000000, acc_fake: 81.250000] [G loss: 0.7727286219596863]\n",
      " 1594 [D loss: 0.03523556888103485, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.7926039099693298]\n",
      " 1595 [D loss: 0.05990224331617355, acc_real: 100.000000, acc_fake: 81.250000] [G loss: 0.8089814186096191]\n",
      " 1596 [D loss: 0.050707053393125534, acc_real: 100.000000, acc_fake: 87.500000] [G loss: 0.793577253818512]\n",
      " 1597 [D loss: 0.052366018295288086, acc_real: 100.000000, acc_fake: 85.937500] [G loss: 0.8002160787582397]\n",
      " 1598 [D loss: 0.03758052736520767, acc_real: 100.000000, acc_fake: 87.500000] [G loss: 0.8004423379898071]\n",
      " 1599 [D loss: 0.04432404413819313, acc_real: 96.875000, acc_fake: 92.187500] [G loss: 0.8149003386497498]\n",
      " 1600 [D loss: 0.059297412633895874, acc_real: 96.875000, acc_fake: 85.937500] [G loss: 0.8260723352432251]\n",
      " 1601 [D loss: 0.03492816537618637, acc_real: 95.312500, acc_fake: 95.312500] [G loss: 0.837677001953125]\n",
      " 1602 [D loss: 0.0741419568657875, acc_real: 90.625000, acc_fake: 93.750000] [G loss: 0.8249537944793701]\n",
      " 1603 [D loss: 0.05134997516870499, acc_real: 93.750000, acc_fake: 95.312500] [G loss: 0.8285917043685913]\n",
      " 1604 [D loss: 0.07428917288780212, acc_real: 92.187500, acc_fake: 92.187500] [G loss: 0.7785078287124634]\n",
      " 1605 [D loss: 0.04747788608074188, acc_real: 100.000000, acc_fake: 87.500000] [G loss: 0.7033997774124146]\n",
      " 1606 [D loss: 0.05313879996538162, acc_real: 100.000000, acc_fake: 87.500000] [G loss: 0.6399462223052979]\n",
      " 1607 [D loss: 0.0897751897573471, acc_real: 100.000000, acc_fake: 76.562500] [G loss: 0.6332637071609497]\n",
      " 1608 [D loss: 0.14626562595367432, acc_real: 100.000000, acc_fake: 64.062500] [G loss: 0.61522376537323]\n",
      " 1609 [D loss: 0.11260372400283813, acc_real: 100.000000, acc_fake: 71.875000] [G loss: 0.6071093082427979]\n",
      " 1610 [D loss: 0.08958030492067337, acc_real: 100.000000, acc_fake: 78.125000] [G loss: 0.6317766308784485]\n",
      " 1611 [D loss: 0.12310377508401871, acc_real: 100.000000, acc_fake: 68.750000] [G loss: 0.5966354608535767]\n",
      " 1612 [D loss: 0.12693053483963013, acc_real: 100.000000, acc_fake: 68.750000] [G loss: 0.6075524091720581]\n",
      " 1613 [D loss: 0.0934518575668335, acc_real: 100.000000, acc_fake: 76.562500] [G loss: 0.6287251710891724]\n",
      " 1614 [D loss: 0.10176654905080795, acc_real: 100.000000, acc_fake: 76.562500] [G loss: 0.6519727110862732]\n",
      " 1615 [D loss: 0.10452184081077576, acc_real: 100.000000, acc_fake: 76.562500] [G loss: 0.6680222749710083]\n",
      " 1616 [D loss: 0.08157162368297577, acc_real: 100.000000, acc_fake: 79.687500] [G loss: 0.7239313125610352]\n",
      " 1617 [D loss: 0.08146433532238007, acc_real: 100.000000, acc_fake: 81.250000] [G loss: 0.7322682738304138]\n",
      " 1618 [D loss: 0.06476812064647675, acc_real: 100.000000, acc_fake: 81.250000] [G loss: 0.7833766937255859]\n",
      " 1619 [D loss: 0.07880473136901855, acc_real: 100.000000, acc_fake: 79.687500] [G loss: 0.7835713624954224]\n",
      " 1620 [D loss: 0.025979867205023766, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.8277046084403992]\n",
      " 1621 [D loss: 0.04208816587924957, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.858193039894104]\n",
      " 1622 [D loss: 0.030746255069971085, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.8655040860176086]\n",
      " 1623 [D loss: 0.006148525048047304, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8817288279533386]\n",
      " 1624 [D loss: 0.01435159333050251, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.9091315269470215]\n",
      " 1625 [D loss: 0.004007797688245773, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9300822019577026]\n",
      " 1626 [D loss: 0.004529078025370836, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9422497749328613]\n",
      " 1627 [D loss: 0.0031830419320613146, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9590537548065186]\n",
      " 1628 [D loss: 0.0013046294916421175, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9707016944885254]\n",
      " 1629 [D loss: 0.0007375957211479545, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9732673764228821]\n",
      " 1630 [D loss: 0.00040178964263759553, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9792792201042175]\n",
      " 1631 [D loss: 0.0010231670457869768, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9809850454330444]\n",
      " 1632 [D loss: 0.00010175023635383695, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9825880527496338]\n",
      " 1633 [D loss: 0.014729887247085571, acc_real: 96.875000, acc_fake: 100.000000] [G loss: 0.9717392921447754]\n",
      " 1634 [D loss: 0.0004949546419084072, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9576043486595154]\n",
      " 1635 [D loss: 0.0021072309464216232, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9435831904411316]\n",
      " 1636 [D loss: 0.009463323280215263, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.934389054775238]\n",
      " 1637 [D loss: 0.0030092820525169373, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9335347414016724]\n",
      " 1638 [D loss: 0.006075491197407246, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9309804439544678]\n",
      " 1639 [D loss: 0.003941320814192295, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.932175874710083]\n",
      " 1640 [D loss: 0.004965886007994413, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9363681077957153]\n",
      " 1641 [D loss: 0.0031422285828739405, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.940653920173645]\n",
      " 1642 [D loss: 0.002901381580159068, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9448895454406738]\n",
      " 1643 [D loss: 0.0009883269667625427, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9491086006164551]\n",
      " 1644 [D loss: 0.0013632538029924035, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9500224590301514]\n",
      " 1645 [D loss: 0.0026236879639327526, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9586337804794312]\n",
      " 1646 [D loss: 0.0006746192229911685, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9614109992980957]\n",
      " 1647 [D loss: 0.0004057502083014697, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.960792064666748]\n",
      " 1648 [D loss: 0.0007523886160925031, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9685888886451721]\n",
      " 1649 [D loss: 0.00022780470317229629, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9716496467590332]\n",
      " 1650 [D loss: 0.00032673103851266205, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9729421138763428]\n",
      " 1651 [D loss: 0.00022531964350491762, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9723999500274658]\n",
      " 1652 [D loss: 0.0016374853439629078, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9710392951965332]\n",
      " 1653 [D loss: 0.00016703458095435053, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9691953659057617]\n",
      " 1654 [D loss: 0.0003056250570807606, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9710923433303833]\n",
      " 1655 [D loss: 0.0005186721100471914, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9693547487258911]\n",
      " 1656 [D loss: 0.000344134314218536, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9675137996673584]\n",
      " 1657 [D loss: 0.00036561404704116285, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9695084691047668]\n",
      " 1658 [D loss: 0.0011264854110777378, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9670061469078064]\n",
      " 1659 [D loss: 0.0004152636101935059, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9658510684967041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1660 [D loss: 0.0005664125783368945, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9642916917800903]\n",
      " 1661 [D loss: 0.00033499643905088305, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9673440456390381]\n",
      " 1662 [D loss: 0.0001472489966545254, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9678014516830444]\n",
      " 1663 [D loss: 0.00040237573557533324, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9672573208808899]\n",
      " 1664 [D loss: 0.0009262224193662405, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9675207138061523]\n",
      " 1665 [D loss: 0.0006287707947194576, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9689579010009766]\n",
      " 1666 [D loss: 0.0009220057399943471, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9701652526855469]\n",
      " 1667 [D loss: 0.0019914372824132442, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.970521092414856]\n",
      " 1668 [D loss: 0.00033882242860272527, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9729580283164978]\n",
      " 1669 [D loss: 0.0006051649106666446, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9726425409317017]\n",
      " 1670 [D loss: 0.00040375080425292253, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9737943410873413]\n",
      " 1671 [D loss: 0.0001850862754508853, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.974341869354248]\n",
      " 1672 [D loss: 0.00010147965076612309, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9731760025024414]\n",
      " 1673 [D loss: 0.00010876477608690038, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9752382636070251]\n",
      " 1674 [D loss: 7.312910747714341e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9783457517623901]\n",
      " 1675 [D loss: 0.0001970731682376936, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9749158024787903]\n",
      " 1676 [D loss: 0.000137406459543854, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9766782522201538]\n",
      " 1677 [D loss: 0.0002716178714763373, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9751341342926025]\n",
      " 1678 [D loss: 0.00011105729208793491, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9764301776885986]\n",
      " 1679 [D loss: 0.0028317535761743784, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9760576486587524]\n",
      " 1680 [D loss: 0.00026305197388865054, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9751547574996948]\n",
      " 1681 [D loss: 0.00025865944917313755, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9717404842376709]\n",
      " 1682 [D loss: 0.00024260146892629564, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9732341766357422]\n",
      " 1683 [D loss: 0.002116063144057989, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9744060039520264]\n",
      " 1684 [D loss: 0.0003217135672457516, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9743684530258179]\n",
      " 1685 [D loss: 0.00038673842209391296, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9733507633209229]\n",
      " 1686 [D loss: 0.00011099822586402297, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9738492369651794]\n",
      " 1687 [D loss: 0.0002049921895377338, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.975675642490387]\n",
      " 1688 [D loss: 0.00010540984658291563, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9765357971191406]\n",
      " 1689 [D loss: 0.00017128611216321588, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9744656085968018]\n",
      " 1690 [D loss: 0.00012265036639291793, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.975290060043335]\n",
      " 1691 [D loss: 0.0004314849793445319, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.975431501865387]\n",
      " 1692 [D loss: 0.005613891873508692, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9751240015029907]\n",
      " 1693 [D loss: 0.00010474755254108459, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9715498089790344]\n",
      " 1694 [D loss: 0.0002499129914212972, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9690619707107544]\n",
      " 1695 [D loss: 0.00015153479762375355, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9717032313346863]\n",
      " 1696 [D loss: 0.0002946092572528869, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9717477560043335]\n",
      " 1697 [D loss: 0.00029084336711093783, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9696512222290039]\n",
      " 1698 [D loss: 0.0003603189834393561, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9687261581420898]\n",
      " 1699 [D loss: 0.00032828605617396533, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9659712314605713]\n",
      " 1700 [D loss: 0.00023490015882998705, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9671236276626587]\n",
      " 1701 [D loss: 0.0002518320980016142, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9671033620834351]\n",
      " 1702 [D loss: 0.00035383220529183745, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9701818823814392]\n",
      " 1703 [D loss: 0.0004935095785185695, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9681828022003174]\n",
      " 1704 [D loss: 0.0004280577995814383, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9664464592933655]\n",
      " 1705 [D loss: 0.00021350712631829083, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9673460721969604]\n",
      " 1706 [D loss: 0.00034521715133450925, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9679778814315796]\n",
      " 1707 [D loss: 0.0005231998511590064, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9700670838356018]\n",
      " 1708 [D loss: 0.0007390956161543727, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9706532955169678]\n",
      " 1709 [D loss: 0.0004284089955035597, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9708400368690491]\n",
      " 1710 [D loss: 0.00202881614677608, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.96883225440979]\n",
      " 1711 [D loss: 0.00022491317940875888, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9700344800949097]\n",
      " 1712 [D loss: 0.0006109473761171103, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9736405611038208]\n",
      " 1713 [D loss: 0.0004340849700383842, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.973434567451477]\n",
      " 1714 [D loss: 0.00014429587463382632, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9742453098297119]\n",
      " 1715 [D loss: 0.00026551465271040797, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9751939177513123]\n",
      " 1716 [D loss: 0.000207356468308717, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9714297652244568]\n",
      " 1717 [D loss: 0.000543019559700042, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9730817079544067]\n",
      " 1718 [D loss: 0.0009922098834067583, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9747251272201538]\n",
      " 1719 [D loss: 0.001282903947867453, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9726724624633789]\n",
      " 1720 [D loss: 0.00021748573635704815, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9752366542816162]\n",
      " 1721 [D loss: 0.0012712855823338032, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.974334716796875]\n",
      " 1722 [D loss: 0.00019398148288019001, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.970907986164093]\n",
      " 1723 [D loss: 0.00043617445044219494, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9716564416885376]\n",
      " 1724 [D loss: 0.0010875571751967072, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9675424098968506]\n",
      " 1725 [D loss: 0.0005468345480039716, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9683897495269775]\n",
      " 1726 [D loss: 0.00020810010028071702, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9677414894104004]\n",
      " 1727 [D loss: 0.0003389311605133116, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9625946283340454]\n",
      " 1728 [D loss: 0.0005305588128976524, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.964089572429657]\n",
      " 1729 [D loss: 0.0011506805894896388, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9608664512634277]\n",
      " 1730 [D loss: 0.0002629123628139496, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9643971920013428]\n",
      " 1731 [D loss: 0.0007611634791828692, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9626368880271912]\n",
      " 1732 [D loss: 0.000672074209433049, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9633538126945496]\n",
      " 1733 [D loss: 0.00024555117124691606, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9599437117576599]\n",
      " 1734 [D loss: 0.0001355412823613733, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.96649569272995]\n",
      " 1735 [D loss: 0.0011107898317277431, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9669932126998901]\n",
      " 1736 [D loss: 0.00026623212033882737, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9668855667114258]\n",
      " 1737 [D loss: 0.002240769099444151, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9658886194229126]\n",
      " 1738 [D loss: 0.00014957225357647985, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9670955538749695]\n",
      " 1739 [D loss: 0.000491382263135165, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9676321744918823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1740 [D loss: 0.0003211646981071681, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9706267714500427]\n",
      " 1741 [D loss: 0.0004944907850585878, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9729874730110168]\n",
      " 1742 [D loss: 0.0005291072302497923, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.970924437046051]\n",
      " 1743 [D loss: 0.0002096537791658193, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9724968671798706]\n",
      " 1744 [D loss: 8.80188454175368e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9716947078704834]\n",
      " 1745 [D loss: 0.00026314734714105725, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9711662530899048]\n",
      " 1746 [D loss: 0.0007958474452607334, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9736828804016113]\n",
      " 1747 [D loss: 0.0005715988227166235, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9713144302368164]\n",
      " 1748 [D loss: 9.722090908326209e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9750372171401978]\n",
      " 1749 [D loss: 0.0002834663901012391, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9747413396835327]\n",
      " 1750 [D loss: 0.00033383615664206445, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9734455347061157]\n",
      " 1751 [D loss: 0.0002369097201153636, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9755480289459229]\n",
      " 1752 [D loss: 0.00011692106636473909, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9737251996994019]\n",
      " 1753 [D loss: 0.00047357974108308554, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9759859442710876]\n",
      " 1754 [D loss: 0.0005237520672380924, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9767739772796631]\n",
      " 1755 [D loss: 0.0004638248938135803, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9739463925361633]\n",
      " 1756 [D loss: 0.0017148051410913467, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9767281413078308]\n",
      " 1757 [D loss: 0.00017535944061819464, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9768390655517578]\n",
      " 1758 [D loss: 0.0004126628628000617, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9765989780426025]\n",
      " 1759 [D loss: 0.000304060144117102, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9758238792419434]\n",
      " 1760 [D loss: 0.00010397649748483673, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9758570790290833]\n",
      " 1761 [D loss: 0.00021142238983884454, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9773744940757751]\n",
      " 1762 [D loss: 0.00025055717560462654, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9764835834503174]\n",
      " 1763 [D loss: 0.0010290552163496614, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9749267101287842]\n",
      " 1764 [D loss: 0.0001520123187219724, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.977157711982727]\n",
      " 1765 [D loss: 0.0003022622549906373, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.975055456161499]\n",
      " 1766 [D loss: 7.033924339339137e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9762673377990723]\n",
      " 1767 [D loss: 0.00030772690661251545, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9755635261535645]\n",
      " 1768 [D loss: 0.0008825294789858162, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9776599407196045]\n",
      " 1769 [D loss: 0.002019988838583231, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9745030403137207]\n",
      " 1770 [D loss: 0.00017942130216397345, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9713158011436462]\n",
      " 1771 [D loss: 0.0009406850440427661, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9668611288070679]\n",
      " 1772 [D loss: 0.000978506519459188, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9630329012870789]\n",
      " 1773 [D loss: 0.0013502724468708038, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9620927572250366]\n",
      " 1774 [D loss: 0.002750807674601674, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9623563885688782]\n",
      " 1775 [D loss: 0.0006075501441955566, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9592159390449524]\n",
      " 1776 [D loss: 0.0007585647981613874, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9617411494255066]\n",
      " 1777 [D loss: 0.0009040248696692288, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9614278674125671]\n",
      " 1778 [D loss: 0.0007023384678177536, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9620643258094788]\n",
      " 1779 [D loss: 0.00041571230394765735, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9641584157943726]\n",
      " 1780 [D loss: 0.0009748082375153899, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9651026725769043]\n",
      " 1781 [D loss: 0.0005241454346105456, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9684245586395264]\n",
      " 1782 [D loss: 0.0005190608208067715, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9705865383148193]\n",
      " 1783 [D loss: 0.00038777763256803155, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9711182117462158]\n",
      " 1784 [D loss: 0.0024733946193009615, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9676376581192017]\n",
      " 1785 [D loss: 0.00018186256056651473, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9721471071243286]\n",
      " 1786 [D loss: 0.000371294969227165, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9723719358444214]\n",
      " 1787 [D loss: 0.0005000855890102684, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9742819666862488]\n",
      " 1788 [D loss: 0.00038486518315039575, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9729450941085815]\n",
      " 1789 [D loss: 0.00023095690994523466, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9759355783462524]\n",
      " 1790 [D loss: 0.001416824059560895, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9767163991928101]\n",
      " 1791 [D loss: 0.00023296019935514778, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9747170209884644]\n",
      " 1792 [D loss: 0.0001992753241211176, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9768425226211548]\n",
      " 1793 [D loss: 0.00013159225636627525, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9771872758865356]\n",
      " 1794 [D loss: 0.0013600056990981102, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9757349491119385]\n",
      " 1795 [D loss: 0.00040918064769357443, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9730481505393982]\n",
      " 1796 [D loss: 0.0010278585832566023, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9728322625160217]\n",
      " 1797 [D loss: 0.00018586008809506893, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.974712610244751]\n",
      " 1798 [D loss: 0.00017774182197172195, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9734535217285156]\n",
      " 1799 [D loss: 8.034909842535853e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9727282524108887]\n",
      " 1800 [D loss: 0.00015530158998444676, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9733079671859741]\n",
      " 1801 [D loss: 0.00037545000668615103, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9721947908401489]\n",
      " 1802 [D loss: 0.00045250868424773216, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9729431867599487]\n",
      " 1803 [D loss: 0.00021078571444377303, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9727957844734192]\n",
      " 1804 [D loss: 0.000313739845296368, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9719866514205933]\n",
      " 1805 [D loss: 0.0001647491881158203, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.973749577999115]\n",
      " 1806 [D loss: 0.0003197884652763605, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9724571704864502]\n",
      " 1807 [D loss: 0.00018777968944050372, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9744513034820557]\n",
      " 1808 [D loss: 0.0003399201377760619, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9741944670677185]\n",
      " 1809 [D loss: 0.00019957139738835394, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9740577936172485]\n",
      " 1810 [D loss: 0.00041501145460642874, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9737803339958191]\n",
      " 1811 [D loss: 0.0004778793372679502, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9732595086097717]\n",
      " 1812 [D loss: 0.00022159038053359836, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9746631383895874]\n",
      " 1813 [D loss: 0.0002173607499571517, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9740034937858582]\n",
      " 1814 [D loss: 6.390819908119738e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9750456809997559]\n",
      " 1815 [D loss: 0.0001182450505439192, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9742137789726257]\n",
      " 1816 [D loss: 0.0045359255746006966, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9725744128227234]\n",
      " 1817 [D loss: 0.00015359357348643243, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9708809852600098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1818 [D loss: 0.0010415532160550356, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9656614661216736]\n",
      " 1819 [D loss: 0.00037919983151368797, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9645271897315979]\n",
      " 1820 [D loss: 0.0008395510958507657, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9630597829818726]\n",
      " 1821 [D loss: 0.0007490069838240743, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9612462520599365]\n",
      " 1822 [D loss: 0.0002298663166584447, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.964633047580719]\n",
      " 1823 [D loss: 0.0007553116884082556, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9641340970993042]\n",
      " 1824 [D loss: 0.0005184595938771963, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9634554386138916]\n",
      " 1825 [D loss: 0.00223333528265357, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9664753675460815]\n",
      " 1826 [D loss: 0.0007757549174129963, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9678588509559631]\n",
      " 1827 [D loss: 0.0001479089551139623, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9711375832557678]\n",
      " 1828 [D loss: 0.0010068885749205947, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9683948159217834]\n",
      " 1829 [D loss: 0.00024538341676816344, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9723508954048157]\n",
      " 1830 [D loss: 0.00012491238885559142, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9716453552246094]\n",
      " 1831 [D loss: 0.0010268379701301455, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9721729755401611]\n",
      " 1832 [D loss: 0.0001742909662425518, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9750134944915771]\n",
      " 1833 [D loss: 0.0003179520135745406, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9754663109779358]\n",
      " 1834 [D loss: 0.00027047222829423845, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9773183465003967]\n",
      " 1835 [D loss: 0.0003466434427537024, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9772537350654602]\n",
      " 1836 [D loss: 0.0001224512525368482, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9782435894012451]\n",
      " 1837 [D loss: 0.00011663101031444967, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9806128740310669]\n",
      " 1838 [D loss: 0.005730479024350643, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9759722948074341]\n",
      " 1839 [D loss: 0.00152637530118227, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9744709730148315]\n",
      " 1840 [D loss: 0.00020211029914207757, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9714850187301636]\n",
      " 1841 [D loss: 0.0002625038323458284, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9695378541946411]\n",
      " 1842 [D loss: 0.0003419085405766964, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9650281667709351]\n",
      " 1843 [D loss: 0.00041518136276863515, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9657754302024841]\n",
      " 1844 [D loss: 0.0006102502811700106, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9639734029769897]\n",
      " 1845 [D loss: 0.00041932438034564257, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9646100997924805]\n",
      " 1846 [D loss: 0.0011463855626061559, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9615281820297241]\n",
      " 1847 [D loss: 0.0003373107756488025, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9638382196426392]\n",
      " 1848 [D loss: 0.0007520901272073388, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9645018577575684]\n",
      " 1849 [D loss: 0.00018933197134174407, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.965610146522522]\n",
      " 1850 [D loss: 0.0008429909939877689, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.964531660079956]\n",
      " 1851 [D loss: 0.0002189429069403559, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9646422863006592]\n",
      " 1852 [D loss: 0.00045317813055589795, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9675297141075134]\n",
      " 1853 [D loss: 0.00023830129066482186, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9686228036880493]\n",
      " 1854 [D loss: 0.0003685340634547174, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9673455953598022]\n",
      " 1855 [D loss: 0.0008368320995941758, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9698803424835205]\n",
      " 1856 [D loss: 0.004357379861176014, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9646297693252563]\n",
      " 1857 [D loss: 0.00018495456606615335, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9665590524673462]\n",
      " 1858 [D loss: 0.0018374744104221463, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9635117053985596]\n",
      " 1859 [D loss: 0.0018836335511878133, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9628955125808716]\n",
      " 1860 [D loss: 0.0006379838450811803, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.965378999710083]\n",
      " 1861 [D loss: 0.00016763535677455366, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9612488746643066]\n",
      " 1862 [D loss: 0.0009184039663523436, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9671208262443542]\n",
      " 1863 [D loss: 0.0007035665912553668, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9680289030075073]\n",
      " 1864 [D loss: 0.002316056750714779, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9671004414558411]\n",
      " 1865 [D loss: 0.005565216299146414, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9693801999092102]\n",
      " 1866 [D loss: 0.00013755838153883815, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9686140418052673]\n",
      " 1867 [D loss: 0.00015002110740169883, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9732309579849243]\n",
      " 1868 [D loss: 0.000354716379661113, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9727898240089417]\n",
      " 1869 [D loss: 0.000377115560695529, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9741184711456299]\n",
      " 1870 [D loss: 0.00016149850853253156, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9752132892608643]\n",
      " 1871 [D loss: 0.00023968318419065326, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9759724140167236]\n",
      " 1872 [D loss: 0.00046631786972284317, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9777610898017883]\n",
      " 1873 [D loss: 0.00033008214086294174, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9769545793533325]\n",
      " 1874 [D loss: 0.00023310698452405632, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9768157005310059]\n",
      " 1875 [D loss: 0.00012869367492385209, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9767436385154724]\n",
      " 1876 [D loss: 5.513295400305651e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9782413244247437]\n",
      " 1877 [D loss: 4.79376467410475e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9769315123558044]\n",
      " 1878 [D loss: 0.0004082092782482505, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9762032628059387]\n",
      " 1879 [D loss: 0.000391500536352396, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9784957766532898]\n",
      " 1880 [D loss: 0.00020464042609091848, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9768116474151611]\n",
      " 1881 [D loss: 0.0010029638651758432, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9772477746009827]\n",
      " 1882 [D loss: 9.606707317288965e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9784019589424133]\n",
      " 1883 [D loss: 0.00020889072038698941, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9819959402084351]\n",
      " 1884 [D loss: 0.000110225984826684, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.978644073009491]\n",
      " 1885 [D loss: 8.143112063407898e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9796819090843201]\n",
      " 1886 [D loss: 7.9252255090978e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9808424711227417]\n",
      " 1887 [D loss: 0.0002042278938461095, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.979971706867218]\n",
      " 1888 [D loss: 0.00015358999371528625, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9803787469863892]\n",
      " 1889 [D loss: 0.00017527083400636911, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9792194366455078]\n",
      " 1890 [D loss: 0.003341417293995619, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9746347069740295]\n",
      " 1891 [D loss: 0.000294982164632529, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9700965881347656]\n",
      " 1892 [D loss: 0.0004842354101128876, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9653342962265015]\n",
      " 1893 [D loss: 0.000728313229046762, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.963722825050354]\n",
      " 1894 [D loss: 0.0010239897528663278, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9573723077774048]\n",
      " 1895 [D loss: 0.0022785624023526907, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9573602676391602]\n",
      " 1896 [D loss: 0.0008585788309574127, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.958059549331665]\n",
      " 1897 [D loss: 0.000995115377008915, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.959096372127533]\n",
      " 1898 [D loss: 0.0001881401549326256, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9574636220932007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1899 [D loss: 0.001879184041172266, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9617545008659363]\n",
      " 1900 [D loss: 0.002113963942974806, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.963273823261261]\n",
      " 1901 [D loss: 0.0022371166851371527, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9666763544082642]\n",
      " 1902 [D loss: 0.0004470270359888673, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9667729139328003]\n",
      " 1903 [D loss: 0.00039058277616277337, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9698976278305054]\n",
      " 1904 [D loss: 0.00023897990467958152, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.971063494682312]\n",
      " 1905 [D loss: 0.00019312824588268995, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9699793457984924]\n",
      " 1906 [D loss: 0.0012351764598861337, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9718266725540161]\n",
      " 1907 [D loss: 0.0001955999032361433, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9715645909309387]\n",
      " 1908 [D loss: 0.00017324608052149415, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9731119871139526]\n",
      " 1909 [D loss: 0.00118097139056772, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9726541042327881]\n",
      " 1910 [D loss: 0.00019398561562411487, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9739090800285339]\n",
      " 1911 [D loss: 0.00017139890405815095, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9747558832168579]\n",
      " 1912 [D loss: 0.0005253322888165712, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9722330570220947]\n",
      " 1913 [D loss: 8.081118721747771e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9746049642562866]\n",
      " 1914 [D loss: 9.554448479320854e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9733338356018066]\n",
      " 1915 [D loss: 0.006765404716134071, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9755440950393677]\n",
      " 1916 [D loss: 0.001174728386104107, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9721253514289856]\n",
      " 1917 [D loss: 0.00022704849834553897, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9691522121429443]\n",
      " 1918 [D loss: 0.0005213997210375965, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9679609537124634]\n",
      " 1919 [D loss: 0.0005121369613334537, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9669532775878906]\n",
      " 1920 [D loss: 0.0001164872373919934, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9652000665664673]\n",
      " 1921 [D loss: 0.0013594100018963218, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9640424251556396]\n",
      " 1922 [D loss: 0.0008156847907230258, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9669189453125]\n",
      " 1923 [D loss: 0.0012078590225428343, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9684327840805054]\n",
      " 1924 [D loss: 0.000440384988905862, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9703811407089233]\n",
      " 1925 [D loss: 0.0005386078846640885, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9688475131988525]\n",
      " 1926 [D loss: 0.0013417508453130722, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9713394641876221]\n",
      " 1927 [D loss: 0.0008223953773267567, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9690921306610107]\n",
      " 1928 [D loss: 0.00036409584572538733, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9715909361839294]\n",
      " 1929 [D loss: 0.001011407352052629, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9718780517578125]\n",
      " 1930 [D loss: 0.0002033696073340252, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9717750549316406]\n",
      " 1931 [D loss: 0.00031844794284552336, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9737998247146606]\n",
      " 1932 [D loss: 0.0002778081689029932, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9744774103164673]\n",
      " 1933 [D loss: 8.336709288414568e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9746078252792358]\n",
      " 1934 [D loss: 2.6493587938603014e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9744378924369812]\n",
      " 1935 [D loss: 0.00015783519484102726, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9764400124549866]\n",
      " 1936 [D loss: 0.0008653972181491554, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9752211570739746]\n",
      " 1937 [D loss: 0.00038200741983018816, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9772858619689941]\n",
      " 1938 [D loss: 0.00020606756152119488, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.976876974105835]\n",
      " 1939 [D loss: 0.00040445185732096434, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9790169596672058]\n",
      " 1940 [D loss: 0.0005364664830267429, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9783760905265808]\n",
      " 1941 [D loss: 0.00032541388645768166, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9764326214790344]\n",
      " 1942 [D loss: 0.00011375278700143099, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9768608212471008]\n",
      " 1943 [D loss: 0.00011776982137234882, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9769358038902283]\n",
      " 1944 [D loss: 5.1641196478158236e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9780550003051758]\n",
      " 1945 [D loss: 0.0007215285440906882, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9773125648498535]\n",
      " 1946 [D loss: 0.0005636119749397039, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9782484769821167]\n",
      " 1947 [D loss: 0.0006935644196346402, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9770689010620117]\n",
      " 1948 [D loss: 0.00015394917863886803, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9782602190971375]\n",
      " 1949 [D loss: 0.00011290956172160804, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9772206544876099]\n",
      " 1950 [D loss: 0.00034935452276840806, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9798426628112793]\n",
      " 1951 [D loss: 0.0045471047051250935, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9708070755004883]\n",
      " 1952 [D loss: 0.0007102004601620138, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9689005613327026]\n",
      " 1953 [D loss: 0.004609107505530119, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9705213308334351]\n",
      " 1954 [D loss: 0.00027327166753821075, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9702185392379761]\n",
      " 1955 [D loss: 0.002775960136204958, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9716144800186157]\n",
      " 1956 [D loss: 0.0003333847562316805, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9695281982421875]\n",
      " 1957 [D loss: 0.00025627342984080315, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9720703959465027]\n",
      " 1958 [D loss: 0.0003174198209308088, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9718338847160339]\n",
      " 1959 [D loss: 0.0004825865908060223, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9755659103393555]\n",
      " 1960 [D loss: 0.0004937733174301684, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9769037961959839]\n",
      " 1961 [D loss: 0.00033099253778345883, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9770327806472778]\n",
      " 1962 [D loss: 0.00010733290400821716, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9758830666542053]\n",
      " 1963 [D loss: 0.0005017403746023774, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9788351058959961]\n",
      " 1964 [D loss: 7.948472921270877e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9802337884902954]\n",
      " 1965 [D loss: 5.436231731437147e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9800065755844116]\n",
      " 1966 [D loss: 0.0002575137186795473, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9809422492980957]\n",
      " 1967 [D loss: 0.0001523677201475948, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9816387891769409]\n",
      " 1968 [D loss: 9.118611342273653e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.982418954372406]\n",
      " 1969 [D loss: 0.002752087777480483, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9810339212417603]\n",
      " 1970 [D loss: 9.521056927042082e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9803034067153931]\n",
      " 1971 [D loss: 0.0004985028645023704, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9758111238479614]\n",
      " 1972 [D loss: 0.00026296672876924276, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.976471483707428]\n",
      " 1973 [D loss: 0.0004378144512884319, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.976384162902832]\n",
      " 1974 [D loss: 0.00011223219189560041, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9762418270111084]\n",
      " 1975 [D loss: 0.0001475372991990298, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9753605127334595]\n",
      " 1976 [D loss: 0.0001947262353496626, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9740867614746094]\n",
      " 1977 [D loss: 0.00010989457223331556, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9764381051063538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1978 [D loss: 0.00013696546375285834, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.978164792060852]\n",
      " 1979 [D loss: 0.00010089231363963336, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9740903377532959]\n",
      " 1980 [D loss: 0.00019046984380111098, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9772081971168518]\n",
      " 1981 [D loss: 0.00030243670335039496, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.976875901222229]\n",
      " 1982 [D loss: 0.00012166069791419432, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9795966148376465]\n",
      " 1983 [D loss: 0.00012629460252355784, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9780112504959106]\n",
      " 1984 [D loss: 0.0003354859072715044, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9787201881408691]\n",
      " 1985 [D loss: 7.501654181396589e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9784300327301025]\n",
      " 1986 [D loss: 0.0009360223193652928, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9797241687774658]\n",
      " 1987 [D loss: 0.00011596437980188057, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9798533916473389]\n",
      " 1988 [D loss: 0.00014473948976956308, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.980340838432312]\n",
      " 1989 [D loss: 0.0003879559226334095, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9804500341415405]\n",
      " 1990 [D loss: 0.0004367721558082849, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9811103343963623]\n",
      " 1991 [D loss: 0.0012766951695084572, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9807234406471252]\n",
      " 1992 [D loss: 0.0002262302441522479, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9790471196174622]\n",
      " 1993 [D loss: 0.00021320594532880932, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9816089868545532]\n",
      " 1994 [D loss: 0.000170139639521949, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9796912670135498]\n",
      " 1995 [D loss: 7.181543332990259e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9788229465484619]\n",
      " 1996 [D loss: 5.724327274947427e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9779744744300842]\n",
      " 1997 [D loss: 0.00032221502624452114, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9768208861351013]\n",
      " 1998 [D loss: 0.00010058631596621126, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9805428981781006]\n",
      " 1999 [D loss: 0.00014552082575391978, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9799985885620117]\n",
      " 2000 [D loss: 7.374890265055001e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9777350425720215]\n",
      " 2001 [D loss: 0.00013347477943170816, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9787282347679138]\n",
      " 2002 [D loss: 8.317479660036042e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9782734513282776]\n",
      " 2003 [D loss: 0.00011449462181190029, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9792008399963379]\n",
      " 2004 [D loss: 0.0002158747083740309, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9801400899887085]\n",
      " 2005 [D loss: 0.0003311408800072968, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9791335463523865]\n",
      " 2006 [D loss: 0.0012832338688895106, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9771659970283508]\n",
      " 2007 [D loss: 0.00011698015441652387, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9873760938644409]\n",
      " 2008 [D loss: 0.002287919633090496, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.980705738067627]\n",
      " 2009 [D loss: 0.0023510553874075413, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9781013131141663]\n",
      " 2010 [D loss: 4.7199871914926916e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9754756689071655]\n",
      " 2011 [D loss: 0.0002515341329853982, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9728173017501831]\n",
      " 2012 [D loss: 0.00031288820900954306, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9745075702667236]\n",
      " 2013 [D loss: 0.00012294446059968323, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9716516733169556]\n",
      " 2014 [D loss: 0.00016016789595596492, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9721021056175232]\n",
      " 2015 [D loss: 0.00051099993288517, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9700444340705872]\n",
      " 2016 [D loss: 0.0002736076130531728, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9664950370788574]\n",
      " 2017 [D loss: 0.0012128325179219246, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9689664840698242]\n",
      " 2018 [D loss: 0.0009399816044606268, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9654855728149414]\n",
      " 2019 [D loss: 0.0005143219605088234, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9649723768234253]\n",
      " 2020 [D loss: 0.0003814849187619984, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9706892967224121]\n",
      " 2021 [D loss: 0.0009190888376906514, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9703651070594788]\n",
      " 2022 [D loss: 8.421707025263458e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9697533845901489]\n",
      " 2023 [D loss: 0.0003292770707048476, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9728879332542419]\n",
      " 2024 [D loss: 0.0014731045812368393, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9703835248947144]\n",
      " 2025 [D loss: 0.0006094406126067042, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9728628396987915]\n",
      " 2026 [D loss: 0.0006213801098056138, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9733871221542358]\n",
      " 2027 [D loss: 0.0001809307432267815, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.974153995513916]\n",
      " 2028 [D loss: 0.00021817539527546614, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9728103876113892]\n",
      " 2029 [D loss: 0.0003340951807331294, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9734097719192505]\n",
      " 2030 [D loss: 0.00029201534925960004, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9732126593589783]\n",
      " 2031 [D loss: 0.000923659186810255, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9719446301460266]\n",
      " 2032 [D loss: 0.0002111310895998031, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9734935164451599]\n",
      " 2033 [D loss: 0.00023107293236535043, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9731885194778442]\n",
      " 2034 [D loss: 0.0002052110794465989, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9701032042503357]\n",
      " 2035 [D loss: 0.0002965127641800791, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9715229868888855]\n",
      " 2036 [D loss: 0.0006926050409674644, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9717476963996887]\n",
      " 2037 [D loss: 0.0012163333594799042, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9717209339141846]\n",
      " 2038 [D loss: 0.0009386941092088819, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9761874079704285]\n",
      " 2039 [D loss: 0.00013162253890186548, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9765412211418152]\n",
      " 2040 [D loss: 0.0007946921396069229, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9771332144737244]\n",
      " 2041 [D loss: 8.270508260466158e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9768235683441162]\n",
      " 2042 [D loss: 0.000769222155213356, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9756463170051575]\n",
      " 2043 [D loss: 0.00010515080793993548, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9755037426948547]\n",
      " 2044 [D loss: 0.0025272180791944265, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9753014445304871]\n",
      " 2045 [D loss: 9.940834570443258e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9744035005569458]\n",
      " 2046 [D loss: 0.00014769195695407689, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9755061864852905]\n",
      " 2047 [D loss: 0.0003565973602235317, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9755818843841553]\n",
      " 2048 [D loss: 0.0001738467690302059, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9772679805755615]\n",
      " 2049 [D loss: 0.000281021959381178, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9779003858566284]\n",
      " 2050 [D loss: 0.00015140879258979112, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9773321747779846]\n",
      " 2051 [D loss: 0.00011876707867486402, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9769046306610107]\n",
      " 2052 [D loss: 6.608914554817602e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9767503142356873]\n",
      " 2053 [D loss: 0.00014098567771725357, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9777325391769409]\n",
      " 2054 [D loss: 0.000263717956840992, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9789784550666809]\n",
      " 2055 [D loss: 0.0005808259011246264, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9783155918121338]\n",
      " 2056 [D loss: 0.00012279098154976964, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9768487215042114]\n",
      " 2057 [D loss: 6.624552770517766e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9768589735031128]\n",
      " 2058 [D loss: 6.325346475932747e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9774892330169678]\n",
      " 2059 [D loss: 0.00014184774772729725, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9744174480438232]\n",
      " 2060 [D loss: 0.005772009026259184, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9734306335449219]\n",
      " 2061 [D loss: 0.0003360772388987243, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9667541980743408]\n",
      " 2062 [D loss: 0.0004436740418896079, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9635220766067505]\n",
      " 2063 [D loss: 0.0009813295910134912, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9606081247329712]\n",
      " 2064 [D loss: 0.0003232425660826266, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9614261388778687]\n",
      " 2065 [D loss: 0.0006512051331810653, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9613958597183228]\n",
      " 2066 [D loss: 0.0007359803421422839, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.960457444190979]\n",
      " 2067 [D loss: 0.0015185218071565032, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9605014324188232]\n",
      " 2068 [D loss: 0.0009242667001672089, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.966862678527832]\n",
      " 2069 [D loss: 0.0002589084324426949, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9677624106407166]\n",
      " 2070 [D loss: 0.0015635212184861302, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9678704142570496]\n",
      " 2071 [D loss: 0.0007801259052939713, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9739158749580383]\n",
      " 2072 [D loss: 0.00032951319008134305, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.972097635269165]\n",
      " 2073 [D loss: 0.0006276028580032289, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9766039252281189]\n",
      " 2074 [D loss: 6.966597720747814e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9772510528564453]\n",
      " 2075 [D loss: 0.00264691817574203, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9798639416694641]\n",
      " 2076 [D loss: 8.763697405811399e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9814051389694214]\n",
      " 2077 [D loss: 9.349369793199003e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9827148914337158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2078 [D loss: 0.0004977682256139815, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9839134216308594]\n",
      " 2079 [D loss: 0.00013425030920188874, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9849525094032288]\n",
      " 2080 [D loss: 4.395842188387178e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9846827387809753]\n",
      " 2081 [D loss: 0.019673451781272888, acc_real: 96.875000, acc_fake: 100.000000] [G loss: 0.8202991485595703]\n",
      " 2082 [D loss: 0.04083665460348129, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.6560988426208496]\n",
      " 2083 [D loss: 0.09970851987600327, acc_real: 100.000000, acc_fake: 73.437500] [G loss: 0.5374784469604492]\n",
      " 2084 [D loss: 0.14369472861289978, acc_real: 100.000000, acc_fake: 65.625000] [G loss: 0.4780452251434326]\n",
      " 2085 [D loss: 0.18706083297729492, acc_real: 100.000000, acc_fake: 54.687500] [G loss: 0.41522330045700073]\n",
      " 2086 [D loss: 0.18341457843780518, acc_real: 100.000000, acc_fake: 56.250000] [G loss: 0.4146110713481903]\n",
      " 2087 [D loss: 0.24853333830833435, acc_real: 100.000000, acc_fake: 40.625000] [G loss: 0.3482664227485657]\n",
      " 2088 [D loss: 0.22150009870529175, acc_real: 100.000000, acc_fake: 51.562500] [G loss: 0.35690516233444214]\n",
      " 2089 [D loss: 0.27629685401916504, acc_real: 100.000000, acc_fake: 37.500000] [G loss: 0.33340221643447876]\n",
      " 2090 [D loss: 0.22906124591827393, acc_real: 100.000000, acc_fake: 50.000000] [G loss: 0.3203762471675873]\n",
      " 2091 [D loss: 0.24115930497646332, acc_real: 100.000000, acc_fake: 46.875000] [G loss: 0.26774120330810547]\n",
      " 2092 [D loss: 0.3135357201099396, acc_real: 100.000000, acc_fake: 31.250000] [G loss: 0.27750444412231445]\n",
      " 2093 [D loss: 0.269819438457489, acc_real: 100.000000, acc_fake: 35.937500] [G loss: 0.2623608112335205]\n",
      " 2094 [D loss: 0.33349472284317017, acc_real: 100.000000, acc_fake: 21.875000] [G loss: 0.24532540142536163]\n",
      " 2095 [D loss: 0.2825504541397095, acc_real: 100.000000, acc_fake: 34.375000] [G loss: 0.24788250029087067]\n",
      " 2096 [D loss: 0.2340267449617386, acc_real: 100.000000, acc_fake: 40.625000] [G loss: 0.21839556097984314]\n",
      " 2097 [D loss: 0.30918824672698975, acc_real: 100.000000, acc_fake: 31.250000] [G loss: 0.22560018301010132]\n",
      " 2098 [D loss: 0.29740720987319946, acc_real: 100.000000, acc_fake: 25.000000] [G loss: 0.16983956098556519]\n",
      " 2099 [D loss: 0.2977885603904724, acc_real: 100.000000, acc_fake: 20.312500] [G loss: 0.17949526011943817]\n",
      " 2100 [D loss: 0.29703494906425476, acc_real: 100.000000, acc_fake: 21.875000] [G loss: 0.17789535224437714]\n",
      " 2101 [D loss: 0.25595352053642273, acc_real: 100.000000, acc_fake: 34.375000] [G loss: 0.19386537373065948]\n",
      " 2102 [D loss: 0.23849686980247498, acc_real: 100.000000, acc_fake: 26.562500] [G loss: 0.24779671430587769]\n",
      " 2103 [D loss: 0.20146781206130981, acc_real: 100.000000, acc_fake: 48.437500] [G loss: 0.2499312311410904]\n",
      " 2104 [D loss: 0.20229342579841614, acc_real: 100.000000, acc_fake: 53.125000] [G loss: 0.30460935831069946]\n",
      " 2105 [D loss: 0.2071029543876648, acc_real: 100.000000, acc_fake: 54.687500] [G loss: 0.36237913370132446]\n",
      " 2106 [D loss: 0.1779499053955078, acc_real: 100.000000, acc_fake: 59.375000] [G loss: 0.389793336391449]\n",
      " 2107 [D loss: 0.17537164688110352, acc_real: 100.000000, acc_fake: 62.500000] [G loss: 0.40801095962524414]\n",
      " 2108 [D loss: 0.1570093333721161, acc_real: 100.000000, acc_fake: 62.500000] [G loss: 0.47362738847732544]\n",
      " 2109 [D loss: 0.18256328999996185, acc_real: 100.000000, acc_fake: 54.687500] [G loss: 0.4719596207141876]\n",
      " 2110 [D loss: 0.13745468854904175, acc_real: 100.000000, acc_fake: 62.500000] [G loss: 0.476997047662735]\n",
      " 2111 [D loss: 0.16608837246894836, acc_real: 100.000000, acc_fake: 56.250000] [G loss: 0.496504545211792]\n",
      " 2112 [D loss: 0.1330452263355255, acc_real: 100.000000, acc_fake: 64.062500] [G loss: 0.5199984312057495]\n",
      " 2113 [D loss: 0.11464333534240723, acc_real: 100.000000, acc_fake: 68.750000] [G loss: 0.5242345333099365]\n",
      " 2114 [D loss: 0.10416775941848755, acc_real: 100.000000, acc_fake: 70.312500] [G loss: 0.5695533156394958]\n",
      " 2115 [D loss: 0.0759996846318245, acc_real: 100.000000, acc_fake: 78.125000] [G loss: 0.5589726567268372]\n",
      " 2116 [D loss: 0.0542663112282753, acc_real: 100.000000, acc_fake: 84.375000] [G loss: 0.6134375333786011]\n",
      " 2117 [D loss: 0.05992423743009567, acc_real: 100.000000, acc_fake: 82.812500] [G loss: 0.6330176591873169]\n",
      " 2118 [D loss: 0.04225759580731392, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.6731929779052734]\n",
      " 2119 [D loss: 0.030949639156460762, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.717303991317749]\n",
      " 2120 [D loss: 0.019067935645580292, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.747809886932373]\n",
      " 2121 [D loss: 0.012455297634005547, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.7932175397872925]\n",
      " 2122 [D loss: 0.006891071796417236, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8297035694122314]\n",
      " 2123 [D loss: 0.0038024112582206726, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8547782897949219]\n",
      " 2124 [D loss: 0.0025975340977311134, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8636508584022522]\n",
      " 2125 [D loss: 0.0028003044426441193, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8857476115226746]\n",
      " 2126 [D loss: 0.0019483313662931323, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8946802020072937]\n",
      " 2127 [D loss: 0.002013088669627905, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8969757556915283]\n",
      " 2128 [D loss: 0.0016246835002675653, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9010593891143799]\n",
      " 2129 [D loss: 0.0017897258512675762, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9086746573448181]\n",
      " 2130 [D loss: 0.0016371426172554493, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9089933037757874]\n",
      " 2131 [D loss: 0.009791040793061256, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9093841910362244]\n",
      " 2132 [D loss: 0.00927090272307396, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9164155721664429]\n",
      " 2133 [D loss: 0.0019235952058807015, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9155288934707642]\n",
      " 2134 [D loss: 0.00976022519171238, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9198135137557983]\n",
      " 2135 [D loss: 0.025536971166729927, acc_real: 95.312500, acc_fake: 100.000000] [G loss: 0.8999033570289612]\n",
      " 2136 [D loss: 0.0025662637781351805, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8905652761459351]\n",
      " 2137 [D loss: 0.0024881348945200443, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8694077730178833]\n",
      " 2138 [D loss: 0.004956662189215422, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.862134575843811]\n",
      " 2139 [D loss: 0.0038814181461930275, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8365026116371155]\n",
      " 2140 [D loss: 0.0048357052728533745, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8381681442260742]\n",
      " 2141 [D loss: 0.005036721471697092, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.831089973449707]\n",
      " 2142 [D loss: 0.007361160591244698, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8228169679641724]\n",
      " 2143 [D loss: 0.005752295255661011, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8195320963859558]\n",
      " 2144 [D loss: 0.007113856263458729, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8267658948898315]\n",
      " 2145 [D loss: 0.007735798601061106, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.828589916229248]\n",
      " 2146 [D loss: 0.0050056250765919685, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8346211910247803]\n",
      " 2147 [D loss: 0.005858948454260826, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8357473015785217]\n",
      " 2148 [D loss: 0.004656811710447073, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8486192226409912]\n",
      " 2149 [D loss: 0.004172163549810648, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8509399890899658]\n",
      " 2150 [D loss: 0.00396798737347126, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8556767702102661]\n",
      " 2151 [D loss: 0.0037454699631780386, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.867531418800354]\n",
      " 2152 [D loss: 0.003700210014358163, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.87102872133255]\n",
      " 2153 [D loss: 0.002100313315168023, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.877813458442688]\n",
      " 2154 [D loss: 0.003441266715526581, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8795558214187622]\n",
      " 2155 [D loss: 0.0029057294595986605, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8827130794525146]\n",
      " 2156 [D loss: 0.0019332602387294173, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8882414102554321]\n",
      " 2157 [D loss: 0.0025652453768998384, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8866152763366699]\n",
      " 2158 [D loss: 0.001814523944631219, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.891049325466156]\n",
      " 2159 [D loss: 0.0017175681423395872, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9030385613441467]\n",
      " 2160 [D loss: 0.0014376414474099874, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9012459516525269]\n",
      " 2161 [D loss: 0.001139212166890502, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9042505025863647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2162 [D loss: 0.001540337223559618, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8997008800506592]\n",
      " 2163 [D loss: 0.0015580594772472978, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9028092622756958]\n",
      " 2164 [D loss: 0.001523700193502009, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9059484601020813]\n",
      " 2165 [D loss: 0.0010382726322859526, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9075777530670166]\n",
      " 2166 [D loss: 0.001515103504061699, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.90936678647995]\n",
      " 2167 [D loss: 0.0012589495163410902, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9093459844589233]\n",
      " 2168 [D loss: 0.0014285794459283352, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9108431935310364]\n",
      " 2169 [D loss: 0.0015070968074724078, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9093208909034729]\n",
      " 2170 [D loss: 0.0012739605735987425, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9114763736724854]\n",
      " 2171 [D loss: 0.0015844508307054639, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9105554819107056]\n",
      " 2172 [D loss: 0.0013325951294973493, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9134210348129272]\n",
      " 2173 [D loss: 0.0012601828202605247, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9159845113754272]\n",
      " 2174 [D loss: 0.0015142057090997696, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9172638654708862]\n",
      " 2175 [D loss: 0.0007856737938709557, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9176037311553955]\n",
      " 2176 [D loss: 0.0012364371214061975, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.922590970993042]\n",
      " 2177 [D loss: 0.0012041873997077346, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9177669286727905]\n",
      " 2178 [D loss: 0.0010822153417393565, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9197602272033691]\n",
      " 2179 [D loss: 0.0011771977879106998, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9209733009338379]\n",
      " 2180 [D loss: 0.0009955440182238817, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9228770136833191]\n",
      " 2181 [D loss: 0.0015335853677242994, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9211338758468628]\n",
      " 2182 [D loss: 0.001143022207543254, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9212993383407593]\n",
      " 2183 [D loss: 0.0011184586910530925, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.922829806804657]\n",
      " 2184 [D loss: 0.0010001722257584333, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9206997156143188]\n",
      " 2185 [D loss: 0.0010279642883688211, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9235845804214478]\n",
      " 2186 [D loss: 0.0010282599832862616, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9211184978485107]\n",
      " 2187 [D loss: 0.0011611515656113625, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9186583757400513]\n",
      " 2188 [D loss: 0.0012849436607211828, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9203544855117798]\n",
      " 2189 [D loss: 0.0010709511116147041, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9168890118598938]\n",
      " 2190 [D loss: 0.0009293672046624124, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9192981123924255]\n",
      " 2191 [D loss: 0.0011232768883928657, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9176269769668579]\n",
      " 2192 [D loss: 0.0013567982241511345, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9188473224639893]\n",
      " 2193 [D loss: 0.0012294193729758263, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9160295724868774]\n",
      " 2194 [D loss: 0.0013269722694531083, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9174282550811768]\n",
      " 2195 [D loss: 0.0015342429978772998, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9206720590591431]\n",
      " 2196 [D loss: 0.0018264310201629996, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9189931154251099]\n",
      " 2197 [D loss: 0.001351088983938098, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9178572297096252]\n",
      " 2198 [D loss: 0.0017841969383880496, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9164737462997437]\n",
      " 2199 [D loss: 0.0013787518255412579, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9173622131347656]\n",
      " 2200 [D loss: 0.001583809033036232, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9138891696929932]\n",
      " 2201 [D loss: 0.0017419545911252499, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9187216758728027]\n",
      " 2202 [D loss: 0.0017514403443783522, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9146246314048767]\n",
      " 2203 [D loss: 0.0018318949732929468, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9159859418869019]\n",
      " 2204 [D loss: 0.0032887018751353025, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9206455945968628]\n",
      " 2205 [D loss: 0.0013382971519604325, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9175306558609009]\n",
      " 2206 [D loss: 0.0016869996907189488, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9173089265823364]\n",
      " 2207 [D loss: 0.0011444094125181437, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9226782321929932]\n",
      " 2208 [D loss: 0.0025800270959734917, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.922044575214386]\n",
      " 2209 [D loss: 0.001637652749195695, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9208297729492188]\n",
      " 2210 [D loss: 0.0014524311991408467, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9261474013328552]\n",
      " 2211 [D loss: 0.0015991220716387033, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9240309596061707]\n",
      " 2212 [D loss: 0.001223159022629261, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9239705801010132]\n",
      " 2213 [D loss: 0.002376663964241743, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9284559488296509]\n",
      " 2214 [D loss: 0.006921115331351757, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9266364574432373]\n",
      " 2215 [D loss: 0.0024003672879189253, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9299185872077942]\n",
      " 2216 [D loss: 0.00110148498788476, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9315919876098633]\n",
      " 2217 [D loss: 0.0029413755983114243, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9338704347610474]\n",
      " 2218 [D loss: 0.0013461896451190114, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9360092282295227]\n",
      " 2219 [D loss: 0.001300484873354435, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9365999698638916]\n",
      " 2220 [D loss: 0.0008685579523444176, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9405978918075562]\n",
      " 2221 [D loss: 0.0012122406624257565, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9377329349517822]\n",
      " 2222 [D loss: 0.001668489072471857, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.939877986907959]\n",
      " 2223 [D loss: 0.0011839106446132064, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9395215511322021]\n",
      " 2224 [D loss: 0.0003839061828330159, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9428215026855469]\n",
      " 2225 [D loss: 0.0018970818491652608, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9434065818786621]\n",
      " 2226 [D loss: 0.0019900500774383545, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9424208402633667]\n",
      " 2227 [D loss: 0.0009347903542220592, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9419368505477905]\n",
      " 2228 [D loss: 0.0013985354453325272, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9451792240142822]\n",
      " 2229 [D loss: 0.001555087510496378, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9493021368980408]\n",
      " 2230 [D loss: 0.0006082482868805528, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9484931826591492]\n",
      " 2231 [D loss: 0.00082780746743083, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9469468593597412]\n",
      " 2232 [D loss: 0.0018313108012080193, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9454308748245239]\n",
      " 2233 [D loss: 0.00034777019754983485, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9497970938682556]\n",
      " 2234 [D loss: 0.0013487058458849788, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9524130821228027]\n",
      " 2235 [D loss: 0.0025909666437655687, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9469725489616394]\n",
      " 2236 [D loss: 0.0013992381282150745, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9580353498458862]\n",
      " 2237 [D loss: 0.0014746959786862135, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.955298900604248]\n",
      " 2238 [D loss: 0.0018637458560988307, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9571908116340637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2239 [D loss: 0.004127280320972204, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9573330283164978]\n",
      " 2240 [D loss: 0.0015452245716005564, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9576737284660339]\n",
      " 2241 [D loss: 0.0007154465420171618, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9600301384925842]\n",
      " 2242 [D loss: 0.0021294166799634695, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9629943370819092]\n",
      " 2243 [D loss: 0.0008606497431173921, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9665331840515137]\n",
      " 2244 [D loss: 0.0009649702114984393, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9663528203964233]\n",
      " 2245 [D loss: 0.00029489237931557, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9702925086021423]\n",
      " 2246 [D loss: 0.00040773849468678236, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9705846905708313]\n",
      " 2247 [D loss: 0.0003095740976277739, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9714574217796326]\n",
      " 2248 [D loss: 0.0014368407428264618, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9703300595283508]\n",
      " 2249 [D loss: 0.0037977159954607487, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9720361232757568]\n",
      " 2250 [D loss: 0.005300522781908512, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9711132049560547]\n",
      " 2251 [D loss: 0.0003322094853501767, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9736167192459106]\n",
      " 2252 [D loss: 0.00016309163765981793, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9708682298660278]\n",
      " 2253 [D loss: 0.00038072001188993454, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9748438596725464]\n",
      " 2254 [D loss: 0.0003520996542647481, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9699053764343262]\n",
      " 2255 [D loss: 0.0018039951100945473, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9690790176391602]\n",
      " 2256 [D loss: 0.00033059160341508687, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9712440371513367]\n",
      " 2257 [D loss: 0.000488542253151536, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9701690077781677]\n",
      " 2258 [D loss: 0.0004226875025779009, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9709503054618835]\n",
      " 2259 [D loss: 0.0002494523942004889, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9725952744483948]\n",
      " 2260 [D loss: 0.0001640913833398372, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9707719087600708]\n",
      " 2261 [D loss: 0.0007379533490166068, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9680891036987305]\n",
      " 2262 [D loss: 0.00021610069961752743, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9694175720214844]\n",
      " 2263 [D loss: 0.0003625188546720892, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9701960682868958]\n",
      " 2264 [D loss: 0.00023520004469901323, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9658918380737305]\n",
      " 2265 [D loss: 0.0002079900004900992, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9669747352600098]\n",
      " 2266 [D loss: 0.0007388395606540143, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9703267812728882]\n",
      " 2267 [D loss: 0.0001410928089171648, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9680039286613464]\n",
      " 2268 [D loss: 0.00021757656941190362, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9688009023666382]\n",
      " 2269 [D loss: 0.00012954419071320444, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9716756343841553]\n",
      " 2270 [D loss: 0.00018818587705027312, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9726112484931946]\n",
      " 2271 [D loss: 0.0014799595810472965, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9700950384140015]\n",
      " 2272 [D loss: 0.0004940595244988799, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9706215262413025]\n",
      " 2273 [D loss: 0.0008373918826691806, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9683145880699158]\n",
      " 2274 [D loss: 0.0011164320167154074, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9662188291549683]\n",
      " 2275 [D loss: 0.0008760840864852071, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9714487195014954]\n",
      " 2276 [D loss: 0.00014080309483688325, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.966812252998352]\n",
      " 2277 [D loss: 0.002532708691433072, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9685351848602295]\n",
      " 2278 [D loss: 0.00041881584911607206, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9647917151451111]\n",
      " 2279 [D loss: 0.00277955480851233, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9691381454467773]\n",
      " 2280 [D loss: 0.00029797054594382644, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9732842445373535]\n",
      " 2281 [D loss: 0.0002547700423747301, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9721251130104065]\n",
      " 2282 [D loss: 0.00027096783742308617, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9726400375366211]\n",
      " 2283 [D loss: 0.0005174519028514624, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.974155068397522]\n",
      " 2284 [D loss: 0.00019976796465925872, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9726200103759766]\n",
      " 2285 [D loss: 0.0008327123941853642, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9759676456451416]\n",
      " 2286 [D loss: 0.00010204219142906368, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9769940376281738]\n",
      " 2287 [D loss: 0.0002616856654640287, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.972177267074585]\n",
      " 2288 [D loss: 0.0016326538752764463, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9765909910202026]\n",
      " 2289 [D loss: 0.00011657619324978441, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9757516980171204]\n",
      " 2290 [D loss: 0.0003992172423750162, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9815959930419922]\n",
      " 2291 [D loss: 7.181590626714751e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9827456474304199]\n",
      " 2292 [D loss: 0.0065184724517166615, acc_real: 98.437500, acc_fake: 98.437500] [G loss: 0.9797708988189697]\n",
      " 2293 [D loss: 0.0007173979538492858, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9803124666213989]\n",
      " 2294 [D loss: 0.00012464936298783869, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9805280566215515]\n",
      " 2295 [D loss: 0.0027769252192229033, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9817630648612976]\n",
      " 2296 [D loss: 0.0015440206043422222, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9769363403320312]\n",
      " 2297 [D loss: 0.0006880031432956457, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9771560430526733]\n",
      " 2298 [D loss: 0.0013652394991368055, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9780946969985962]\n",
      " 2299 [D loss: 0.00099760212469846, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9770746231079102]\n",
      " 2300 [D loss: 0.0002544104354456067, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9770631790161133]\n",
      " 2301 [D loss: 0.0012080500600859523, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9717850685119629]\n",
      " 2302 [D loss: 0.00040795499808155, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.978166401386261]\n",
      " 2303 [D loss: 0.0023448155261576176, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9739347696304321]\n",
      " 2304 [D loss: 0.00016099160711746663, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9784092307090759]\n",
      " 2305 [D loss: 0.0002661943144630641, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9747395515441895]\n",
      " 2306 [D loss: 0.00047037049080245197, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9751646518707275]\n",
      " 2307 [D loss: 0.00034957879688590765, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9759843349456787]\n",
      " 2308 [D loss: 5.417035936261527e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9792135953903198]\n",
      " 2309 [D loss: 0.006689826957881451, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9759570360183716]\n",
      " 2310 [D loss: 0.0017526100855320692, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9767928123474121]\n",
      " 2311 [D loss: 0.00011316970631014556, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9768528938293457]\n",
      " 2312 [D loss: 0.0010520629584789276, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9780725240707397]\n",
      " 2313 [D loss: 0.002174877794459462, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9779586791992188]\n",
      " 2314 [D loss: 0.0014699086314067245, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9775079488754272]\n",
      " 2315 [D loss: 0.0001751456002239138, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.978864312171936]\n",
      " 2316 [D loss: 0.00015975987480487674, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9793287515640259]\n",
      " 2317 [D loss: 0.0006175176822580397, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9770993590354919]\n",
      " 2318 [D loss: 4.92738327011466e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9793887138366699]\n",
      " 2319 [D loss: 0.0001425098453182727, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9799987077713013]\n",
      " 2320 [D loss: 0.00012933439575135708, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9813206195831299]\n",
      " 2321 [D loss: 0.001972783589735627, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9829117059707642]\n",
      " 2322 [D loss: 0.0002036002406384796, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9831237196922302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2323 [D loss: 0.0001509826979599893, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9801913499832153]\n",
      " 2324 [D loss: 8.691685798112303e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9779411554336548]\n",
      " 2325 [D loss: 0.0007880861521698534, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9774531126022339]\n",
      " 2326 [D loss: 0.00017926341388374567, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9815989136695862]\n",
      " 2327 [D loss: 0.00038527935976162553, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9795243144035339]\n",
      " 2328 [D loss: 0.00012955736019648612, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9757674932479858]\n",
      " 2329 [D loss: 0.0046155513264238834, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9821279048919678]\n",
      " 2330 [D loss: 4.625874134944752e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9822036027908325]\n",
      " 2331 [D loss: 0.0010174036724492908, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9871860146522522]\n",
      " 2332 [D loss: 0.0002784885582514107, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.985689640045166]\n",
      " 2333 [D loss: 0.00011159369751112536, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.986801266670227]\n",
      " 2334 [D loss: 0.00015627940592821687, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9846363067626953]\n",
      " 2335 [D loss: 0.00017157720867544413, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9872908592224121]\n",
      " 2336 [D loss: 0.00022606036509387195, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.986409068107605]\n",
      " 2337 [D loss: 8.485511352773756e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9858114719390869]\n",
      " 2338 [D loss: 0.003378277877345681, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9815508127212524]\n",
      " 2339 [D loss: 0.00014002191892359406, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.979022741317749]\n",
      " 2340 [D loss: 6.394618685590103e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9802286624908447]\n",
      " 2341 [D loss: 0.00011758521577576175, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9804490804672241]\n",
      " 2342 [D loss: 0.0005375831387937069, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9846646785736084]\n",
      " 2343 [D loss: 0.0017764867516234517, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.979117214679718]\n",
      " 2344 [D loss: 0.006386160850524902, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9813444018363953]\n",
      " 2345 [D loss: 0.00021455205569509417, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.98191899061203]\n",
      " 2346 [D loss: 0.002251272089779377, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9832037687301636]\n",
      " 2347 [D loss: 0.0001289640786126256, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9846417903900146]\n",
      " 2348 [D loss: 0.00024603743804618716, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9852243065834045]\n",
      " 2349 [D loss: 0.0013395206769928336, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9817342162132263]\n",
      " 2350 [D loss: 0.00039033539360389113, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9821648597717285]\n",
      " 2351 [D loss: 0.002136219758540392, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9848407506942749]\n",
      " 2352 [D loss: 0.0017795134335756302, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9841629266738892]\n",
      " 2353 [D loss: 0.00027733598835766315, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9847362041473389]\n",
      " 2354 [D loss: 0.0008950017509050667, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9823875427246094]\n",
      " 2355 [D loss: 0.0014167166082188487, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9809530377388]\n",
      " 2356 [D loss: 0.0021183479111641645, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9841740131378174]\n",
      " 2357 [D loss: 2.5179251679219306e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9857077598571777]\n",
      " 2358 [D loss: 0.00012040915316902101, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9858055114746094]\n",
      " 2359 [D loss: 0.0008064819849096239, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9850484132766724]\n",
      " 2360 [D loss: 0.00037945149233564734, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9858788251876831]\n",
      " 2361 [D loss: 0.0005493857897818089, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.986312747001648]\n",
      " 2362 [D loss: 0.00245088804513216, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9865249395370483]\n",
      " 2363 [D loss: 0.0006391252973116934, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9822347164154053]\n",
      " 2364 [D loss: 0.00010560771625023335, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9842672348022461]\n",
      " 2365 [D loss: 0.0006637375336140394, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9830777645111084]\n",
      " 2366 [D loss: 0.00020255756680853665, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9818398952484131]\n",
      " 2367 [D loss: 7.997246575541794e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9849292039871216]\n",
      " 2368 [D loss: 0.00021537391876336187, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9839535355567932]\n",
      " 2369 [D loss: 0.0003346871817484498, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9815453886985779]\n",
      " 2370 [D loss: 6.326116272248328e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.980010449886322]\n",
      " 2371 [D loss: 6.474374094977975e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9811097383499146]\n",
      " 2372 [D loss: 0.0025531162973493338, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9825172424316406]\n",
      " 2373 [D loss: 0.0007850181427784264, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9849876165390015]\n",
      " 2374 [D loss: 0.00046660186490044, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9831241369247437]\n",
      " 2375 [D loss: 0.0001667145115789026, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9844231605529785]\n",
      " 2376 [D loss: 0.0001589027961017564, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9844470024108887]\n",
      " 2377 [D loss: 0.0012811997439712286, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9854749441146851]\n",
      " 2378 [D loss: 7.28440354578197e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9887974262237549]\n",
      " 2379 [D loss: 9.148714889306575e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9869544506072998]\n",
      " 2380 [D loss: 0.0008707829983904958, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9883286952972412]\n",
      " 2381 [D loss: 5.243958730716258e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9892887473106384]\n",
      " 2382 [D loss: 0.0012575556756928563, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9869164824485779]\n",
      " 2383 [D loss: 0.00022820470621809363, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9888533353805542]\n",
      " 2384 [D loss: 0.00013128927093930542, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9859785437583923]\n",
      " 2385 [D loss: 0.00022399715089704841, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9885686039924622]\n",
      " 2386 [D loss: 0.0001642590359551832, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9873358011245728]\n",
      " 2387 [D loss: 0.00035085732815787196, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9880820512771606]\n",
      " 2388 [D loss: 9.39012534217909e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9858121871948242]\n",
      " 2389 [D loss: 0.0007106144912540913, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9864420294761658]\n",
      " 2390 [D loss: 4.0566985262557864e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9882627129554749]\n",
      " 2391 [D loss: 3.7681656976928934e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9884765148162842]\n",
      " 2392 [D loss: 3.240777732571587e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9852555990219116]\n",
      " 2393 [D loss: 0.00011988660116912797, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9881870150566101]\n",
      " 2394 [D loss: 0.00023275730200111866, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9886152744293213]\n",
      " 2395 [D loss: 5.365812103264034e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9883871078491211]\n",
      " 2396 [D loss: 0.0027511396910995245, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9877637624740601]\n",
      " 2397 [D loss: 0.0012912371894344687, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9865928888320923]\n",
      " 2398 [D loss: 0.0006352827185764909, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9851230382919312]\n",
      " 2399 [D loss: 0.00025399518199265003, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.986089825630188]\n",
      " 2400 [D loss: 3.964122151955962e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9852895736694336]\n",
      " 2401 [D loss: 0.00033242395147681236, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9839983582496643]\n",
      " 2402 [D loss: 0.0008575759711675346, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9839814901351929]\n",
      " 2403 [D loss: 0.00025852074031718075, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9864749908447266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2404 [D loss: 0.000538920343387872, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9843685626983643]\n",
      " 2405 [D loss: 3.572144851204939e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9857004880905151]\n",
      " 2406 [D loss: 0.000279054744169116, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9853736162185669]\n",
      " 2407 [D loss: 5.2484603656921536e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9868268966674805]\n",
      " 2408 [D loss: 0.0013249412877485156, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9855137467384338]\n",
      " 2409 [D loss: 1.088510180125013e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9874699115753174]\n",
      " 2410 [D loss: 0.0002096442913170904, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9874885678291321]\n",
      " 2411 [D loss: 0.0001737731508910656, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9894622564315796]\n",
      " 2412 [D loss: 2.5093295334954746e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.989223062992096]\n",
      " 2413 [D loss: 0.00012604467337951064, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9892091751098633]\n",
      " 2414 [D loss: 5.3207029850455e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9901214838027954]\n",
      " 2415 [D loss: 5.967964170849882e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9886296987533569]\n",
      " 2416 [D loss: 7.891727000242099e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9903470873832703]\n",
      " 2417 [D loss: 0.00026659807190299034, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9882979989051819]\n",
      " 2418 [D loss: 6.450027285609394e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.990174412727356]\n",
      " 2419 [D loss: 4.5117110857972875e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9909437298774719]\n",
      " 2420 [D loss: 0.0002524141746107489, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9908763766288757]\n",
      " 2421 [D loss: 9.301683166995645e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9884178042411804]\n",
      " 2422 [D loss: 8.489972969982773e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9899182319641113]\n",
      " 2423 [D loss: 0.00013528126874007285, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9886717200279236]\n",
      " 2424 [D loss: 3.8377293094526976e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9905498623847961]\n",
      " 2425 [D loss: 0.0005586867337115109, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9891091585159302]\n",
      " 2426 [D loss: 4.919241473544389e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9902666807174683]\n",
      " 2427 [D loss: 5.001305544283241e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9878448247909546]\n",
      " 2428 [D loss: 0.00013567728456109762, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9895824193954468]\n",
      " 2429 [D loss: 0.0001060651775333099, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9894353747367859]\n",
      " 2430 [D loss: 0.00020888628205284476, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9885583519935608]\n",
      " 2431 [D loss: 1.282942685065791e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9881861805915833]\n",
      " 2432 [D loss: 0.0005784071981906891, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.987669825553894]\n",
      " 2433 [D loss: 0.0001007670653052628, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9880527257919312]\n",
      " 2434 [D loss: 3.026028753083665e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9883383512496948]\n",
      " 2435 [D loss: 3.810094131040387e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9893307685852051]\n",
      " 2436 [D loss: 0.003102380782365799, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9878966212272644]\n",
      " 2437 [D loss: 7.493895100196823e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9890809059143066]\n",
      " 2438 [D loss: 0.001264610094949603, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9895171523094177]\n",
      " 2439 [D loss: 1.3473207218339667e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9888088703155518]\n",
      " 2440 [D loss: 6.95340713718906e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9888579845428467]\n",
      " 2441 [D loss: 1.489808528276626e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9910520911216736]\n",
      " 2442 [D loss: 0.00023917388170957565, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9906513690948486]\n",
      " 2443 [D loss: 8.787275874055922e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9904330968856812]\n",
      " 2444 [D loss: 3.946400465792976e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9906299114227295]\n",
      " 2445 [D loss: 0.00011833278404083103, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9911093711853027]\n",
      " 2446 [D loss: 4.8244590288959444e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9913848042488098]\n",
      " 2447 [D loss: 0.0020016354974359274, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9911356568336487]\n",
      " 2448 [D loss: 8.031402830965817e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927582144737244]\n",
      " 2449 [D loss: 0.0001762679312378168, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9915120601654053]\n",
      " 2450 [D loss: 6.752930858056061e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9930729866027832]\n",
      " 2451 [D loss: 0.0037526432424783707, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9926673769950867]\n",
      " 2452 [D loss: 0.00011816198821179569, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9916836023330688]\n",
      " 2453 [D loss: 0.0012231598375365138, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9908286929130554]\n",
      " 2454 [D loss: 0.0003221974475309253, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9922299385070801]\n",
      " 2455 [D loss: 6.9441107370948885e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9914389848709106]\n",
      " 2456 [D loss: 0.000168472935911268, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9911272525787354]\n",
      " 2457 [D loss: 0.0008120605489239097, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.989124059677124]\n",
      " 2458 [D loss: 2.591321390355006e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9881336092948914]\n",
      " 2459 [D loss: 0.0015919083962216973, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9885032176971436]\n",
      " 2460 [D loss: 0.00041907804552465677, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9899453520774841]\n",
      " 2461 [D loss: 0.00017815826868172735, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9883868098258972]\n",
      " 2462 [D loss: 3.573123103706166e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9899572730064392]\n",
      " 2463 [D loss: 0.0002820959489326924, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9892493486404419]\n",
      " 2464 [D loss: 5.180342122912407e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9914717674255371]\n",
      " 2465 [D loss: 2.7292397135170177e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9876192808151245]\n",
      " 2466 [D loss: 9.854690870270133e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9877297878265381]\n",
      " 2467 [D loss: 6.750445027137175e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9896305203437805]\n",
      " 2468 [D loss: 0.00029736600117757916, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9878631830215454]\n",
      " 2469 [D loss: 1.9013181372429244e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9879664182662964]\n",
      " 2470 [D loss: 0.00010665407171472907, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9891014695167542]\n",
      " 2471 [D loss: 5.058619353803806e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9881438612937927]\n",
      " 2472 [D loss: 1.2954213161719963e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9896408915519714]\n",
      " 2473 [D loss: 0.00023204321041703224, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9908727407455444]\n",
      " 2474 [D loss: 0.0003838465199805796, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9903070330619812]\n",
      " 2475 [D loss: 0.0004222187853883952, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9906395673751831]\n",
      " 2476 [D loss: 2.5937522877939045e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9897994995117188]\n",
      " 2477 [D loss: 0.0001199229882331565, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9913519620895386]\n",
      " 2478 [D loss: 0.0001734502729959786, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9917696118354797]\n",
      " 2479 [D loss: 1.842202982516028e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9905704855918884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2480 [D loss: 9.458079148316756e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9925593137741089]\n",
      " 2481 [D loss: 3.126132287434302e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9901328682899475]\n",
      " 2482 [D loss: 0.0006643827073276043, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9916709065437317]\n",
      " 2483 [D loss: 1.798101766325999e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9908192753791809]\n",
      " 2484 [D loss: 8.333919140568469e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9920041561126709]\n",
      " 2485 [D loss: 0.00010043684596894309, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9909520745277405]\n",
      " 2486 [D loss: 9.322129335487261e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9920428991317749]\n",
      " 2487 [D loss: 9.670196595834568e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9924013614654541]\n",
      " 2488 [D loss: 0.00016390546807087958, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927559494972229]\n",
      " 2489 [D loss: 4.212067142361775e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933328032493591]\n",
      " 2490 [D loss: 1.8110684322891757e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9931243658065796]\n",
      " 2491 [D loss: 8.878317748894915e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9928540587425232]\n",
      " 2492 [D loss: 1.702632653177716e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9926559925079346]\n",
      " 2493 [D loss: 0.00012360229447949678, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933208227157593]\n",
      " 2494 [D loss: 1.1863987310789526e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935324192047119]\n",
      " 2495 [D loss: 1.9669296307256445e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993633508682251]\n",
      " 2496 [D loss: 1.404004797223024e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993808388710022]\n",
      " 2497 [D loss: 4.751233063871041e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935797452926636]\n",
      " 2498 [D loss: 0.00015129799430724233, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9930981397628784]\n",
      " 2499 [D loss: 8.840089321893174e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936203956604004]\n",
      " 2500 [D loss: 4.8771093133836985e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935219883918762]\n",
      " 2501 [D loss: 1.4803275007579941e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937812089920044]\n",
      " 2502 [D loss: 0.00016918971959967166, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927803874015808]\n",
      " 2503 [D loss: 1.0798593393701594e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9922768473625183]\n",
      " 2504 [D loss: 1.9425913706072606e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9932605028152466]\n",
      " 2505 [D loss: 8.381778752664104e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933010339736938]\n",
      " 2506 [D loss: 7.718754204688594e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9929794073104858]\n",
      " 2507 [D loss: 8.592966878495645e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9930727481842041]\n",
      " 2508 [D loss: 0.0020325148943811655, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9920666217803955]\n",
      " 2509 [D loss: 0.000565606402233243, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9921345710754395]\n",
      " 2510 [D loss: 5.5219385103555396e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9924668073654175]\n",
      " 2511 [D loss: 3.4741115086944774e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.991349458694458]\n",
      " 2512 [D loss: 6.635674253629986e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9924955368041992]\n",
      " 2513 [D loss: 0.00013826137001160532, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9901347160339355]\n",
      " 2514 [D loss: 0.00011241843458265066, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.990128755569458]\n",
      " 2515 [D loss: 6.089361704653129e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9904384613037109]\n",
      " 2516 [D loss: 0.0005159839638508856, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9909727573394775]\n",
      " 2517 [D loss: 4.2083302105311304e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.988375186920166]\n",
      " 2518 [D loss: 1.991883254959248e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9906531572341919]\n",
      " 2519 [D loss: 2.5223282136721537e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9899470806121826]\n",
      " 2520 [D loss: 0.00011500386608531699, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.990993082523346]\n",
      " 2521 [D loss: 0.00011280983744654804, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9908592700958252]\n",
      " 2522 [D loss: 0.002695925533771515, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9905678033828735]\n",
      " 2523 [D loss: 0.0001148179144365713, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9922690391540527]\n",
      " 2524 [D loss: 0.0005256451549939811, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936341643333435]\n",
      " 2525 [D loss: 3.546476364135742e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937492609024048]\n",
      " 2526 [D loss: 1.669615085120313e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9932717084884644]\n",
      " 2527 [D loss: 0.00012045529729221016, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947457313537598]\n",
      " 2528 [D loss: 0.0003798319667112082, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953280091285706]\n",
      " 2529 [D loss: 1.417006478732219e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994942307472229]\n",
      " 2530 [D loss: 5.005191724194447e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947471618652344]\n",
      " 2531 [D loss: 8.6183164967224e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949837327003479]\n",
      " 2532 [D loss: 7.191729764599586e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946836233139038]\n",
      " 2533 [D loss: 0.00010070762073155493, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943683743476868]\n",
      " 2534 [D loss: 5.798962956760079e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994921088218689]\n",
      " 2535 [D loss: 9.281871825805865e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941902160644531]\n",
      " 2536 [D loss: 0.00013195272185839713, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941736459732056]\n",
      " 2537 [D loss: 4.174521382083185e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944920539855957]\n",
      " 2538 [D loss: 1.2732026334560942e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943404197692871]\n",
      " 2539 [D loss: 6.914863206475275e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994253396987915]\n",
      " 2540 [D loss: 0.00013272221258375794, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944602251052856]\n",
      " 2541 [D loss: 9.360887634102255e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947105050086975]\n",
      " 2542 [D loss: 1.758049438649323e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945009350776672]\n",
      " 2543 [D loss: 9.556921577313915e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994166374206543]\n",
      " 2544 [D loss: 0.0005424599512480199, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947925806045532]\n",
      " 2545 [D loss: 0.0007420207839459181, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950181245803833]\n",
      " 2546 [D loss: 0.00029022025410085917, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942965507507324]\n",
      " 2547 [D loss: 0.00013416267756838351, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942302107810974]\n",
      " 2548 [D loss: 1.810313551686704e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945967793464661]\n",
      " 2549 [D loss: 0.0003355566004756838, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944508075714111]\n",
      " 2550 [D loss: 7.304554856091272e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994254469871521]\n",
      " 2551 [D loss: 1.1410467777750455e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947146773338318]\n",
      " 2552 [D loss: 3.948778612539172e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9939822554588318]\n",
      " 2553 [D loss: 1.1818699931609444e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994339108467102]\n",
      " 2554 [D loss: 1.647290810069535e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99443119764328]\n",
      " 2555 [D loss: 2.4840135210979497e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936600923538208]\n",
      " 2556 [D loss: 0.0003152912831865251, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944806098937988]\n",
      " 2557 [D loss: 1.1918485142814461e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940985441207886]\n",
      " 2558 [D loss: 5.5864111345727e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994266152381897]\n",
      " 2559 [D loss: 7.986807759152725e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941765069961548]\n",
      " 2560 [D loss: 9.739478628034703e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933298826217651]\n",
      " 2561 [D loss: 7.01979797668173e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938817024230957]\n",
      " 2562 [D loss: 2.8169588404125534e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943202137947083]\n",
      " 2563 [D loss: 0.00028300497797317803, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937232732772827]\n",
      " 2564 [D loss: 1.9442491975496523e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9934282302856445]\n",
      " 2565 [D loss: 3.5192639188608155e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9925400018692017]\n",
      " 2566 [D loss: 4.160702519584447e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927816390991211]\n",
      " 2567 [D loss: 1.7907917936099693e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9932693243026733]\n",
      " 2568 [D loss: 6.429145287256688e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993112325668335]\n",
      " 2569 [D loss: 4.793597327079624e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9902057647705078]\n",
      " 2570 [D loss: 1.949246507138014e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9928455352783203]\n",
      " 2571 [D loss: 0.00017739474424161017, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9924520254135132]\n",
      " 2572 [D loss: 1.1942212040594313e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9909791946411133]\n",
      " 2573 [D loss: 5.860444071004167e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927242398262024]\n",
      " 2574 [D loss: 7.348712824750692e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935043454170227]\n",
      " 2575 [D loss: 0.00015367144078481942, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933968782424927]\n",
      " 2576 [D loss: 3.8069862057454884e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9920597076416016]\n",
      " 2577 [D loss: 1.2328840057307389e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936589002609253]\n",
      " 2578 [D loss: 1.5329009329434484e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9924107789993286]\n",
      " 2579 [D loss: 2.2712549252901226e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9930282235145569]\n",
      " 2580 [D loss: 2.5198760340572335e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9922291040420532]\n",
      " 2581 [D loss: 0.00012181660713395104, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927892684936523]\n",
      " 2582 [D loss: 0.001183227403089404, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993246853351593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2583 [D loss: 1.0705082786444109e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938464164733887]\n",
      " 2584 [D loss: 2.5136425392702222e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942116737365723]\n",
      " 2585 [D loss: 5.994517414364964e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994106113910675]\n",
      " 2586 [D loss: 3.8995050999801606e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945807456970215]\n",
      " 2587 [D loss: 1.2401985259202775e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947819709777832]\n",
      " 2588 [D loss: 1.4400374311662745e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950991868972778]\n",
      " 2589 [D loss: 1.0537224625295494e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945045113563538]\n",
      " 2590 [D loss: 4.627921043720562e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948186874389648]\n",
      " 2591 [D loss: 0.00015205047384370118, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948570728302002]\n",
      " 2592 [D loss: 2.10709167731693e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947283267974854]\n",
      " 2593 [D loss: 0.00046389808994717896, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938490390777588]\n",
      " 2594 [D loss: 7.194955287559424e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937950372695923]\n",
      " 2595 [D loss: 6.609418960579205e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941321611404419]\n",
      " 2596 [D loss: 0.00017886492423713207, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936298131942749]\n",
      " 2597 [D loss: 7.480557542294264e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936016798019409]\n",
      " 2598 [D loss: 6.660885992459953e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933681488037109]\n",
      " 2599 [D loss: 0.00014399344217963517, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993415355682373]\n",
      " 2600 [D loss: 2.2648215235676616e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927809238433838]\n",
      " 2601 [D loss: 2.087478969770018e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9928485751152039]\n",
      " 2602 [D loss: 3.329659375594929e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933069944381714]\n",
      " 2603 [D loss: 3.5220114114054013e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9931017160415649]\n",
      " 2604 [D loss: 7.044029189273715e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9923887252807617]\n",
      " 2605 [D loss: 3.510790702421218e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938474297523499]\n",
      " 2606 [D loss: 8.682841325935442e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9934759140014648]\n",
      " 2607 [D loss: 3.184544766554609e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9929031133651733]\n",
      " 2608 [D loss: 8.120689017232507e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9928798675537109]\n",
      " 2609 [D loss: 3.4738412068691105e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9922981858253479]\n",
      " 2610 [D loss: 0.0001197865349240601, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935822486877441]\n",
      " 2611 [D loss: 1.0734541319834534e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9921443462371826]\n",
      " 2612 [D loss: 2.2150388758745976e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936897158622742]\n",
      " 2613 [D loss: 1.7517086234875023e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993159294128418]\n",
      " 2614 [D loss: 5.004692138754763e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993690013885498]\n",
      " 2615 [D loss: 1.3758131899521686e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9932639598846436]\n",
      " 2616 [D loss: 4.0029972296906635e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933022856712341]\n",
      " 2617 [D loss: 8.115440869005397e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933586120605469]\n",
      " 2618 [D loss: 1.286665428779088e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9934488534927368]\n",
      " 2619 [D loss: 3.215115066268481e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9932078123092651]\n",
      " 2620 [D loss: 3.941213435609825e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937838315963745]\n",
      " 2621 [D loss: 1.7513233615318313e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9931750893592834]\n",
      " 2622 [D loss: 6.930750532774255e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.992452085018158]\n",
      " 2623 [D loss: 9.947164471668657e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9934687614440918]\n",
      " 2624 [D loss: 7.429095603583846e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9915115833282471]\n",
      " 2625 [D loss: 9.371550549985841e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933102130889893]\n",
      " 2626 [D loss: 7.957213711051736e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940997958183289]\n",
      " 2627 [D loss: 6.436828698497266e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.991185188293457]\n",
      " 2628 [D loss: 0.00018248171545565128, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9928704500198364]\n",
      " 2629 [D loss: 5.310423148330301e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936022162437439]\n",
      " 2630 [D loss: 4.537761560641229e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937864542007446]\n",
      " 2631 [D loss: 0.00044254696695134044, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935171604156494]\n",
      " 2632 [D loss: 3.8033609598642215e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940627217292786]\n",
      " 2633 [D loss: 5.121643698657863e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.992890477180481]\n",
      " 2634 [D loss: 6.279877652559662e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942556023597717]\n",
      " 2635 [D loss: 6.369959464791464e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943206906318665]\n",
      " 2636 [D loss: 6.466508784797043e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994219183921814]\n",
      " 2637 [D loss: 8.206322490877938e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941310286521912]\n",
      " 2638 [D loss: 0.0006483719334937632, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945172071456909]\n",
      " 2639 [D loss: 8.64570465637371e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946727156639099]\n",
      " 2640 [D loss: 0.00011946358426939696, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949612617492676]\n",
      " 2641 [D loss: 1.6772844901424833e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953280687332153]\n",
      " 2642 [D loss: 7.895103408372961e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994964599609375]\n",
      " 2643 [D loss: 7.814482160029002e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946730732917786]\n",
      " 2644 [D loss: 1.0049268894363195e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951484203338623]\n",
      " 2645 [D loss: 0.0001318507274845615, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954279661178589]\n",
      " 2646 [D loss: 4.957257260684855e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952499866485596]\n",
      " 2647 [D loss: 2.7335678169038147e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955967664718628]\n",
      " 2648 [D loss: 1.4073042621021159e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995057225227356]\n",
      " 2649 [D loss: 3.569479304132983e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950947761535645]\n",
      " 2650 [D loss: 4.363818334240932e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995576024055481]\n",
      " 2651 [D loss: 0.0006024513859301805, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952453970909119]\n",
      " 2652 [D loss: 2.2230799004319124e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952670335769653]\n",
      " 2653 [D loss: 2.7384525310480967e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952961206436157]\n",
      " 2654 [D loss: 0.00010770466178655624, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945063591003418]\n",
      " 2655 [D loss: 2.3180840798886493e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948221445083618]\n",
      " 2656 [D loss: 7.31433647160884e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947320818901062]\n",
      " 2657 [D loss: 3.373793879291043e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942886829376221]\n",
      " 2658 [D loss: 0.00017308889073319733, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949591159820557]\n",
      " 2659 [D loss: 0.0001276032126042992, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944928884506226]\n",
      " 2660 [D loss: 3.1990759453037754e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944905042648315]\n",
      " 2661 [D loss: 2.301511813129764e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9934800863265991]\n",
      " 2662 [D loss: 2.0776809833478183e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994082510471344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2663 [D loss: 2.8267029847484082e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9925965666770935]\n",
      " 2664 [D loss: 1.1309963156236336e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941737651824951]\n",
      " 2665 [D loss: 2.5263891075155698e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927728176116943]\n",
      " 2666 [D loss: 7.607746738358401e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937981963157654]\n",
      " 2667 [D loss: 5.9493402659427375e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935452938079834]\n",
      " 2668 [D loss: 6.435510294977576e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933426380157471]\n",
      " 2669 [D loss: 4.14572459703777e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937306046485901]\n",
      " 2670 [D loss: 4.610777978086844e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938558340072632]\n",
      " 2671 [D loss: 1.5838533727219328e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935988187789917]\n",
      " 2672 [D loss: 4.8078381951199844e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9924132227897644]\n",
      " 2673 [D loss: 1.8728653230937198e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944266080856323]\n",
      " 2674 [D loss: 2.7115404009236954e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9939455986022949]\n",
      " 2675 [D loss: 1.720975706120953e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940445423126221]\n",
      " 2676 [D loss: 4.912758868158562e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938485622406006]\n",
      " 2677 [D loss: 1.2837121175834909e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938065409660339]\n",
      " 2678 [D loss: 6.984523679420818e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938410520553589]\n",
      " 2679 [D loss: 7.343372999457642e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9934855699539185]\n",
      " 2680 [D loss: 7.394185377052054e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937182068824768]\n",
      " 2681 [D loss: 3.591541826608591e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940948486328125]\n",
      " 2682 [D loss: 3.1906040476314956e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99411541223526]\n",
      " 2683 [D loss: 4.026437090942636e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938504695892334]\n",
      " 2684 [D loss: 7.164818816818297e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947395324707031]\n",
      " 2685 [D loss: 4.559976332529914e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943530559539795]\n",
      " 2686 [D loss: 9.536057041259483e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942066669464111]\n",
      " 2687 [D loss: 0.00033083916059695184, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945980310440063]\n",
      " 2688 [D loss: 7.037643808871508e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941765069961548]\n",
      " 2689 [D loss: 4.623514541890472e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9922722578048706]\n",
      " 2690 [D loss: 8.826753401081078e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942202568054199]\n",
      " 2691 [D loss: 4.341592557466356e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948506355285645]\n",
      " 2692 [D loss: 3.97135409002658e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947632551193237]\n",
      " 2693 [D loss: 3.4978485928149894e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948336482048035]\n",
      " 2694 [D loss: 4.921635991195217e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950964450836182]\n",
      " 2695 [D loss: 6.830990969319828e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950303435325623]\n",
      " 2696 [D loss: 8.27086296339985e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950481653213501]\n",
      " 2697 [D loss: 0.00011087208258686587, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953025579452515]\n",
      " 2698 [D loss: 2.9964735404064413e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953004121780396]\n",
      " 2699 [D loss: 1.563023033668287e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954167604446411]\n",
      " 2700 [D loss: 7.134331099223346e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937500953674316]\n",
      " 2701 [D loss: 9.834152297116816e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995353639125824]\n",
      " 2702 [D loss: 9.68436143011786e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994952917098999]\n",
      " 2703 [D loss: 4.752529275720008e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950764775276184]\n",
      " 2704 [D loss: 0.00016689109907019883, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951106309890747]\n",
      " 2705 [D loss: 1.3154172847862355e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946269989013672]\n",
      " 2706 [D loss: 2.3141792553360574e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950481057167053]\n",
      " 2707 [D loss: 0.0001121468230849132, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944251775741577]\n",
      " 2708 [D loss: 4.108095708943438e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948391318321228]\n",
      " 2709 [D loss: 1.1087903658335563e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943487048149109]\n",
      " 2710 [D loss: 4.4008456825395115e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935224056243896]\n",
      " 2711 [D loss: 9.367955499328673e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9934614896774292]\n",
      " 2712 [D loss: 6.846234100521542e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935126304626465]\n",
      " 2713 [D loss: 5.316714577929815e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994644284248352]\n",
      " 2714 [D loss: 1.4312589883047622e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945893287658691]\n",
      " 2715 [D loss: 2.3303637135541067e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9922471046447754]\n",
      " 2716 [D loss: 5.21819720233907e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9939546585083008]\n",
      " 2717 [D loss: 7.527804882556666e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994957685470581]\n",
      " 2718 [D loss: 1.2821335985790938e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946720600128174]\n",
      " 2719 [D loss: 5.7299897889606655e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936276078224182]\n",
      " 2720 [D loss: 5.077190144220367e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944266676902771]\n",
      " 2721 [D loss: 3.560436380212195e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944554567337036]\n",
      " 2722 [D loss: 1.0603303962852806e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99476158618927]\n",
      " 2723 [D loss: 8.031217475945596e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945606589317322]\n",
      " 2724 [D loss: 2.211012179031968e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947378039360046]\n",
      " 2725 [D loss: 5.269333269097842e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944615364074707]\n",
      " 2726 [D loss: 2.7312978545523947e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948043823242188]\n",
      " 2727 [D loss: 2.5990879294113256e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933431148529053]\n",
      " 2728 [D loss: 2.8505695809144527e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947056770324707]\n",
      " 2729 [D loss: 1.5393568901345134e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947429895401001]\n",
      " 2730 [D loss: 1.4085357179283164e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942187666893005]\n",
      " 2731 [D loss: 6.0051515902159736e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945739507675171]\n",
      " 2732 [D loss: 5.819374109705677e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948678016662598]\n",
      " 2733 [D loss: 7.189495590864681e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936313629150391]\n",
      " 2734 [D loss: 3.365531301824376e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946315288543701]\n",
      " 2735 [D loss: 8.8004480858217e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943965673446655]\n",
      " 2736 [D loss: 5.834373951074667e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948139786720276]\n",
      " 2737 [D loss: 3.9955793909030035e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948186874389648]\n",
      " 2738 [D loss: 5.654969754687045e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951541423797607]\n",
      " 2739 [D loss: 2.652922921697609e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944719076156616]\n",
      " 2740 [D loss: 2.2106305550551042e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952237606048584]\n",
      " 2741 [D loss: 7.68298650655197e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994865894317627]\n",
      " 2742 [D loss: 0.00010338328138459474, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99462890625]\n",
      " 2743 [D loss: 4.371199793240521e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936838150024414]\n",
      " 2744 [D loss: 4.1204570152331144e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950273036956787]\n",
      " 2745 [D loss: 1.1170568541274406e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933481216430664]\n",
      " 2746 [D loss: 1.3355156625038944e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945208430290222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2747 [D loss: 1.1500446817080956e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945665597915649]\n",
      " 2748 [D loss: 1.333525597146945e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994848370552063]\n",
      " 2749 [D loss: 9.354403118777554e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945456981658936]\n",
      " 2750 [D loss: 9.45826104725711e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941834211349487]\n",
      " 2751 [D loss: 2.657090817592689e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99349045753479]\n",
      " 2752 [D loss: 3.515888693073066e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946705102920532]\n",
      " 2753 [D loss: 3.5094140002911445e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950339794158936]\n",
      " 2754 [D loss: 0.00016603370022494346, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949623942375183]\n",
      " 2755 [D loss: 1.8020980405708542e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949078559875488]\n",
      " 2756 [D loss: 0.00012072070967406034, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993979811668396]\n",
      " 2757 [D loss: 1.846308987296652e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951865673065186]\n",
      " 2758 [D loss: 2.3487577891501132e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951411485671997]\n",
      " 2759 [D loss: 4.284152510081185e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994489848613739]\n",
      " 2760 [D loss: 4.4067214730603155e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947155714035034]\n",
      " 2761 [D loss: 3.026053946086904e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947644472122192]\n",
      " 2762 [D loss: 1.593287379364483e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951938390731812]\n",
      " 2763 [D loss: 5.994789535179734e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950254559516907]\n",
      " 2764 [D loss: 4.783152689924464e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941951036453247]\n",
      " 2765 [D loss: 4.144199920119718e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941458702087402]\n",
      " 2766 [D loss: 0.00014633340470027179, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9939461350440979]\n",
      " 2767 [D loss: 1.1050416105717886e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947660565376282]\n",
      " 2768 [D loss: 2.1756884507340146e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951615333557129]\n",
      " 2769 [D loss: 0.00011329121480230242, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951331615447998]\n",
      " 2770 [D loss: 1.4393900528375525e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952921867370605]\n",
      " 2771 [D loss: 1.592205990164075e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940399527549744]\n",
      " 2772 [D loss: 1.0677111276891083e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950280785560608]\n",
      " 2773 [D loss: 7.136345084290951e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946532249450684]\n",
      " 2774 [D loss: 7.096533863659715e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948003888130188]\n",
      " 2775 [D loss: 6.990413112362148e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949049949645996]\n",
      " 2776 [D loss: 9.014841452881228e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949257969856262]\n",
      " 2777 [D loss: 1.2547625374281779e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951462745666504]\n",
      " 2778 [D loss: 2.8000111342407763e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947574138641357]\n",
      " 2779 [D loss: 4.610132236848585e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952380657196045]\n",
      " 2780 [D loss: 0.00019287655595690012, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99488365650177]\n",
      " 2781 [D loss: 9.476700142840855e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953157901763916]\n",
      " 2782 [D loss: 1.5087114661582746e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954279661178589]\n",
      " 2783 [D loss: 1.3262046195450239e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952421188354492]\n",
      " 2784 [D loss: 3.379562258487567e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953082799911499]\n",
      " 2785 [D loss: 1.4770746929571033e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949342012405396]\n",
      " 2786 [D loss: 1.0559699148871005e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952298998832703]\n",
      " 2787 [D loss: 0.0002324667148059234, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952310919761658]\n",
      " 2788 [D loss: 8.60023355926387e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952408671379089]\n",
      " 2789 [D loss: 1.9372509996173903e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994549036026001]\n",
      " 2790 [D loss: 4.471750798984431e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944832921028137]\n",
      " 2791 [D loss: 0.00012593631981872022, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953562021255493]\n",
      " 2792 [D loss: 3.979039775003912e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954005479812622]\n",
      " 2793 [D loss: 4.6986688175820746e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952092170715332]\n",
      " 2794 [D loss: 4.214048749417998e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950407147407532]\n",
      " 2795 [D loss: 0.00014605236356146634, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952483177185059]\n",
      " 2796 [D loss: 2.910004332079552e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949243068695068]\n",
      " 2797 [D loss: 8.172824163921177e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948766231536865]\n",
      " 2798 [D loss: 4.335179983172566e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954156875610352]\n",
      " 2799 [D loss: 6.314256461337209e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945151209831238]\n",
      " 2800 [D loss: 1.7651309462962672e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954075813293457]\n",
      " 2801 [D loss: 7.1913268584467005e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942139387130737]\n",
      " 2802 [D loss: 0.00012006641190964729, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951193332672119]\n",
      " 2803 [D loss: 1.4295836081146263e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946512579917908]\n",
      " 2804 [D loss: 2.1726895283791237e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944973587989807]\n",
      " 2805 [D loss: 5.4152293159859255e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950360059738159]\n",
      " 2806 [D loss: 3.0356686693266965e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948500990867615]\n",
      " 2807 [D loss: 8.380677172681317e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995233416557312]\n",
      " 2808 [D loss: 4.980329322279431e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949055314064026]\n",
      " 2809 [D loss: 7.724332135694567e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949873685836792]\n",
      " 2810 [D loss: 2.4264713829325046e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944728016853333]\n",
      " 2811 [D loss: 1.7133554138126783e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950846433639526]\n",
      " 2812 [D loss: 5.211781626712764e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954558610916138]\n",
      " 2813 [D loss: 6.514229880849598e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952762126922607]\n",
      " 2814 [D loss: 0.00013077155745122582, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946773648262024]\n",
      " 2815 [D loss: 4.816170985577628e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951032996177673]\n",
      " 2816 [D loss: 3.341709543747129e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948297142982483]\n",
      " 2817 [D loss: 4.580659879138693e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946044683456421]\n",
      " 2818 [D loss: 3.426892135394155e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.992409348487854]\n",
      " 2819 [D loss: 3.3295241337327752e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944847822189331]\n",
      " 2820 [D loss: 3.3591925330256345e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947932362556458]\n",
      " 2821 [D loss: 3.233061761420686e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946991205215454]\n",
      " 2822 [D loss: 5.098135079606436e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941629767417908]\n",
      " 2823 [D loss: 1.06774796222453e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945102334022522]\n",
      " 2824 [D loss: 7.104142241587397e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948607683181763]\n",
      " 2825 [D loss: 5.946085821051383e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946571588516235]\n",
      " 2826 [D loss: 3.5943221519119106e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948288798332214]\n",
      " 2827 [D loss: 2.26786528401135e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942222237586975]\n",
      " 2828 [D loss: 6.54482573736459e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948303699493408]\n",
      " 2829 [D loss: 1.1097316928498913e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949222803115845]\n",
      " 2830 [D loss: 2.4746400413278025e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948911666870117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2831 [D loss: 0.0001619017421035096, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949453473091125]\n",
      " 2832 [D loss: 9.434293133381288e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951768517494202]\n",
      " 2833 [D loss: 1.1650712622213177e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952133893966675]\n",
      " 2834 [D loss: 2.9243630706332624e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995184600353241]\n",
      " 2835 [D loss: 1.7378812117385678e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951066970825195]\n",
      " 2836 [D loss: 3.088045195909217e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950766563415527]\n",
      " 2837 [D loss: 2.704041253309697e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952542185783386]\n",
      " 2838 [D loss: 0.00010820538591360673, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950113892555237]\n",
      " 2839 [D loss: 2.343700180063024e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941062927246094]\n",
      " 2840 [D loss: 7.728851414867677e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948593378067017]\n",
      " 2841 [D loss: 2.5885362902045017e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953688383102417]\n",
      " 2842 [D loss: 4.793078915099613e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953756332397461]\n",
      " 2843 [D loss: 0.00016819851589389145, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948936700820923]\n",
      " 2844 [D loss: 2.958320692414418e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953413009643555]\n",
      " 2845 [D loss: 4.1981929825851694e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995614230632782]\n",
      " 2846 [D loss: 0.00010873139399336651, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994886040687561]\n",
      " 2847 [D loss: 2.7628280804492533e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953420162200928]\n",
      " 2848 [D loss: 4.2198371374979615e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995458722114563]\n",
      " 2849 [D loss: 2.4189646410377463e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954022765159607]\n",
      " 2850 [D loss: 7.073095184750855e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943458437919617]\n",
      " 2851 [D loss: 1.7751472114468925e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949020147323608]\n",
      " 2852 [D loss: 3.0143324693199247e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953485727310181]\n",
      " 2853 [D loss: 1.668940603849478e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953740239143372]\n",
      " 2854 [D loss: 5.974496161798015e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953663945198059]\n",
      " 2855 [D loss: 0.00012895496911369264, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99517422914505]\n",
      " 2856 [D loss: 2.3067275833454914e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948949813842773]\n",
      " 2857 [D loss: 1.1591901056817733e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994717001914978]\n",
      " 2858 [D loss: 6.6951802182302345e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945262670516968]\n",
      " 2859 [D loss: 4.780504241352901e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953181743621826]\n",
      " 2860 [D loss: 1.6506470274180174e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995154082775116]\n",
      " 2861 [D loss: 5.1030019676545635e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950191974639893]\n",
      " 2862 [D loss: 3.735359314305242e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995345950126648]\n",
      " 2863 [D loss: 3.0401410185731947e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953386783599854]\n",
      " 2864 [D loss: 3.1044805837154854e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948640465736389]\n",
      " 2865 [D loss: 1.3067857253190596e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944829940795898]\n",
      " 2866 [D loss: 6.368236427078955e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952689409255981]\n",
      " 2867 [D loss: 8.446586434729397e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949504137039185]\n",
      " 2868 [D loss: 1.2744661944452673e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954638481140137]\n",
      " 2869 [D loss: 1.7816792023950256e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957156181335449]\n",
      " 2870 [D loss: 1.85100689122919e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949980974197388]\n",
      " 2871 [D loss: 6.319619387795683e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949923157691956]\n",
      " 2872 [D loss: 0.00010299339919583872, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953887462615967]\n",
      " 2873 [D loss: 1.8705797629081644e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995094358921051]\n",
      " 2874 [D loss: 1.232463455380639e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948911666870117]\n",
      " 2875 [D loss: 6.964067324588541e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948868751525879]\n",
      " 2876 [D loss: 5.009840606362559e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950993061065674]\n",
      " 2877 [D loss: 3.137326712021604e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946504831314087]\n",
      " 2878 [D loss: 8.283543138531968e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950754046440125]\n",
      " 2879 [D loss: 2.164663419534918e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950488805770874]\n",
      " 2880 [D loss: 5.72587805436342e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948861002922058]\n",
      " 2881 [D loss: 3.227632305424777e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952785968780518]\n",
      " 2882 [D loss: 1.3901521924708504e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952654838562012]\n",
      " 2883 [D loss: 1.651999627938494e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941310882568359]\n",
      " 2884 [D loss: 2.5113387891906314e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944385290145874]\n",
      " 2885 [D loss: 5.361272542359075e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949918985366821]\n",
      " 2886 [D loss: 2.9077282306388952e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947196245193481]\n",
      " 2887 [D loss: 1.5824412912479602e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994555652141571]\n",
      " 2888 [D loss: 4.9726008910511155e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950894117355347]\n",
      " 2889 [D loss: 9.122348274104297e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946931600570679]\n",
      " 2890 [D loss: 3.969711178797297e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944533109664917]\n",
      " 2891 [D loss: 1.6385329217882827e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945036172866821]\n",
      " 2892 [D loss: 2.4203473003581166e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950799345970154]\n",
      " 2893 [D loss: 9.227132250089198e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937427639961243]\n",
      " 2894 [D loss: 9.138192581303883e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945504665374756]\n",
      " 2895 [D loss: 9.768818927113898e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950437545776367]\n",
      " 2896 [D loss: 2.0520178622973617e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952102899551392]\n",
      " 2897 [D loss: 6.3644288275099825e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952033758163452]\n",
      " 2898 [D loss: 9.815891098696738e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945433139801025]\n",
      " 2899 [D loss: 5.915592919336632e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957382678985596]\n",
      " 2900 [D loss: 3.1611327813152457e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954092502593994]\n",
      " 2901 [D loss: 1.6731883079046384e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950324296951294]\n",
      " 2902 [D loss: 1.0181637662753928e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953469038009644]\n",
      " 2903 [D loss: 9.914441761793569e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954156875610352]\n",
      " 2904 [D loss: 2.5034639747900655e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954133629798889]\n",
      " 2905 [D loss: 3.771252750084386e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955893754959106]\n",
      " 2906 [D loss: 2.1581595319730695e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949085712432861]\n",
      " 2907 [D loss: 4.258033186488319e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954429864883423]\n",
      " 2908 [D loss: 1.7254984413739294e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9956159591674805]\n",
      " 2909 [D loss: 0.00011651358363451436, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995008111000061]\n",
      " 2910 [D loss: 8.697015800862573e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955286383628845]\n",
      " 2911 [D loss: 3.305288191768341e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953848719596863]\n",
      " 2912 [D loss: 0.0001997686777031049, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954099655151367]\n",
      " 2913 [D loss: 3.2392185858043376e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954173564910889]\n",
      " 2914 [D loss: 1.943454844877124e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953906536102295]\n",
      " 2915 [D loss: 0.00012836893438361585, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955644607543945]\n",
      " 2916 [D loss: 0.00012136089208070189, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955285787582397]\n",
      " 2917 [D loss: 7.28681479813531e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995843768119812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2918 [D loss: 1.5191603779385332e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957162141799927]\n",
      " 2919 [D loss: 8.481898476020433e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955406785011292]\n",
      " 2920 [D loss: 2.5211022148141637e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958204030990601]\n",
      " 2921 [D loss: 1.0145288797502872e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959158897399902]\n",
      " 2922 [D loss: 4.0858292777556926e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955213069915771]\n",
      " 2923 [D loss: 1.3681814380106516e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958583116531372]\n",
      " 2924 [D loss: 1.037039328366518e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958783388137817]\n",
      " 2925 [D loss: 0.0004925163229927421, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952231645584106]\n",
      " 2926 [D loss: 3.083716728724539e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946087598800659]\n",
      " 2927 [D loss: 2.0352239516796544e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947906136512756]\n",
      " 2928 [D loss: 2.0017549104522914e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946190714836121]\n",
      " 2929 [D loss: 2.4504870452801697e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947353601455688]\n",
      " 2930 [D loss: 0.0005151873338036239, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9939956665039062]\n",
      " 2931 [D loss: 8.772153523750603e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935286641120911]\n",
      " 2932 [D loss: 3.3345720567012904e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9916203618049622]\n",
      " 2933 [D loss: 6.712604954373091e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942671060562134]\n",
      " 2934 [D loss: 9.275643606088124e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940104484558105]\n",
      " 2935 [D loss: 2.7778514777310193e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949150085449219]\n",
      " 2936 [D loss: 3.279677912360057e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944647550582886]\n",
      " 2937 [D loss: 2.3943954147398472e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9939677119255066]\n",
      " 2938 [D loss: 7.621131771884393e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944107532501221]\n",
      " 2939 [D loss: 7.091862244124059e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948394298553467]\n",
      " 2940 [D loss: 1.7969436157727614e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9939559698104858]\n",
      " 2941 [D loss: 6.683655556116719e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946984052658081]\n",
      " 2942 [D loss: 2.8138424568169285e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947347640991211]\n",
      " 2943 [D loss: 5.6261765166709665e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995233952999115]\n",
      " 2944 [D loss: 4.117764547117986e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952034950256348]\n",
      " 2945 [D loss: 0.00011493317288113758, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950793981552124]\n",
      " 2946 [D loss: 3.870981345244218e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949231743812561]\n",
      " 2947 [D loss: 1.5899070149316685e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943908452987671]\n",
      " 2948 [D loss: 7.825329703337047e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949201345443726]\n",
      " 2949 [D loss: 0.0005195491830818355, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942169785499573]\n",
      " 2950 [D loss: 3.294230737083126e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951713681221008]\n",
      " 2951 [D loss: 2.2437386633100687e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945626854896545]\n",
      " 2952 [D loss: 0.00011953381181228906, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948416948318481]\n",
      " 2953 [D loss: 5.116515967529267e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957405924797058]\n",
      " 2954 [D loss: 0.0001243545557372272, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955364465713501]\n",
      " 2955 [D loss: 1.385744872095529e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955160617828369]\n",
      " 2956 [D loss: 2.3473630790249445e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951707720756531]\n",
      " 2957 [D loss: 2.9219039788586088e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948329925537109]\n",
      " 2958 [D loss: 0.001469955313950777, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958675503730774]\n",
      " 2959 [D loss: 2.9953333068988286e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962589740753174]\n",
      " 2960 [D loss: 1.4976294551161118e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964838027954102]\n",
      " 2961 [D loss: 1.2092361885152059e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971157312393188]\n",
      " 2962 [D loss: 1.4300496786745498e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9973490238189697]\n",
      " 2963 [D loss: 0.00032944988925009966, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9974536299705505]\n",
      " 2964 [D loss: 2.02830392481701e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9974259734153748]\n",
      " 2965 [D loss: 2.237312037323136e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9975495338439941]\n",
      " 2966 [D loss: 8.978798291536805e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9975452423095703]\n",
      " 2967 [D loss: 0.00028636559727601707, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997395932674408]\n",
      " 2968 [D loss: 1.1072721690652543e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9974059462547302]\n",
      " 2969 [D loss: 1.2098440492991358e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9975265860557556]\n",
      " 2970 [D loss: 9.351618928121752e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972032308578491]\n",
      " 2971 [D loss: 1.5686280221416382e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997173547744751]\n",
      " 2972 [D loss: 2.4395258151344024e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971675872802734]\n",
      " 2973 [D loss: 0.0005007207510061562, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970294237136841]\n",
      " 2974 [D loss: 1.3613831697512069e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970600605010986]\n",
      " 2975 [D loss: 7.858255412429571e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969466924667358]\n",
      " 2976 [D loss: 2.520700718378066e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966838359832764]\n",
      " 2977 [D loss: 1.8789794921758585e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963115453720093]\n",
      " 2978 [D loss: 2.0981124180252664e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965490102767944]\n",
      " 2979 [D loss: 5.3588178161589894e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963729977607727]\n",
      " 2980 [D loss: 1.1772573998314328e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960931539535522]\n",
      " 2981 [D loss: 3.3883970900205895e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962080121040344]\n",
      " 2982 [D loss: 5.313173460308462e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960387945175171]\n",
      " 2983 [D loss: 1.583199991728179e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960482120513916]\n",
      " 2984 [D loss: 2.1730684238718823e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996008038520813]\n",
      " 2985 [D loss: 1.7089854736695997e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954229593276978]\n",
      " 2986 [D loss: 6.607226623600582e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958693981170654]\n",
      " 2987 [D loss: 4.262925358489156e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958046674728394]\n",
      " 2988 [D loss: 5.819104262627661e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960468411445618]\n",
      " 2989 [D loss: 3.2854572964424733e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959973692893982]\n",
      " 2990 [D loss: 7.214540346467402e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955636262893677]\n",
      " 2991 [D loss: 2.6509987947065383e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995867133140564]\n",
      " 2992 [D loss: 3.1715771910967305e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955697059631348]\n",
      " 2993 [D loss: 0.00015381575212813914, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959757328033447]\n",
      " 2994 [D loss: 7.613706657139119e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960079193115234]\n",
      " 2995 [D loss: 9.578830940881744e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960439205169678]\n",
      " 2996 [D loss: 1.2490167137002572e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957348704338074]\n",
      " 2997 [D loss: 4.70790018880507e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961498975753784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2998 [D loss: 3.984379873145372e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995919942855835]\n",
      " 2999 [D loss: 5.643545591738075e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955408573150635]\n",
      " 3000 [D loss: 1.3875188415113371e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960220456123352]\n",
      " 3001 [D loss: 1.25733367895009e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959830641746521]\n",
      " 3002 [D loss: 4.721786353911739e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958611726760864]\n",
      " 3003 [D loss: 3.30542366100417e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960334897041321]\n",
      " 3004 [D loss: 5.24390816281084e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99617600440979]\n",
      " 3005 [D loss: 3.5534599192033056e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958682060241699]\n",
      " 3006 [D loss: 8.877235813997686e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958124160766602]\n",
      " 3007 [D loss: 1.0058745829155669e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957925081253052]\n",
      " 3008 [D loss: 1.549673470435664e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961854219436646]\n",
      " 3009 [D loss: 2.2169185740494868e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960813522338867]\n",
      " 3010 [D loss: 4.227456429362064e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962625503540039]\n",
      " 3011 [D loss: 1.6460862752865069e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960596561431885]\n",
      " 3012 [D loss: 8.90950468601659e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958324432373047]\n",
      " 3013 [D loss: 2.2782464839110617e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962155818939209]\n",
      " 3014 [D loss: 5.227780548011651e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960795640945435]\n",
      " 3015 [D loss: 7.067146725603379e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960654973983765]\n",
      " 3016 [D loss: 4.16690090787597e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960950613021851]\n",
      " 3017 [D loss: 1.550894376123324e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961590766906738]\n",
      " 3018 [D loss: 8.090678420558106e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962627291679382]\n",
      " 3019 [D loss: 8.776733920967672e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959052801132202]\n",
      " 3020 [D loss: 3.6826318137173075e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958819150924683]\n",
      " 3021 [D loss: 1.6869973933353322e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959695339202881]\n",
      " 3022 [D loss: 3.6968078347854316e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960370063781738]\n",
      " 3023 [D loss: 3.4742351999739185e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957593679428101]\n",
      " 3024 [D loss: 4.389110836200416e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960558414459229]\n",
      " 3025 [D loss: 0.00024377663794439286, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959807395935059]\n",
      " 3026 [D loss: 1.2856940884375945e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959285855293274]\n",
      " 3027 [D loss: 3.742748231161386e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961066246032715]\n",
      " 3028 [D loss: 7.159999768191483e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996380627155304]\n",
      " 3029 [D loss: 6.468558240158018e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962281584739685]\n",
      " 3030 [D loss: 1.0034831575467251e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963656067848206]\n",
      " 3031 [D loss: 3.684744569909526e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996575653553009]\n",
      " 3032 [D loss: 1.4491547517536674e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965457320213318]\n",
      " 3033 [D loss: 3.6704861940961564e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965043663978577]\n",
      " 3034 [D loss: 2.002813016588334e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963428378105164]\n",
      " 3035 [D loss: 3.534711368047283e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961438775062561]\n",
      " 3036 [D loss: 4.1942189454857726e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966959357261658]\n",
      " 3037 [D loss: 4.1185478039551526e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965479373931885]\n",
      " 3038 [D loss: 1.2949746633239556e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966253638267517]\n",
      " 3039 [D loss: 2.1272229787427932e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968656897544861]\n",
      " 3040 [D loss: 2.266998762934236e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965898990631104]\n",
      " 3041 [D loss: 8.048908966884483e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996868908405304]\n",
      " 3042 [D loss: 0.00010327737254556268, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969056844711304]\n",
      " 3043 [D loss: 1.2651077668124344e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961961507797241]\n",
      " 3044 [D loss: 1.295861329708714e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968553781509399]\n",
      " 3045 [D loss: 1.0815761925186962e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996701717376709]\n",
      " 3046 [D loss: 4.359773811302148e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996587872505188]\n",
      " 3047 [D loss: 1.5030486792966258e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961464405059814]\n",
      " 3048 [D loss: 6.567085802089423e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965642094612122]\n",
      " 3049 [D loss: 1.5334945828726632e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963639378547668]\n",
      " 3050 [D loss: 1.3075718925392721e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964468479156494]\n",
      " 3051 [D loss: 4.921334402752109e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995987594127655]\n",
      " 3052 [D loss: 2.1708996428060345e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965514540672302]\n",
      " 3053 [D loss: 1.0487608960829675e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961910843849182]\n",
      " 3054 [D loss: 1.5495264960918576e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963213801383972]\n",
      " 3055 [D loss: 3.522817223711172e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963370561599731]\n",
      " 3056 [D loss: 1.9633159809018252e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961726069450378]\n",
      " 3057 [D loss: 2.9957097922306275e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964162707328796]\n",
      " 3058 [D loss: 6.191366992425174e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960781335830688]\n",
      " 3059 [D loss: 4.938288111588918e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961881637573242]\n",
      " 3060 [D loss: 2.6419673304189928e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961430430412292]\n",
      " 3061 [D loss: 3.2394955269410275e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961647987365723]\n",
      " 3062 [D loss: 2.354153366468381e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958543181419373]\n",
      " 3063 [D loss: 5.288958618621109e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964572787284851]\n",
      " 3064 [D loss: 4.679964604292763e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959152936935425]\n",
      " 3065 [D loss: 7.620920769113582e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963750839233398]\n",
      " 3066 [D loss: 1.5486511983908713e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961881637573242]\n",
      " 3067 [D loss: 6.04767683398677e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957994222640991]\n",
      " 3068 [D loss: 3.3178424928337336e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961766004562378]\n",
      " 3069 [D loss: 0.0001465062377974391, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963806867599487]\n",
      " 3070 [D loss: 6.0678071349684615e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960200786590576]\n",
      " 3071 [D loss: 5.696960215573199e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963009357452393]\n",
      " 3072 [D loss: 2.401643769189832e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965277910232544]\n",
      " 3073 [D loss: 3.972802005591802e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962718486785889]\n",
      " 3074 [D loss: 1.3748744095209986e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961998462677002]\n",
      " 3075 [D loss: 2.9874979645683197e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99634850025177]\n",
      " 3076 [D loss: 1.9540126231731847e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958536624908447]\n",
      " 3077 [D loss: 5.28838472746429e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963757395744324]\n",
      " 3078 [D loss: 1.7679434449746623e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963078498840332]\n",
      " 3079 [D loss: 1.48118206197978e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963768720626831]\n",
      " 3080 [D loss: 5.672744919138495e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962692260742188]\n",
      " 3081 [D loss: 7.392247880488867e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962272047996521]\n",
      " 3082 [D loss: 4.630123839888256e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960433840751648]\n",
      " 3083 [D loss: 4.8615306695865e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996611475944519]\n",
      " 3084 [D loss: 7.169297987275058e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963051080703735]\n",
      " 3085 [D loss: 2.232599172202754e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958605170249939]\n",
      " 3086 [D loss: 5.951977982476819e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962285757064819]\n",
      " 3087 [D loss: 2.649884845595807e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961578845977783]\n",
      " 3088 [D loss: 1.4818508589087287e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995893120765686]\n",
      " 3089 [D loss: 1.3747753655479755e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996301531791687]\n",
      " 3090 [D loss: 1.5925270417938009e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962247610092163]\n",
      " 3091 [D loss: 6.362139174598269e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961878061294556]\n",
      " 3092 [D loss: 6.122942977526691e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961048364639282]\n",
      " 3093 [D loss: 1.5218769476632588e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965032339096069]\n",
      " 3094 [D loss: 1.8109160009771585e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960928559303284]\n",
      " 3095 [D loss: 1.8878753280660021e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960912466049194]\n",
      " 3096 [D loss: 1.571946995682083e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964433312416077]\n",
      " 3097 [D loss: 1.5147322756092763e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964229464530945]\n",
      " 3098 [D loss: 2.1698522232327377e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962084293365479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3099 [D loss: 6.941780611668946e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963618516921997]\n",
      " 3100 [D loss: 3.407890471862629e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960663318634033]\n",
      " 3101 [D loss: 4.626685949915554e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961005449295044]\n",
      " 3102 [D loss: 1.205578610097291e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963933229446411]\n",
      " 3103 [D loss: 3.181239662808366e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964044094085693]\n",
      " 3104 [D loss: 9.13984786166111e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961495399475098]\n",
      " 3105 [D loss: 4.975153842678992e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960824251174927]\n",
      " 3106 [D loss: 2.5097142497543246e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957955479621887]\n",
      " 3107 [D loss: 4.676701337302802e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962308406829834]\n",
      " 3108 [D loss: 1.2357438208709937e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960448741912842]\n",
      " 3109 [D loss: 2.3634467652300373e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958922266960144]\n",
      " 3110 [D loss: 2.0515269625320798e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960628747940063]\n",
      " 3111 [D loss: 9.499835869064555e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963825941085815]\n",
      " 3112 [D loss: 3.910806299245451e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958370923995972]\n",
      " 3113 [D loss: 3.888440005539451e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952378869056702]\n",
      " 3114 [D loss: 1.5428777260240167e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959431886672974]\n",
      " 3115 [D loss: 4.528644240053836e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995747983455658]\n",
      " 3116 [D loss: 3.138328611385077e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958910942077637]\n",
      " 3117 [D loss: 3.5575592391978716e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960017204284668]\n",
      " 3118 [D loss: 4.285673185222549e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958450794219971]\n",
      " 3119 [D loss: 1.5863452063058503e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964661002159119]\n",
      " 3120 [D loss: 3.426327339184354e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995803952217102]\n",
      " 3121 [D loss: 6.326641596388072e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954964518547058]\n",
      " 3122 [D loss: 8.014149898372125e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959311485290527]\n",
      " 3123 [D loss: 1.7938955352292396e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959967136383057]\n",
      " 3124 [D loss: 1.403959140588995e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958941340446472]\n",
      " 3125 [D loss: 7.215494406409562e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960675239562988]\n",
      " 3126 [D loss: 1.943991492225905e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959774017333984]\n",
      " 3127 [D loss: 1.1053315574827138e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995947003364563]\n",
      " 3128 [D loss: 1.6821932149468921e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961673617362976]\n",
      " 3129 [D loss: 7.938497219583951e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960875511169434]\n",
      " 3130 [D loss: 1.5658640677429503e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961452484130859]\n",
      " 3131 [D loss: 1.6700313381079468e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996037483215332]\n",
      " 3132 [D loss: 1.009591233014362e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959365129470825]\n",
      " 3133 [D loss: 5.296788458508672e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958860874176025]\n",
      " 3134 [D loss: 2.155012680304935e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963364601135254]\n",
      " 3135 [D loss: 3.6483013445831602e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960962533950806]\n",
      " 3136 [D loss: 0.00012720238009933382, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958903789520264]\n",
      " 3137 [D loss: 5.252639766695211e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960813522338867]\n",
      " 3138 [D loss: 3.6426558835955802e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962043762207031]\n",
      " 3139 [D loss: 3.0912081001588376e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959722757339478]\n",
      " 3140 [D loss: 1.5954727814460057e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964492321014404]\n",
      " 3141 [D loss: 3.432425364735536e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962197542190552]\n",
      " 3142 [D loss: 2.55502186519152e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963297843933105]\n",
      " 3143 [D loss: 2.9550394629040966e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962601661682129]\n",
      " 3144 [D loss: 8.743704711378086e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962181448936462]\n",
      " 3145 [D loss: 4.581885877996683e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960577487945557]\n",
      " 3146 [D loss: 2.869650415959768e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962688088417053]\n",
      " 3147 [D loss: 2.2974384137341985e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961444139480591]\n",
      " 3148 [D loss: 2.071243443424464e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959940910339355]\n",
      " 3149 [D loss: 2.4080281946226023e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960989952087402]\n",
      " 3150 [D loss: 7.109602847776841e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960545301437378]\n",
      " 3151 [D loss: 1.5711211744928733e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960863590240479]\n",
      " 3152 [D loss: 1.1981745956290979e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958208799362183]\n",
      " 3153 [D loss: 1.0571120583335869e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961673021316528]\n",
      " 3154 [D loss: 2.325013838344603e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957274198532104]\n",
      " 3155 [D loss: 3.207335339538986e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959284663200378]\n",
      " 3156 [D loss: 2.004877842409769e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959099888801575]\n",
      " 3157 [D loss: 8.103670552372932e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955323934555054]\n",
      " 3158 [D loss: 2.1832786387676606e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958098530769348]\n",
      " 3159 [D loss: 4.2994168325094506e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962775707244873]\n",
      " 3160 [D loss: 2.5244364678655984e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960736036300659]\n",
      " 3161 [D loss: 0.00010299568384652957, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964257478713989]\n",
      " 3162 [D loss: 2.4631376618344802e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963558912277222]\n",
      " 3163 [D loss: 2.2180254291015444e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960425496101379]\n",
      " 3164 [D loss: 3.5567625218391186e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964525103569031]\n",
      " 3165 [D loss: 4.188860657450277e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963250160217285]\n",
      " 3166 [D loss: 3.361363269505091e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962655305862427]\n",
      " 3167 [D loss: 5.986264113744255e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962584972381592]\n",
      " 3168 [D loss: 4.075149990967475e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966470003128052]\n",
      " 3169 [D loss: 1.8726044572758838e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963833689689636]\n",
      " 3170 [D loss: 1.756120468598965e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996433436870575]\n",
      " 3171 [D loss: 2.996786861331202e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965541362762451]\n",
      " 3172 [D loss: 1.0720315913204104e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962930679321289]\n",
      " 3173 [D loss: 3.2669329357304377e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99657142162323]\n",
      " 3174 [D loss: 3.0231387427193113e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996291995048523]\n",
      " 3175 [D loss: 4.011495093436679e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966446161270142]\n",
      " 3176 [D loss: 2.1305677364580333e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965144395828247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3177 [D loss: 6.248058980418136e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965360164642334]\n",
      " 3178 [D loss: 6.790428415115457e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961414337158203]\n",
      " 3179 [D loss: 2.365894033573568e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963589906692505]\n",
      " 3180 [D loss: 3.308335408291896e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960892200469971]\n",
      " 3181 [D loss: 9.138055247603916e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963434934616089]\n",
      " 3182 [D loss: 1.2784777936758474e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959444999694824]\n",
      " 3183 [D loss: 1.102707415157056e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963393211364746]\n",
      " 3184 [D loss: 1.8725768313743174e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960877299308777]\n",
      " 3185 [D loss: 3.2898173230933025e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962724447250366]\n",
      " 3186 [D loss: 2.203653366450453e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962387084960938]\n",
      " 3187 [D loss: 1.8945551119031734e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962141513824463]\n",
      " 3188 [D loss: 9.672754458733834e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960939288139343]\n",
      " 3189 [D loss: 3.084681111431564e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961833953857422]\n",
      " 3190 [D loss: 2.947576604128699e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963476657867432]\n",
      " 3191 [D loss: 2.7497694645717274e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963490962982178]\n",
      " 3192 [D loss: 2.170928155464935e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962601065635681]\n",
      " 3193 [D loss: 2.298824256286025e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961031079292297]\n",
      " 3194 [D loss: 3.8102523831184953e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957436323165894]\n",
      " 3195 [D loss: 2.1637624740833417e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958019256591797]\n",
      " 3196 [D loss: 1.4496702533506323e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962611198425293]\n",
      " 3197 [D loss: 5.819934358441969e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996285617351532]\n",
      " 3198 [D loss: 2.8514673431345727e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959418773651123]\n",
      " 3199 [D loss: 1.596136257830949e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950969219207764]\n",
      " 3200 [D loss: 1.9807018816209165e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961392283439636]\n",
      " 3201 [D loss: 2.639966260176152e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996432900428772]\n",
      " 3202 [D loss: 7.870038643886801e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964206218719482]\n",
      " 3203 [D loss: 5.705991952709155e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964340925216675]\n",
      " 3204 [D loss: 2.489949565642746e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962592124938965]\n",
      " 3205 [D loss: 4.6234208639361896e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960648417472839]\n",
      " 3206 [D loss: 7.977768291311804e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960677623748779]\n",
      " 3207 [D loss: 1.0864952855627052e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996458888053894]\n",
      " 3208 [D loss: 2.2928359612706117e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962074756622314]\n",
      " 3209 [D loss: 2.813227183651179e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961315393447876]\n",
      " 3210 [D loss: 7.059858489810722e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961607456207275]\n",
      " 3211 [D loss: 4.2834668420255184e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958972334861755]\n",
      " 3212 [D loss: 2.6425310352351516e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959635734558105]\n",
      " 3213 [D loss: 2.3221284664032282e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954244494438171]\n",
      " 3214 [D loss: 5.110565780341858e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959741830825806]\n",
      " 3215 [D loss: 2.18444279198593e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962172508239746]\n",
      " 3216 [D loss: 2.1248683879093733e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959030747413635]\n",
      " 3217 [D loss: 2.595878322608769e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996288537979126]\n",
      " 3218 [D loss: 2.2378017092705704e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959646463394165]\n",
      " 3219 [D loss: 3.826221472991165e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960905313491821]\n",
      " 3220 [D loss: 2.3639715891476953e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961305856704712]\n",
      " 3221 [D loss: 3.3255137168453075e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996269702911377]\n",
      " 3222 [D loss: 3.109664703515591e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9956074357032776]\n",
      " 3223 [D loss: 4.570405963022495e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961408376693726]\n",
      " 3224 [D loss: 2.089743247779552e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959316253662109]\n",
      " 3225 [D loss: 2.116606992785819e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962718486785889]\n",
      " 3226 [D loss: 2.456542006257223e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960241317749023]\n",
      " 3227 [D loss: 4.707158041128423e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959926605224609]\n",
      " 3228 [D loss: 3.1031391699798405e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959759712219238]\n",
      " 3229 [D loss: 1.5746093140478479e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962337017059326]\n",
      " 3230 [D loss: 2.598441824375186e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962397813796997]\n",
      " 3231 [D loss: 1.2137094017816707e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957242608070374]\n",
      " 3232 [D loss: 1.7178811049234355e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959123134613037]\n",
      " 3233 [D loss: 4.648837148124585e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996157705783844]\n",
      " 3234 [D loss: 3.875022230204195e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961755275726318]\n",
      " 3235 [D loss: 1.8717845478022355e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963827133178711]\n",
      " 3236 [D loss: 1.345252030660049e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959681034088135]\n",
      " 3237 [D loss: 2.755830792011693e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961987137794495]\n",
      " 3238 [D loss: 1.429617896064883e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962388277053833]\n",
      " 3239 [D loss: 1.5750980310258456e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960794448852539]\n",
      " 3240 [D loss: 1.5863104181335075e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960901737213135]\n",
      " 3241 [D loss: 2.574115205788985e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962466955184937]\n",
      " 3242 [D loss: 5.285851329972502e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962499737739563]\n",
      " 3243 [D loss: 3.4059274184983224e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996283769607544]\n",
      " 3244 [D loss: 6.253174888115609e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963468909263611]\n",
      " 3245 [D loss: 4.975762567482889e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959988594055176]\n",
      " 3246 [D loss: 1.751742820488289e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962897300720215]\n",
      " 3247 [D loss: 1.244617669726722e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964679479598999]\n",
      " 3248 [D loss: 1.783669517863018e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963980913162231]\n",
      " 3249 [D loss: 1.5096072729647858e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963487386703491]\n",
      " 3250 [D loss: 8.289126526506152e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964483976364136]\n",
      " 3251 [D loss: 1.6338126442860812e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963064789772034]\n",
      " 3252 [D loss: 1.8220598576590419e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964361190795898]\n",
      " 3253 [D loss: 1.4076687193664839e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964203834533691]\n",
      " 3254 [D loss: 1.633447254789644e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960114359855652]\n",
      " 3255 [D loss: 2.31529384109308e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965497255325317]\n",
      " 3256 [D loss: 4.033332515973598e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965008497238159]\n",
      " 3257 [D loss: 2.046716190307052e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964954853057861]\n",
      " 3258 [D loss: 1.7457286958233453e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965319633483887]\n",
      " 3259 [D loss: 2.371669779677177e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967795610427856]\n",
      " 3260 [D loss: 1.2736415555991698e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966588616371155]\n",
      " 3261 [D loss: 6.534158728754846e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964605569839478]\n",
      " 3262 [D loss: 2.8713532174151624e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964921474456787]\n",
      " 3263 [D loss: 1.3174836567486636e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965935945510864]\n",
      " 3264 [D loss: 5.17752232553903e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966541528701782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3265 [D loss: 5.138087090017507e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965896606445312]\n",
      " 3266 [D loss: 4.319437357480638e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964611530303955]\n",
      " 3267 [D loss: 3.0817750484857243e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996364951133728]\n",
      " 3268 [D loss: 2.607083160910406e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963678121566772]\n",
      " 3269 [D loss: 2.6435959625814576e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963105916976929]\n",
      " 3270 [D loss: 3.3488711778772995e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996492862701416]\n",
      " 3271 [D loss: 3.673108949442394e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962595701217651]\n",
      " 3272 [D loss: 3.528219394866028e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996321439743042]\n",
      " 3273 [D loss: 2.844505615939852e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960446357727051]\n",
      " 3274 [D loss: 1.986943743759184e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961922764778137]\n",
      " 3275 [D loss: 2.949768258986296e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961972236633301]\n",
      " 3276 [D loss: 3.257585831306642e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964861273765564]\n",
      " 3277 [D loss: 9.968189260689542e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962997436523438]\n",
      " 3278 [D loss: 1.3954017049400136e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961740970611572]\n",
      " 3279 [D loss: 4.617560989572667e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962916374206543]\n",
      " 3280 [D loss: 1.9366702872503083e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960094690322876]\n",
      " 3281 [D loss: 1.5506555428146385e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959858655929565]\n",
      " 3282 [D loss: 3.7760510167572647e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996498703956604]\n",
      " 3283 [D loss: 3.5623349958768813e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961861371994019]\n",
      " 3284 [D loss: 1.8420887499814853e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960387945175171]\n",
      " 3285 [D loss: 9.780783329915721e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957635402679443]\n",
      " 3286 [D loss: 1.1045060091419145e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963668584823608]\n",
      " 3287 [D loss: 6.074290013202699e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961296916007996]\n",
      " 3288 [D loss: 2.353404397581471e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962304830551147]\n",
      " 3289 [D loss: 3.576257995518972e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962625503540039]\n",
      " 3290 [D loss: 4.4605849325307645e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959958791732788]\n",
      " 3291 [D loss: 1.4934787941456307e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963449239730835]\n",
      " 3292 [D loss: 3.6041714338352904e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964016675949097]\n",
      " 3293 [D loss: 2.9932145935163135e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962101578712463]\n",
      " 3294 [D loss: 5.9954750213364605e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963834285736084]\n",
      " 3295 [D loss: 2.1143218873476144e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961646795272827]\n",
      " 3296 [D loss: 4.0868885662348475e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962641000747681]\n",
      " 3297 [D loss: 1.8175439890910638e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996284008026123]\n",
      " 3298 [D loss: 1.0599544111755677e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960414171218872]\n",
      " 3299 [D loss: 3.5034520351473475e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965623021125793]\n",
      " 3300 [D loss: 6.2134004110703245e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963161945343018]\n",
      " 3301 [D loss: 1.469375092710834e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958788156509399]\n",
      " 3302 [D loss: 3.2414889119536383e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962245225906372]\n",
      " 3303 [D loss: 9.880781362880953e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963959455490112]\n",
      " 3304 [D loss: 2.0386989945109235e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964325428009033]\n",
      " 3305 [D loss: 4.26722135671298e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961087703704834]\n",
      " 3306 [D loss: 1.9496055756462738e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961549639701843]\n",
      " 3307 [D loss: 1.4344255987452925e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961711168289185]\n",
      " 3308 [D loss: 1.1202710993529763e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963798522949219]\n",
      " 3309 [D loss: 2.9198190532042645e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959971904754639]\n",
      " 3310 [D loss: 3.17816375172697e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99628746509552]\n",
      " 3311 [D loss: 8.883776899892837e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995864748954773]\n",
      " 3312 [D loss: 1.4887873476254754e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961526393890381]\n",
      " 3313 [D loss: 2.833106464095181e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963290095329285]\n",
      " 3314 [D loss: 2.0127852167206584e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957412481307983]\n",
      " 3315 [D loss: 1.6851732652867213e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959800243377686]\n",
      " 3316 [D loss: 2.1593507426587166e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959126710891724]\n",
      " 3317 [D loss: 4.3736254156101495e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963058233261108]\n",
      " 3318 [D loss: 7.769094736431725e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962717294692993]\n",
      " 3319 [D loss: 1.3842271982866805e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963745474815369]\n",
      " 3320 [D loss: 2.1729204036091687e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965721964836121]\n",
      " 3321 [D loss: 3.7655308915418573e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955381155014038]\n",
      " 3322 [D loss: 4.244153842591913e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960978627204895]\n",
      " 3323 [D loss: 1.6073760207291343e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996332049369812]\n",
      " 3324 [D loss: 3.9098376873880625e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966380596160889]\n",
      " 3325 [D loss: 1.2127833542763256e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962870478630066]\n",
      " 3326 [D loss: 1.7098116586566903e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963656663894653]\n",
      " 3327 [D loss: 1.8966695733979577e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963680505752563]\n",
      " 3328 [D loss: 4.1507824789732695e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961037635803223]\n",
      " 3329 [D loss: 5.834892817802029e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964257478713989]\n",
      " 3330 [D loss: 4.438346513779834e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963817000389099]\n",
      " 3331 [D loss: 4.4459364289650694e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965595006942749]\n",
      " 3332 [D loss: 7.2570783231640235e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964892864227295]\n",
      " 3333 [D loss: 5.298396899888758e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964097142219543]\n",
      " 3334 [D loss: 3.0252380383899435e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964966773986816]\n",
      " 3335 [D loss: 5.725174560211599e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967283606529236]\n",
      " 3336 [D loss: 2.9996776902407873e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968550205230713]\n",
      " 3337 [D loss: 2.2339943370752735e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996623158454895]\n",
      " 3338 [D loss: 4.141360477660783e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967285394668579]\n",
      " 3339 [D loss: 1.547844112792518e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967719316482544]\n",
      " 3340 [D loss: 1.6534095266251825e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99681556224823]\n",
      " 3341 [D loss: 4.8012334445957094e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967132806777954]\n",
      " 3342 [D loss: 2.3642337509954814e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967319965362549]\n",
      " 3343 [D loss: 6.551592832693132e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968521595001221]\n",
      " 3344 [D loss: 1.8553946574684232e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969768524169922]\n",
      " 3345 [D loss: 3.3066996820707573e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968197345733643]\n",
      " 3346 [D loss: 3.4320842132729013e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969376921653748]\n",
      " 3347 [D loss: 1.8580699361336883e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968092441558838]\n",
      " 3348 [D loss: 1.3872509043721948e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964809417724609]\n",
      " 3349 [D loss: 1.2526558066383586e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963377118110657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3350 [D loss: 1.596448964846786e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996850848197937]\n",
      " 3351 [D loss: 1.213759787788149e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996860146522522]\n",
      " 3352 [D loss: 0.00010382589971413836, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967715740203857]\n",
      " 3353 [D loss: 1.4366277127919602e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967829585075378]\n",
      " 3354 [D loss: 4.281921519577736e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966851472854614]\n",
      " 3355 [D loss: 5.580810466199182e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964243173599243]\n",
      " 3356 [D loss: 1.9435567082837224e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967498183250427]\n",
      " 3357 [D loss: 4.660369540943066e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964460134506226]\n",
      " 3358 [D loss: 0.00039532833034172654, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963356256484985]\n",
      " 3359 [D loss: 1.3296417819219641e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965916275978088]\n",
      " 3360 [D loss: 1.975319355551619e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966868162155151]\n",
      " 3361 [D loss: 2.2438027372118086e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967570304870605]\n",
      " 3362 [D loss: 4.353580152383074e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969098567962646]\n",
      " 3363 [D loss: 2.1114651644893456e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969527721405029]\n",
      " 3364 [D loss: 1.782757180990302e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996826171875]\n",
      " 3365 [D loss: 1.2379738336676382e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971145391464233]\n",
      " 3366 [D loss: 1.98322504729731e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969136714935303]\n",
      " 3367 [D loss: 3.498899104670272e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968727827072144]\n",
      " 3368 [D loss: 6.113050403655507e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970785975456238]\n",
      " 3369 [D loss: 1.6741080344218062e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971150159835815]\n",
      " 3370 [D loss: 1.2871809076386853e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970848560333252]\n",
      " 3371 [D loss: 1.3424256394500844e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970057606697083]\n",
      " 3372 [D loss: 1.8623682080942672e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969967007637024]\n",
      " 3373 [D loss: 2.0360150756459916e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971304535865784]\n",
      " 3374 [D loss: 1.1376868087609182e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970422983169556]\n",
      " 3375 [D loss: 7.876972813392058e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969557523727417]\n",
      " 3376 [D loss: 1.3248818504507653e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969650506973267]\n",
      " 3377 [D loss: 2.236165528302081e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997004508972168]\n",
      " 3378 [D loss: 0.0001985935668926686, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968384504318237]\n",
      " 3379 [D loss: 1.3341984868020518e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970089197158813]\n",
      " 3380 [D loss: 4.751073902298231e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967339634895325]\n",
      " 3381 [D loss: 6.066892638045829e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965919256210327]\n",
      " 3382 [D loss: 1.9347426132299006e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996510922908783]\n",
      " 3383 [D loss: 1.2301738024689257e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966676235198975]\n",
      " 3384 [D loss: 3.0850551411276683e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962149262428284]\n",
      " 3385 [D loss: 9.593606591806747e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966205358505249]\n",
      " 3386 [D loss: 1.528532084194012e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964584708213806]\n",
      " 3387 [D loss: 1.8911728147941176e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963926076889038]\n",
      " 3388 [D loss: 4.264809831511229e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963979721069336]\n",
      " 3389 [D loss: 3.527744411258027e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966444373130798]\n",
      " 3390 [D loss: 1.583729317644611e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963313341140747]\n",
      " 3391 [D loss: 2.932286406576168e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967344999313354]\n",
      " 3392 [D loss: 5.6802364269969985e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963897466659546]\n",
      " 3393 [D loss: 1.645291945351346e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963171482086182]\n",
      " 3394 [D loss: 3.608675797295291e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995997965335846]\n",
      " 3395 [D loss: 2.50573111770791e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961244463920593]\n",
      " 3396 [D loss: 5.420023626356851e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962844848632812]\n",
      " 3397 [D loss: 1.1901274774572812e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996239185333252]\n",
      " 3398 [D loss: 2.401443225608091e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963212013244629]\n",
      " 3399 [D loss: 9.035561561177019e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961721897125244]\n",
      " 3400 [D loss: 3.332420419610571e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963545203208923]\n",
      " 3401 [D loss: 0.00010400282189948484, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962579011917114]\n",
      " 3402 [D loss: 2.071770722977817e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964035153388977]\n",
      " 3403 [D loss: 1.1575362805160694e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963715076446533]\n",
      " 3404 [D loss: 7.167808234953554e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962406754493713]\n",
      " 3405 [D loss: 1.0059361557068769e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963829517364502]\n",
      " 3406 [D loss: 3.943712727050297e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964901208877563]\n",
      " 3407 [D loss: 8.255665306933224e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964005947113037]\n",
      " 3408 [D loss: 7.75478474679403e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966592192649841]\n",
      " 3409 [D loss: 2.171570258724387e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968082308769226]\n",
      " 3410 [D loss: 2.6199998046649853e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963386058807373]\n",
      " 3411 [D loss: 2.6117879315279424e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968758821487427]\n",
      " 3412 [D loss: 3.5709635994862765e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966400861740112]\n",
      " 3413 [D loss: 1.6857003402037662e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964842200279236]\n",
      " 3414 [D loss: 3.2733805710449815e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968410134315491]\n",
      " 3415 [D loss: 4.70246777695138e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966602921485901]\n",
      " 3416 [D loss: 2.6284274099452887e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968757629394531]\n",
      " 3417 [D loss: 2.0463014152483083e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967554807662964]\n",
      " 3418 [D loss: 1.999427877308335e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996650755405426]\n",
      " 3419 [D loss: 8.72454165801173e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965255856513977]\n",
      " 3420 [D loss: 7.101988012436777e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968889951705933]\n",
      " 3421 [D loss: 1.296256073146651e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967193007469177]\n",
      " 3422 [D loss: 2.7660826162900776e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967522025108337]\n",
      " 3423 [D loss: 9.275027878175024e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968535304069519]\n",
      " 3424 [D loss: 2.4976348868221976e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996851921081543]\n",
      " 3425 [D loss: 5.101263013784774e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996921718120575]\n",
      " 3426 [D loss: 6.93374895490706e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968403577804565]\n",
      " 3427 [D loss: 1.4073718830331927e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968618154525757]\n",
      " 3428 [D loss: 3.63473518518731e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969263076782227]\n",
      " 3429 [D loss: 6.4799460233189166e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996664822101593]\n",
      " 3430 [D loss: 2.365689397265669e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965084791183472]\n",
      " 3431 [D loss: 2.3291351681109518e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964849948883057]\n",
      " 3432 [D loss: 2.7674764169205446e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966772794723511]\n",
      " 3433 [D loss: 2.5582057787687518e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965892434120178]\n",
      " 3434 [D loss: 2.148789917555405e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967558979988098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3435 [D loss: 3.892801942129154e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967308640480042]\n",
      " 3436 [D loss: 1.3055277122475673e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965707659721375]\n",
      " 3437 [D loss: 1.4451543393079191e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996562659740448]\n",
      " 3438 [D loss: 3.0644318940176163e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967104196548462]\n",
      " 3439 [D loss: 2.256624156871112e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964784383773804]\n",
      " 3440 [D loss: 1.4492674154098495e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965823888778687]\n",
      " 3441 [D loss: 9.293470952798089e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965341091156006]\n",
      " 3442 [D loss: 9.516354111838154e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967560768127441]\n",
      " 3443 [D loss: 2.6968448310071835e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965507984161377]\n",
      " 3444 [D loss: 3.4982938359462423e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966223239898682]\n",
      " 3445 [D loss: 1.6535573195142206e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963006377220154]\n",
      " 3446 [D loss: 8.058027560764458e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966040849685669]\n",
      " 3447 [D loss: 3.056630703213159e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967535734176636]\n",
      " 3448 [D loss: 4.958972567692399e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965356588363647]\n",
      " 3449 [D loss: 2.5591771191102453e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966059327125549]\n",
      " 3450 [D loss: 1.4216134331945796e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963570833206177]\n",
      " 3451 [D loss: 1.3715900422539562e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996510922908783]\n",
      " 3452 [D loss: 7.293131602637004e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967814087867737]\n",
      " 3453 [D loss: 2.649734142323723e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965596199035645]\n",
      " 3454 [D loss: 1.497272478445666e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966328144073486]\n",
      " 3455 [D loss: 2.5477099825366167e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966287016868591]\n",
      " 3456 [D loss: 1.9378339857212268e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965121150016785]\n",
      " 3457 [D loss: 1.3693634173250757e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965055584907532]\n",
      " 3458 [D loss: 1.6398789739469066e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966059327125549]\n",
      " 3459 [D loss: 1.4591873878089245e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996789813041687]\n",
      " 3460 [D loss: 4.685171916207764e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968377947807312]\n",
      " 3461 [D loss: 8.694645657669753e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966013431549072]\n",
      " 3462 [D loss: 1.457791313441703e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965143203735352]\n",
      " 3463 [D loss: 4.183394594292622e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965413808822632]\n",
      " 3464 [D loss: 3.999562977696769e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967001676559448]\n",
      " 3465 [D loss: 2.5640147214289755e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965322613716125]\n",
      " 3466 [D loss: 1.773175063135568e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966333508491516]\n",
      " 3467 [D loss: 3.8491612031066325e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966698884963989]\n",
      " 3468 [D loss: 2.5083543278014986e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966754913330078]\n",
      " 3469 [D loss: 1.3353011127037462e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966185092926025]\n",
      " 3470 [D loss: 5.584733116847929e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966393709182739]\n",
      " 3471 [D loss: 1.8950213416246697e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966148138046265]\n",
      " 3472 [D loss: 2.1415842184069334e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963850975036621]\n",
      " 3473 [D loss: 1.9213673567719525e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966356158256531]\n",
      " 3474 [D loss: 2.0382183265610365e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996607780456543]\n",
      " 3475 [D loss: 1.0121335435542278e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967515468597412]\n",
      " 3476 [D loss: 8.330925993504934e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968796968460083]\n",
      " 3477 [D loss: 3.984831892012153e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966453313827515]\n",
      " 3478 [D loss: 3.3414764857297996e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967746138572693]\n",
      " 3479 [D loss: 1.568301399856864e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965227842330933]\n",
      " 3480 [D loss: 4.14625319535844e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963403940200806]\n",
      " 3481 [D loss: 4.0042432374320924e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963761568069458]\n",
      " 3482 [D loss: 4.20980222770595e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965603351593018]\n",
      " 3483 [D loss: 1.6135918485815637e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963008165359497]\n",
      " 3484 [D loss: 1.635271019040374e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965593218803406]\n",
      " 3485 [D loss: 1.4116551483311923e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961222410202026]\n",
      " 3486 [D loss: 3.3723076739988755e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964524507522583]\n",
      " 3487 [D loss: 4.603721208695788e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965471625328064]\n",
      " 3488 [D loss: 1.760723762345151e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964133501052856]\n",
      " 3489 [D loss: 2.070777554763481e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963446855545044]\n",
      " 3490 [D loss: 1.6523455315109459e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965296983718872]\n",
      " 3491 [D loss: 2.075673000945244e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996294379234314]\n",
      " 3492 [D loss: 3.7559341308224248e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965696334838867]\n",
      " 3493 [D loss: 1.947904138432932e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959584474563599]\n",
      " 3494 [D loss: 1.9412324036238715e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964157342910767]\n",
      " 3495 [D loss: 1.2720877293759258e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963585138320923]\n",
      " 3496 [D loss: 2.3639363462280016e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963845610618591]\n",
      " 3497 [D loss: 1.745409463183023e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966434240341187]\n",
      " 3498 [D loss: 3.026416152351885e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964797496795654]\n",
      " 3499 [D loss: 5.601654265774414e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964349269866943]\n",
      " 3500 [D loss: 3.991943594883196e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957212209701538]\n",
      " 3501 [D loss: 2.413965830783127e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966033697128296]\n",
      " 3502 [D loss: 2.899441369663691e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965083599090576]\n",
      " 3503 [D loss: 2.7350840809958754e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965111017227173]\n",
      " 3504 [D loss: 1.4151362847769633e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965881109237671]\n",
      " 3505 [D loss: 2.8290778573136777e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963532090187073]\n",
      " 3506 [D loss: 2.1561168978223577e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965000748634338]\n",
      " 3507 [D loss: 1.0911929166468326e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966020584106445]\n",
      " 3508 [D loss: 0.00036874949000775814, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966644048690796]\n",
      " 3509 [D loss: 1.6460415281471796e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965616464614868]\n",
      " 3510 [D loss: 2.252093963761581e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967647194862366]\n",
      " 3511 [D loss: 1.814682832446124e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970859885215759]\n",
      " 3512 [D loss: 1.418209421899519e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972453117370605]\n",
      " 3513 [D loss: 1.129207703343127e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9973818063735962]\n",
      " 3514 [D loss: 2.994291207869537e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997503399848938]\n",
      " 3515 [D loss: 1.3664208609043271e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9974826574325562]\n",
      " 3516 [D loss: 0.00023319842875935137, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9974682331085205]\n",
      " 3517 [D loss: 2.0969899196643382e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972519874572754]\n",
      " 3518 [D loss: 0.00012493682152125984, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971789717674255]\n",
      " 3519 [D loss: 7.776121492497623e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970223903656006]\n",
      " 3520 [D loss: 4.784717930306215e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971330761909485]\n",
      " 3521 [D loss: 4.158201591053512e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969422221183777]\n",
      " 3522 [D loss: 1.5161776900640689e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996995210647583]\n",
      " 3523 [D loss: 2.8510198717413004e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966953992843628]\n",
      " 3524 [D loss: 2.800517904688604e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968106746673584]\n",
      " 3525 [D loss: 2.7480762128107017e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966690540313721]\n",
      " 3526 [D loss: 3.2937612104433356e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964896440505981]\n",
      " 3527 [D loss: 2.5840922717179637e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99675452709198]\n",
      " 3528 [D loss: 4.823559720534831e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966648817062378]\n",
      " 3529 [D loss: 1.5742126606710372e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966717958450317]\n",
      " 3530 [D loss: 1.1759490234908299e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964367747306824]\n",
      " 3531 [D loss: 2.107557975250529e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996737003326416]\n",
      " 3532 [D loss: 2.067590230581118e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967595338821411]\n",
      " 3533 [D loss: 1.6331973711203318e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965646266937256]\n",
      " 3534 [D loss: 2.7081798634753795e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966859817504883]\n",
      " 3535 [D loss: 1.4135008541416028e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966493844985962]\n",
      " 3536 [D loss: 1.498292135693191e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965214729309082]\n",
      " 3537 [D loss: 1.420012381458946e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968724250793457]\n",
      " 3538 [D loss: 5.787333520856919e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968389272689819]\n",
      " 3539 [D loss: 2.135909426215221e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964416027069092]\n",
      " 3540 [D loss: 1.3566065035774955e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967360496520996]\n",
      " 3541 [D loss: 1.1530244137247792e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968562126159668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3542 [D loss: 2.262221278215293e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965560436248779]\n",
      " 3543 [D loss: 6.26711835138849e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964879751205444]\n",
      " 3544 [D loss: 1.3353057966014603e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967809915542603]\n",
      " 3545 [D loss: 2.983122840305441e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967398643493652]\n",
      " 3546 [D loss: 9.012730515678413e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965875148773193]\n",
      " 3547 [D loss: 5.492086984304478e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996529221534729]\n",
      " 3548 [D loss: 1.8475886463420466e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961357712745667]\n",
      " 3549 [D loss: 8.999025158118457e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964638948440552]\n",
      " 3550 [D loss: 3.50005484506255e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996446967124939]\n",
      " 3551 [D loss: 1.358469717160915e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966422915458679]\n",
      " 3552 [D loss: 2.8405067951098317e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996574342250824]\n",
      " 3553 [D loss: 2.7510100153449457e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968268871307373]\n",
      " 3554 [D loss: 2.0199670416332083e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965357780456543]\n",
      " 3555 [D loss: 3.537935117492452e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964750409126282]\n",
      " 3556 [D loss: 3.0257735943450825e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967687129974365]\n",
      " 3557 [D loss: 2.042288087977795e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967182874679565]\n",
      " 3558 [D loss: 7.738762178632896e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965906143188477]\n",
      " 3559 [D loss: 5.608582341665169e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966917634010315]\n",
      " 3560 [D loss: 2.1224836018518545e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996507465839386]\n",
      " 3561 [D loss: 1.378733850287972e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996434211730957]\n",
      " 3562 [D loss: 2.625197339511942e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996738612651825]\n",
      " 3563 [D loss: 1.860446900536772e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965628981590271]\n",
      " 3564 [D loss: 1.4696677681058645e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966112375259399]\n",
      " 3565 [D loss: 1.5245871054503368e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966869354248047]\n",
      " 3566 [D loss: 7.90650155977346e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966892004013062]\n",
      " 3567 [D loss: 1.2196774150652345e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966323971748352]\n",
      " 3568 [D loss: 2.549493274273118e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968276023864746]\n",
      " 3569 [D loss: 1.5612528159181238e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969111084938049]\n",
      " 3570 [D loss: 1.141395387094235e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969428777694702]\n",
      " 3571 [D loss: 1.773611870703462e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968256950378418]\n",
      " 3572 [D loss: 2.009656327572884e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968511462211609]\n",
      " 3573 [D loss: 5.8303799050918315e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969698190689087]\n",
      " 3574 [D loss: 1.1940875310756383e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996925413608551]\n",
      " 3575 [D loss: 1.9146020804328145e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969005584716797]\n",
      " 3576 [D loss: 1.1387346603441983e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969373345375061]\n",
      " 3577 [D loss: 6.751305591023993e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966934323310852]\n",
      " 3578 [D loss: 8.449574124824721e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970305562019348]\n",
      " 3579 [D loss: 4.708411779574817e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969773292541504]\n",
      " 3580 [D loss: 7.172521236498142e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997134804725647]\n",
      " 3581 [D loss: 4.489728326007025e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969058036804199]\n",
      " 3582 [D loss: 2.6865614017879125e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969495534896851]\n",
      " 3583 [D loss: 1.2306737744438578e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970158338546753]\n",
      " 3584 [D loss: 1.437204446119722e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968268275260925]\n",
      " 3585 [D loss: 2.0199081518512685e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970147609710693]\n",
      " 3586 [D loss: 2.2084054762672167e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969688653945923]\n",
      " 3587 [D loss: 1.1899681339855306e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969736933708191]\n",
      " 3588 [D loss: 1.6332020322806784e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968223571777344]\n",
      " 3589 [D loss: 1.9699680251505924e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968545436859131]\n",
      " 3590 [D loss: 1.875568341347389e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997032880783081]\n",
      " 3591 [D loss: 2.971118874484091e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970182180404663]\n",
      " 3592 [D loss: 5.792267984361388e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967769384384155]\n",
      " 3593 [D loss: 1.2310240435908781e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968953132629395]\n",
      " 3594 [D loss: 9.783812856767327e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969128370285034]\n",
      " 3595 [D loss: 9.704736294224858e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969497919082642]\n",
      " 3596 [D loss: 5.926782705500955e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966212511062622]\n",
      " 3597 [D loss: 1.9424830952630145e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965167045593262]\n",
      " 3598 [D loss: 1.1898373486474156e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966468811035156]\n",
      " 3599 [D loss: 3.6237411222828086e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960284233093262]\n",
      " 3600 [D loss: 4.034040102851577e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963679313659668]\n",
      " 3601 [D loss: 2.122662863257574e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964137077331543]\n",
      " 3602 [D loss: 1.9306842204969143e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963851571083069]\n",
      " 3603 [D loss: 2.9639611966558732e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964486360549927]\n",
      " 3604 [D loss: 3.5790053516393527e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962417483329773]\n",
      " 3605 [D loss: 2.106300598825328e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965381622314453]\n",
      " 3606 [D loss: 4.146432274865219e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9956657886505127]\n",
      " 3607 [D loss: 0.00011028229346266016, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996381938457489]\n",
      " 3608 [D loss: 2.9837760848749895e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966100454330444]\n",
      " 3609 [D loss: 1.713695837679552e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966295957565308]\n",
      " 3610 [D loss: 4.252298822393641e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996589183807373]\n",
      " 3611 [D loss: 2.6268107831128873e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966241121292114]\n",
      " 3612 [D loss: 3.125180001006811e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964084625244141]\n",
      " 3613 [D loss: 1.2830284958909033e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967235922813416]\n",
      " 3614 [D loss: 1.9383046492293943e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964216351509094]\n",
      " 3615 [D loss: 4.181404165137792e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966862201690674]\n",
      " 3616 [D loss: 3.484858552837977e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965381026268005]\n",
      " 3617 [D loss: 2.5679612463136436e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966684579849243]\n",
      " 3618 [D loss: 1.6099053027573973e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965265393257141]\n",
      " 3619 [D loss: 1.6272024367935956e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965266585350037]\n",
      " 3620 [D loss: 1.1806382644863334e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996721088886261]\n",
      " 3621 [D loss: 2.1195833141973708e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968019723892212]\n",
      " 3622 [D loss: 7.928804734547157e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966909289360046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3623 [D loss: 1.5056285747050424e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966968894004822]\n",
      " 3624 [D loss: 7.119600013538729e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996780276298523]\n",
      " 3625 [D loss: 1.9394765331526287e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965878129005432]\n",
      " 3626 [D loss: 2.713012918320601e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966119527816772]\n",
      " 3627 [D loss: 1.7877501932161977e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967284202575684]\n",
      " 3628 [D loss: 1.8077113054459915e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962040781974792]\n",
      " 3629 [D loss: 1.1099837138317525e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966674447059631]\n",
      " 3630 [D loss: 1.6519866221642587e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967193603515625]\n",
      " 3631 [D loss: 2.7533969841897488e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966182112693787]\n",
      " 3632 [D loss: 1.9718672774615698e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965903759002686]\n",
      " 3633 [D loss: 1.0395440767752007e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965247511863708]\n",
      " 3634 [D loss: 1.4706093907079776e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966003894805908]\n",
      " 3635 [D loss: 3.125746843579691e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965832829475403]\n",
      " 3636 [D loss: 1.6665352404743317e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967182874679565]\n",
      " 3637 [D loss: 8.682110092195217e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965454339981079]\n",
      " 3638 [D loss: 7.598997399327345e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966651201248169]\n",
      " 3639 [D loss: 1.2846962818002794e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963991045951843]\n",
      " 3640 [D loss: 5.759323357779067e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966529607772827]\n",
      " 3641 [D loss: 2.5797266971494537e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967220425605774]\n",
      " 3642 [D loss: 2.439662694087019e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964677095413208]\n",
      " 3643 [D loss: 2.4296232368214987e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966155290603638]\n",
      " 3644 [D loss: 1.6619765119685326e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967155456542969]\n",
      " 3645 [D loss: 4.831350452150218e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966734647750854]\n",
      " 3646 [D loss: 3.854995156871155e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967148303985596]\n",
      " 3647 [D loss: 1.628431505196204e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965433478355408]\n",
      " 3648 [D loss: 1.512279050075449e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996450662612915]\n",
      " 3649 [D loss: 2.745094434430939e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964823126792908]\n",
      " 3650 [D loss: 1.9811619495158084e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966635704040527]\n",
      " 3651 [D loss: 1.2194186638225801e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966617226600647]\n",
      " 3652 [D loss: 6.979775207582861e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963549375534058]\n",
      " 3653 [D loss: 2.2283488760876935e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968041181564331]\n",
      " 3654 [D loss: 2.0742620563396486e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967451691627502]\n",
      " 3655 [D loss: 1.6391454664699268e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996816098690033]\n",
      " 3656 [D loss: 6.919407496752683e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967595338821411]\n",
      " 3657 [D loss: 3.0728738238394726e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996810793876648]\n",
      " 3658 [D loss: 4.590652224578662e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966851472854614]\n",
      " 3659 [D loss: 1.1864826774399262e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996747612953186]\n",
      " 3660 [D loss: 4.507072844717186e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962286949157715]\n",
      " 3661 [D loss: 1.0782744084281148e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965851306915283]\n",
      " 3662 [D loss: 8.938272003433667e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969005584716797]\n",
      " 3663 [D loss: 1.694505954219494e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967705011367798]\n",
      " 3664 [D loss: 1.3476621916197473e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966790676116943]\n",
      " 3665 [D loss: 1.5717266705905786e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965684413909912]\n",
      " 3666 [D loss: 1.3733342711930163e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967613220214844]\n",
      " 3667 [D loss: 2.0000750737381168e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965158700942993]\n",
      " 3668 [D loss: 4.080218786839396e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965683817863464]\n",
      " 3669 [D loss: 1.0418266356282402e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969866871833801]\n",
      " 3670 [D loss: 1.5969549167493824e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967483282089233]\n",
      " 3671 [D loss: 2.3997413336473983e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968228340148926]\n",
      " 3672 [D loss: 2.1295916212693555e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967057704925537]\n",
      " 3673 [D loss: 6.932677933946252e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967943429946899]\n",
      " 3674 [D loss: 5.299866188579472e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967799782752991]\n",
      " 3675 [D loss: 2.5412327886442654e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968433976173401]\n",
      " 3676 [D loss: 3.951980943384115e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966233968734741]\n",
      " 3677 [D loss: 1.5105792954273056e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966155886650085]\n",
      " 3678 [D loss: 2.3500256247643847e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967597723007202]\n",
      " 3679 [D loss: 9.63261572906049e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966874122619629]\n",
      " 3680 [D loss: 2.2757249098503962e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967339038848877]\n",
      " 3681 [D loss: 1.1324297020109952e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968271255493164]\n",
      " 3682 [D loss: 1.3187398053560173e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969858527183533]\n",
      " 3683 [D loss: 3.6226833799446467e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967061281204224]\n",
      " 3684 [D loss: 1.6056653748819372e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966446757316589]\n",
      " 3685 [D loss: 3.7876502574363258e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965502023696899]\n",
      " 3686 [D loss: 3.065744067498599e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965444207191467]\n",
      " 3687 [D loss: 1.8506510741644888e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996627688407898]\n",
      " 3688 [D loss: 6.609137017221656e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996796727180481]\n",
      " 3689 [D loss: 7.375141649390571e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966777563095093]\n",
      " 3690 [D loss: 1.4854315395496087e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969188570976257]\n",
      " 3691 [D loss: 3.499510057736188e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996660590171814]\n",
      " 3692 [D loss: 2.284111133121769e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965722560882568]\n",
      " 3693 [D loss: 1.0375665624451358e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968035221099854]\n",
      " 3694 [D loss: 5.2541063269018196e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966312646865845]\n",
      " 3695 [D loss: 3.003708570759045e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966095089912415]\n",
      " 3696 [D loss: 3.0347289339260897e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965760707855225]\n",
      " 3697 [D loss: 1.916527480716468e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996791422367096]\n",
      " 3698 [D loss: 1.1709007594618015e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967296123504639]\n",
      " 3699 [D loss: 1.1630108929239213e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966149926185608]\n",
      " 3700 [D loss: 6.815806045779027e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967550039291382]\n",
      " 3701 [D loss: 8.978398909675889e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968781471252441]\n",
      " 3702 [D loss: 5.5417685871361755e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968135356903076]\n",
      " 3703 [D loss: 6.838204171799589e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967083930969238]\n",
      " 3704 [D loss: 5.315812813933007e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996640145778656]\n",
      " 3705 [D loss: 9.152715392701793e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967190623283386]\n",
      " 3706 [D loss: 1.7448262497055111e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966694116592407]\n",
      " 3707 [D loss: 1.4818097042734735e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99665367603302]\n",
      " 3708 [D loss: 1.5066369769556331e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967434406280518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3709 [D loss: 3.693239250424085e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968886375427246]\n",
      " 3710 [D loss: 1.8167118469136767e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967728853225708]\n",
      " 3711 [D loss: 6.099332495068666e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967238903045654]\n",
      " 3712 [D loss: 5.3310399380279705e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967688322067261]\n",
      " 3713 [D loss: 5.393522314989241e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969531297683716]\n",
      " 3714 [D loss: 1.7541121906106127e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996832013130188]\n",
      " 3715 [D loss: 1.5489681572944392e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997028112411499]\n",
      " 3716 [D loss: 1.2530592812254326e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968960881233215]\n",
      " 3717 [D loss: 4.604817149811424e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970410466194153]\n",
      " 3718 [D loss: 3.2600817121419823e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969989061355591]\n",
      " 3719 [D loss: 1.3201109140936751e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969501495361328]\n",
      " 3720 [D loss: 1.2211252169436193e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969449043273926]\n",
      " 3721 [D loss: 2.0345817119959975e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971599578857422]\n",
      " 3722 [D loss: 6.324728474282892e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970921874046326]\n",
      " 3723 [D loss: 1.932406576088397e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972125291824341]\n",
      " 3724 [D loss: 1.8126369241144857e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969061017036438]\n",
      " 3725 [D loss: 2.0228369521646528e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997187077999115]\n",
      " 3726 [D loss: 1.2471782611100934e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972481727600098]\n",
      " 3727 [D loss: 1.0863116131076822e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997077465057373]\n",
      " 3728 [D loss: 1.0778736623251461e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997170090675354]\n",
      " 3729 [D loss: 9.891481340673636e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971500635147095]\n",
      " 3730 [D loss: 1.323172909906134e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996962308883667]\n",
      " 3731 [D loss: 1.0570073527560453e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970762729644775]\n",
      " 3732 [D loss: 1.301782276641461e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970301389694214]\n",
      " 3733 [D loss: 2.096512162097497e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997231125831604]\n",
      " 3734 [D loss: 1.7154803799712681e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971631765365601]\n",
      " 3735 [D loss: 3.47412824339699e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969176054000854]\n",
      " 3736 [D loss: 2.357616040171706e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972426295280457]\n",
      " 3737 [D loss: 1.1608557315412327e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971818923950195]\n",
      " 3738 [D loss: 1.4993310060162912e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970013499259949]\n",
      " 3739 [D loss: 1.3574946251537767e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996940016746521]\n",
      " 3740 [D loss: 2.560118218752905e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970309138298035]\n",
      " 3741 [D loss: 1.2954527846886776e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996856689453125]\n",
      " 3742 [D loss: 1.3890062291466165e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996928870677948]\n",
      " 3743 [D loss: 8.96301628472429e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969128966331482]\n",
      " 3744 [D loss: 7.1491867856821045e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996880292892456]\n",
      " 3745 [D loss: 1.545738768982119e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968161582946777]\n",
      " 3746 [D loss: 4.624576467904262e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967942833900452]\n",
      " 3747 [D loss: 1.7641698377701687e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996862530708313]\n",
      " 3748 [D loss: 1.0773976555356057e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967488646507263]\n",
      " 3749 [D loss: 2.8572301289386814e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969241619110107]\n",
      " 3750 [D loss: 2.2741546672477853e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99692702293396]\n",
      " 3751 [D loss: 1.365159846500319e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967318773269653]\n",
      " 3752 [D loss: 4.017726951133227e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968416690826416]\n",
      " 3753 [D loss: 1.2539896943053463e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966663122177124]\n",
      " 3754 [D loss: 1.6917069842747878e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968031644821167]\n",
      " 3755 [D loss: 1.7705806385492906e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967988729476929]\n",
      " 3756 [D loss: 1.3390273352342774e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969667196273804]\n",
      " 3757 [D loss: 2.3138677534007e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966793656349182]\n",
      " 3758 [D loss: 1.9701456039911136e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966820478439331]\n",
      " 3759 [D loss: 1.919181158882566e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969075918197632]\n",
      " 3760 [D loss: 1.936735316121485e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968134164810181]\n",
      " 3761 [D loss: 1.2743029174089315e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968379735946655]\n",
      " 3762 [D loss: 6.69030441713403e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968951940536499]\n",
      " 3763 [D loss: 1.539444383524824e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968169927597046]\n",
      " 3764 [D loss: 2.14638657780597e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99632728099823]\n",
      " 3765 [D loss: 8.22308356873691e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969439506530762]\n",
      " 3766 [D loss: 2.578167823230615e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966406226158142]\n",
      " 3767 [D loss: 2.699347987800138e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968588352203369]\n",
      " 3768 [D loss: 1.5008199625299312e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968177080154419]\n",
      " 3769 [D loss: 1.855520508797781e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967490434646606]\n",
      " 3770 [D loss: 1.6817159576021368e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966721534729004]\n",
      " 3771 [D loss: 7.238727903313702e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966764450073242]\n",
      " 3772 [D loss: 2.330489678570302e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965498447418213]\n",
      " 3773 [D loss: 4.4599037209991366e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99667888879776]\n",
      " 3774 [D loss: 7.440864919772139e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967384338378906]\n",
      " 3775 [D loss: 2.217232122347923e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996955394744873]\n",
      " 3776 [D loss: 3.67035727322218e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965864419937134]\n",
      " 3777 [D loss: 2.7337021037965314e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967664480209351]\n",
      " 3778 [D loss: 2.9530988285841886e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968200922012329]\n",
      " 3779 [D loss: 1.769838831933157e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968106150627136]\n",
      " 3780 [D loss: 2.5138308501482243e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967447519302368]\n",
      " 3781 [D loss: 1.3128767477610381e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967449307441711]\n",
      " 3782 [D loss: 2.820036115736002e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965155720710754]\n",
      " 3783 [D loss: 2.823483328029397e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968218207359314]\n",
      " 3784 [D loss: 2.4555301934015006e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996629536151886]\n",
      " 3785 [D loss: 1.6959193089860491e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968785047531128]\n",
      " 3786 [D loss: 2.9283628464327194e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968020915985107]\n",
      " 3787 [D loss: 5.1038232413702644e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967780113220215]\n",
      " 3788 [D loss: 7.762077075312845e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996821403503418]\n",
      " 3789 [D loss: 2.688144832063699e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967352151870728]\n",
      " 3790 [D loss: 1.0449600722495234e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996592104434967]\n",
      " 3791 [D loss: 4.4700614125758875e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968119263648987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3792 [D loss: 4.732705747301225e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996618390083313]\n",
      " 3793 [D loss: 7.680489034100901e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99689781665802]\n",
      " 3794 [D loss: 1.301110501117364e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996607780456543]\n",
      " 3795 [D loss: 1.7409072370355716e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964461326599121]\n",
      " 3796 [D loss: 6.041729193384526e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966608285903931]\n",
      " 3797 [D loss: 5.239456640993012e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968745708465576]\n",
      " 3798 [D loss: 1.8275162574354908e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967350363731384]\n",
      " 3799 [D loss: 1.6364930388590437e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966368079185486]\n",
      " 3800 [D loss: 3.1356687486550072e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996737003326416]\n",
      " 3801 [D loss: 2.038805905613117e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968146085739136]\n",
      " 3802 [D loss: 1.774068437043752e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966313242912292]\n",
      " 3803 [D loss: 5.278756816551322e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967812299728394]\n",
      " 3804 [D loss: 2.210558704973664e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965053796768188]\n",
      " 3805 [D loss: 8.02892191131832e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965637922286987]\n",
      " 3806 [D loss: 1.355327094643144e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967422485351562]\n",
      " 3807 [D loss: 1.2407674603309715e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966765642166138]\n",
      " 3808 [D loss: 5.293115009408211e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967056512832642]\n",
      " 3809 [D loss: 1.8245209503220394e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967888593673706]\n",
      " 3810 [D loss: 1.6279437886623782e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966540336608887]\n",
      " 3811 [D loss: 1.1615994708336075e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967913627624512]\n",
      " 3812 [D loss: 2.8193027901579626e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966741800308228]\n",
      " 3813 [D loss: 1.4442661267821677e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967368841171265]\n",
      " 3814 [D loss: 1.9679623619595077e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966598749160767]\n",
      " 3815 [D loss: 5.29511999047827e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967931509017944]\n",
      " 3816 [D loss: 1.3070681461613276e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967060089111328]\n",
      " 3817 [D loss: 1.1524680303409696e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966191053390503]\n",
      " 3818 [D loss: 2.464139470248483e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969021081924438]\n",
      " 3819 [D loss: 7.337134775298182e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967195391654968]\n",
      " 3820 [D loss: 1.1758584150811657e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965475797653198]\n",
      " 3821 [D loss: 1.3426251825876534e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967302680015564]\n",
      " 3822 [D loss: 5.747943760070484e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967411756515503]\n",
      " 3823 [D loss: 4.718623131338973e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968317747116089]\n",
      " 3824 [D loss: 2.834700353560038e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968135356903076]\n",
      " 3825 [D loss: 2.329669769096654e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967362284660339]\n",
      " 3826 [D loss: 1.7216055994140333e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967954158782959]\n",
      " 3827 [D loss: 3.654892680060584e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968023300170898]\n",
      " 3828 [D loss: 1.4747338354936801e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965735077857971]\n",
      " 3829 [D loss: 1.0327525160391815e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966511130332947]\n",
      " 3830 [D loss: 2.9085435926390346e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966475963592529]\n",
      " 3831 [D loss: 4.16958891946706e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968409538269043]\n",
      " 3832 [D loss: 4.183626970188925e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970073699951172]\n",
      " 3833 [D loss: 1.1757117590605048e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968252182006836]\n",
      " 3834 [D loss: 1.3226159580881358e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968531131744385]\n",
      " 3835 [D loss: 1.3874470141672646e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970151782035828]\n",
      " 3836 [D loss: 7.722308509983122e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970302581787109]\n",
      " 3837 [D loss: 4.915081262879539e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967279434204102]\n",
      " 3838 [D loss: 1.336312607236323e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968112707138062]\n",
      " 3839 [D loss: 5.879895979887806e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967725872993469]\n",
      " 3840 [D loss: 1.0047054274764378e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968345761299133]\n",
      " 3841 [D loss: 1.3592665482065058e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969143271446228]\n",
      " 3842 [D loss: 1.4658713780590915e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968161582946777]\n",
      " 3843 [D loss: 4.118622200621758e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967188835144043]\n",
      " 3844 [D loss: 2.4640887659188593e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969276189804077]\n",
      " 3845 [D loss: 1.2543141565402038e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967206716537476]\n",
      " 3846 [D loss: 1.287334953303798e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968898892402649]\n",
      " 3847 [D loss: 3.192536678398028e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997002899646759]\n",
      " 3848 [D loss: 3.987852323916741e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969395399093628]\n",
      " 3849 [D loss: 1.3074652542854892e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968674182891846]\n",
      " 3850 [D loss: 8.011440513655543e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966060519218445]\n",
      " 3851 [D loss: 1.1360218650224851e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968007802963257]\n",
      " 3852 [D loss: 2.111794628945063e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968757629394531]\n",
      " 3853 [D loss: 1.7455106444685953e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968132972717285]\n",
      " 3854 [D loss: 1.905612634800491e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967962503433228]\n",
      " 3855 [D loss: 2.039300397882471e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968627691268921]\n",
      " 3856 [D loss: 2.3498751033912413e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968171119689941]\n",
      " 3857 [D loss: 1.2900261481263442e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968236684799194]\n",
      " 3858 [D loss: 1.5443431038875133e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967532753944397]\n",
      " 3859 [D loss: 9.890305818771594e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968934059143066]\n",
      " 3860 [D loss: 1.5166049706749618e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996863603591919]\n",
      " 3861 [D loss: 2.5123104023805354e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968794584274292]\n",
      " 3862 [D loss: 5.257205884845462e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966676235198975]\n",
      " 3863 [D loss: 2.5794956854952034e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967155456542969]\n",
      " 3864 [D loss: 2.721165401453618e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967014789581299]\n",
      " 3865 [D loss: 2.6962379706674255e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969590306282043]\n",
      " 3866 [D loss: 9.482809218752664e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996674656867981]\n",
      " 3867 [D loss: 3.461459527898114e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996789813041687]\n",
      " 3868 [D loss: 2.4516441499145003e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967672228813171]\n",
      " 3869 [D loss: 4.447790161066223e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970301985740662]\n",
      " 3870 [D loss: 1.271506562261493e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967740774154663]\n",
      " 3871 [D loss: 5.1278466344228946e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967073202133179]\n",
      " 3872 [D loss: 2.0255843082850333e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968788027763367]\n",
      " 3873 [D loss: 1.4599590940633789e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967281818389893]\n",
      " 3874 [D loss: 2.2487940896098735e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968323707580566]\n",
      " 3875 [D loss: 2.680610577954212e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970235824584961]\n",
      " 3876 [D loss: 1.5054698451422155e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966980814933777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3877 [D loss: 2.0948400560882874e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968104362487793]\n",
      " 3878 [D loss: 1.3108768143865746e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967266917228699]\n",
      " 3879 [D loss: 1.2160883215983631e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996802568435669]\n",
      " 3880 [D loss: 8.926222108129878e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965721964836121]\n",
      " 3881 [D loss: 2.1519986148632597e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965336918830872]\n",
      " 3882 [D loss: 1.4735800277776434e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969220161437988]\n",
      " 3883 [D loss: 7.134054158086656e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996482789516449]\n",
      " 3884 [D loss: 1.7213948240168975e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966793060302734]\n",
      " 3885 [D loss: 2.2102881302998867e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967555403709412]\n",
      " 3886 [D loss: 4.9902641876542475e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968101978302002]\n",
      " 3887 [D loss: 1.769580762811529e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996685802936554]\n",
      " 3888 [D loss: 2.0748461793118622e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966037273406982]\n",
      " 3889 [D loss: 5.736595994676463e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996806263923645]\n",
      " 3890 [D loss: 1.2960395906702615e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968631863594055]\n",
      " 3891 [D loss: 4.654558779293438e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965897798538208]\n",
      " 3892 [D loss: 3.0674400477437302e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968530535697937]\n",
      " 3893 [D loss: 1.3233519666755456e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966446161270142]\n",
      " 3894 [D loss: 1.1206560657228692e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968515038490295]\n",
      " 3895 [D loss: 1.1511170896483236e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966173768043518]\n",
      " 3896 [D loss: 1.37319766508881e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969168901443481]\n",
      " 3897 [D loss: 1.4559731198460213e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968709945678711]\n",
      " 3898 [D loss: 2.712154810069478e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967688322067261]\n",
      " 3899 [D loss: 1.555129870212113e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968123435974121]\n",
      " 3900 [D loss: 1.1124913726234809e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967046976089478]\n",
      " 3901 [D loss: 1.281975414713088e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967696666717529]\n",
      " 3902 [D loss: 4.086975968675688e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966681003570557]\n",
      " 3903 [D loss: 8.331838216690812e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969692230224609]\n",
      " 3904 [D loss: 3.679446990645374e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967952966690063]\n",
      " 3905 [D loss: 2.7112655516248196e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969698190689087]\n",
      " 3906 [D loss: 1.0865300055229454e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967882633209229]\n",
      " 3907 [D loss: 4.447899300430436e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967242479324341]\n",
      " 3908 [D loss: 2.7581629638007144e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971128702163696]\n",
      " 3909 [D loss: 2.3991356101760175e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969013333320618]\n",
      " 3910 [D loss: 3.290711674708291e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969037771224976]\n",
      " 3911 [D loss: 2.527120159356855e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996911883354187]\n",
      " 3912 [D loss: 1.1921625628019683e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996951699256897]\n",
      " 3913 [D loss: 1.962877831829246e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969342350959778]\n",
      " 3914 [D loss: 1.515255917183822e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969979524612427]\n",
      " 3915 [D loss: 1.1852787338284543e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996903657913208]\n",
      " 3916 [D loss: 1.5543261042694212e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970568418502808]\n",
      " 3917 [D loss: 3.0842884370940737e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996860682964325]\n",
      " 3918 [D loss: 1.1559315680642612e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968420267105103]\n",
      " 3919 [D loss: 4.4389203139871825e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967767000198364]\n",
      " 3920 [D loss: 4.652919415093493e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969164729118347]\n",
      " 3921 [D loss: 1.3168076975489384e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968020915985107]\n",
      " 3922 [D loss: 5.004940248909406e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967107176780701]\n",
      " 3923 [D loss: 2.1867685973120388e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966978430747986]\n",
      " 3924 [D loss: 8.923922905523796e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996802568435669]\n",
      " 3925 [D loss: 3.1678482628194615e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968791007995605]\n",
      " 3926 [D loss: 1.7874242530524498e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968770742416382]\n",
      " 3927 [D loss: 4.530760270426981e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968854188919067]\n",
      " 3928 [D loss: 1.2748239441862097e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968316555023193]\n",
      " 3929 [D loss: 1.6504980067111319e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968991279602051]\n",
      " 3930 [D loss: 6.595463219127851e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968091249465942]\n",
      " 3931 [D loss: 5.141291239851853e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968781471252441]\n",
      " 3932 [D loss: 3.007846316904761e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969966411590576]\n",
      " 3933 [D loss: 7.614406058564782e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969726800918579]\n",
      " 3934 [D loss: 1.5342981214416795e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969435334205627]\n",
      " 3935 [D loss: 1.5623754734406248e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969870448112488]\n",
      " 3936 [D loss: 5.2810742090514395e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968836307525635]\n",
      " 3937 [D loss: 1.4176065405990812e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970070123672485]\n",
      " 3938 [D loss: 3.917729827662697e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969751834869385]\n",
      " 3939 [D loss: 1.1939733667531982e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969478845596313]\n",
      " 3940 [D loss: 2.192628699049237e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969269633293152]\n",
      " 3941 [D loss: 1.2971804608241655e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969500303268433]\n",
      " 3942 [D loss: 1.2996757732253172e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969698786735535]\n",
      " 3943 [D loss: 2.8650138119701296e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996940016746521]\n",
      " 3944 [D loss: 4.513894054980483e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996932864189148]\n",
      " 3945 [D loss: 8.856494559950079e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969937801361084]\n",
      " 3946 [D loss: 1.5971056654962013e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969245195388794]\n",
      " 3947 [D loss: 2.5480603653704748e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970155358314514]\n",
      " 3948 [D loss: 1.1392296528356383e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970961809158325]\n",
      " 3949 [D loss: 9.753357517183758e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969404339790344]\n",
      " 3950 [D loss: 1.065848891812493e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969935417175293]\n",
      " 3951 [D loss: 2.535917474233429e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970086812973022]\n",
      " 3952 [D loss: 4.8008805606514215e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969748854637146]\n",
      " 3953 [D loss: 5.569414497585967e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967453479766846]\n",
      " 3954 [D loss: 1.6422468434029724e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970314502716064]\n",
      " 3955 [D loss: 5.7318329709232785e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967787265777588]\n",
      " 3956 [D loss: 5.594213234871859e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969149827957153]\n",
      " 3957 [D loss: 1.1953280818488565e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967883825302124]\n",
      " 3958 [D loss: 1.2509667612903286e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970703125]\n",
      " 3959 [D loss: 1.7656634554441553e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969049096107483]\n",
      " 3960 [D loss: 1.919268925121287e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968528747558594]\n",
      " 3961 [D loss: 3.3104122394433944e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969689846038818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3962 [D loss: 1.7150481426142505e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969344139099121]\n",
      " 3963 [D loss: 1.1802587778220186e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970819354057312]\n",
      " 3964 [D loss: 1.1736917713278672e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969760179519653]\n",
      " 3965 [D loss: 2.47593152380432e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996810793876648]\n",
      " 3966 [D loss: 1.8651488744581002e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997013509273529]\n",
      " 3967 [D loss: 9.604388651496265e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970327615737915]\n",
      " 3968 [D loss: 1.1302336133667268e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969083070755005]\n",
      " 3969 [D loss: 5.450312073662644e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969258308410645]\n",
      " 3970 [D loss: 2.8517883947642986e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969006180763245]\n",
      " 3971 [D loss: 1.1284806760158972e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969545602798462]\n",
      " 3972 [D loss: 1.926306595123606e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969638586044312]\n",
      " 3973 [D loss: 3.1486865736951586e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967993497848511]\n",
      " 3974 [D loss: 7.338971499848412e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996794581413269]\n",
      " 3975 [D loss: 3.0405476536543574e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968193769454956]\n",
      " 3976 [D loss: 3.0222181521821767e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968643188476562]\n",
      " 3977 [D loss: 1.2077240398866707e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967778921127319]\n",
      " 3978 [D loss: 9.784257599676494e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996857762336731]\n",
      " 3979 [D loss: 3.551190729922382e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966822266578674]\n",
      " 3980 [D loss: 4.603596607921645e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965687990188599]\n",
      " 3981 [D loss: 1.3677731658390258e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996854841709137]\n",
      " 3982 [D loss: 2.4899072741391137e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996878981590271]\n",
      " 3983 [D loss: 1.5080231605679728e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967871308326721]\n",
      " 3984 [D loss: 1.4705979083373677e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967935681343079]\n",
      " 3985 [D loss: 1.7322531675745267e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969348907470703]\n",
      " 3986 [D loss: 4.4957637328479905e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969164133071899]\n",
      " 3987 [D loss: 1.705034628685098e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965923428535461]\n",
      " 3988 [D loss: 5.326709469954949e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968533515930176]\n",
      " 3989 [D loss: 2.9371776690823026e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996773362159729]\n",
      " 3990 [D loss: 1.5548980627499986e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967584609985352]\n",
      " 3991 [D loss: 2.5052060664165765e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967706203460693]\n",
      " 3992 [D loss: 9.612356279831147e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968259930610657]\n",
      " 3993 [D loss: 1.5347754924732726e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964766502380371]\n",
      " 3994 [D loss: 1.094996378014912e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996825098991394]\n",
      " 3995 [D loss: 1.7354810779579566e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969998002052307]\n",
      " 3996 [D loss: 1.2854726492150803e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965410232543945]\n",
      " 3997 [D loss: 1.5410953437822172e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968395829200745]\n",
      " 3998 [D loss: 3.257124717492843e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968273639678955]\n",
      " 3999 [D loss: 2.408017280686181e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967988729476929]\n",
      " 4000 [D loss: 1.4885406471876195e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967389702796936]\n",
      " 4001 [D loss: 8.487795639666729e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968174695968628]\n",
      " 4002 [D loss: 1.8925694575955276e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969508647918701]\n",
      " 4003 [D loss: 2.2913732209417503e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968271851539612]\n",
      " 4004 [D loss: 1.4306098819361068e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99684739112854]\n",
      " 4005 [D loss: 5.688310011464637e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969673752784729]\n",
      " 4006 [D loss: 6.536872660944937e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966609477996826]\n",
      " 4007 [D loss: 2.4140922505466733e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967920780181885]\n",
      " 4008 [D loss: 3.0246008009271463e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967066049575806]\n",
      " 4009 [D loss: 1.6164578937605256e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968082904815674]\n",
      " 4010 [D loss: 1.759288693392591e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969815015792847]\n",
      " 4011 [D loss: 1.8736013771558646e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970088601112366]\n",
      " 4012 [D loss: 1.5034295756777283e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996777355670929]\n",
      " 4013 [D loss: 1.5963680652930634e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968202114105225]\n",
      " 4014 [D loss: 4.920261744700838e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968359470367432]\n",
      " 4015 [D loss: 1.6494855117343832e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969137907028198]\n",
      " 4016 [D loss: 5.118464287079405e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971021413803101]\n",
      " 4017 [D loss: 1.4238946732803015e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968675971031189]\n",
      " 4018 [D loss: 1.4886284134263406e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968835115432739]\n",
      " 4019 [D loss: 1.3139843986209598e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969385862350464]\n",
      " 4020 [D loss: 1.416083364347287e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971809387207031]\n",
      " 4021 [D loss: 1.21736536584649e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970812797546387]\n",
      " 4022 [D loss: 9.54270944930613e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967870116233826]\n",
      " 4023 [D loss: 1.223828803631477e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971311688423157]\n",
      " 4024 [D loss: 1.3049610743109952e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996953547000885]\n",
      " 4025 [D loss: 1.692647856543772e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970502853393555]\n",
      " 4026 [D loss: 1.47900311731064e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969927072525024]\n",
      " 4027 [D loss: 1.553400011289341e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971622228622437]\n",
      " 4028 [D loss: 2.3703573788225185e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968593120574951]\n",
      " 4029 [D loss: 2.379962324994267e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970408082008362]\n",
      " 4030 [D loss: 1.0090532668982632e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970269203186035]\n",
      " 4031 [D loss: 2.8221652428328525e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970299005508423]\n",
      " 4032 [D loss: 2.385315838182578e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969106912612915]\n",
      " 4033 [D loss: 1.1166576996402e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969650506973267]\n",
      " 4034 [D loss: 1.541254732728703e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969315528869629]\n",
      " 4035 [D loss: 6.963524356251583e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969210028648376]\n",
      " 4036 [D loss: 2.4289381599373883e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971035122871399]\n",
      " 4037 [D loss: 1.8782938013828243e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971369504928589]\n",
      " 4038 [D loss: 3.093845225521363e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971823692321777]\n",
      " 4039 [D loss: 1.651649654377252e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972302913665771]\n",
      " 4040 [D loss: 1.8804377077685785e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971487522125244]\n",
      " 4041 [D loss: 1.3131427749613067e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971495270729065]\n",
      " 4042 [D loss: 1.218259740198846e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971567392349243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4043 [D loss: 1.938096147569013e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970887303352356]\n",
      " 4044 [D loss: 1.4512173720504506e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972600936889648]\n",
      " 4045 [D loss: 5.049888841313077e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970698356628418]\n",
      " 4046 [D loss: 1.9300332496641204e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996941864490509]\n",
      " 4047 [D loss: 5.538337063626386e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970909357070923]\n",
      " 4048 [D loss: 1.6867970771272667e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970791935920715]\n",
      " 4049 [D loss: 1.3468417137119104e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971058368682861]\n",
      " 4050 [D loss: 1.3785104329144815e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970716238021851]\n",
      " 4051 [D loss: 6.163487796584377e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970710873603821]\n",
      " 4052 [D loss: 3.136520717816893e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970383644104004]\n",
      " 4053 [D loss: 1.869875177362701e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970492124557495]\n",
      " 4054 [D loss: 9.487566217103449e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969322681427002]\n",
      " 4055 [D loss: 3.51999210579379e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971072673797607]\n",
      " 4056 [D loss: 9.38516848236759e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970263242721558]\n",
      " 4057 [D loss: 1.6842120658111526e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970073699951172]\n",
      " 4058 [D loss: 2.384962044743588e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996793270111084]\n",
      " 4059 [D loss: 6.6353004513075575e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968525171279907]\n",
      " 4060 [D loss: 1.9537533262337092e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970296621322632]\n",
      " 4061 [D loss: 7.300565812329296e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971036314964294]\n",
      " 4062 [D loss: 1.7249533357244218e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970382452011108]\n",
      " 4063 [D loss: 2.9212158096925123e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971235990524292]\n",
      " 4064 [D loss: 2.7741866688302252e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969058036804199]\n",
      " 4065 [D loss: 1.0797229151648935e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970670938491821]\n",
      " 4066 [D loss: 1.3154431144357659e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972143769264221]\n",
      " 4067 [D loss: 7.721642759861425e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971369504928589]\n",
      " 4068 [D loss: 2.684037326616817e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969536662101746]\n",
      " 4069 [D loss: 4.652146344596986e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968881607055664]\n",
      " 4070 [D loss: 5.8951177379640285e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970723390579224]\n",
      " 4071 [D loss: 1.2291465054659056e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971252679824829]\n",
      " 4072 [D loss: 1.1374213499948382e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9973024129867554]\n",
      " 4073 [D loss: 3.1514930469711544e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968686699867249]\n",
      " 4074 [D loss: 4.126379280933179e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969569444656372]\n",
      " 4075 [D loss: 1.1106926649517845e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971566796302795]\n",
      " 4076 [D loss: 1.1496977094793692e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971727132797241]\n",
      " 4077 [D loss: 1.1602422773648868e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971187114715576]\n",
      " 4078 [D loss: 1.4764113984711003e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971538782119751]\n",
      " 4079 [D loss: 1.313195298280334e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967513084411621]\n",
      " 4080 [D loss: 2.307818476765533e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971654415130615]\n",
      " 4081 [D loss: 1.4736622233613161e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971478581428528]\n",
      " 4082 [D loss: 1.3836134939992917e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997043251991272]\n",
      " 4083 [D loss: 7.09985033608973e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969387054443359]\n",
      " 4084 [D loss: 1.0336730156268459e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971323013305664]\n",
      " 4085 [D loss: 2.6391708161099814e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969514012336731]\n",
      " 4086 [D loss: 3.371795855855453e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996992826461792]\n",
      " 4087 [D loss: 3.1313973067881307e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968608617782593]\n",
      " 4088 [D loss: 1.620784132683184e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970126748085022]\n",
      " 4089 [D loss: 1.5096061360964086e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969452023506165]\n",
      " 4090 [D loss: 1.3223752830526792e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970200657844543]\n",
      " 4091 [D loss: 1.8013375893133343e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970408082008362]\n",
      " 4092 [D loss: 1.3220064829511102e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969428777694702]\n",
      " 4093 [D loss: 2.2954316136747366e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970579147338867]\n",
      " 4094 [D loss: 6.739917353115743e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970830678939819]\n",
      " 4095 [D loss: 1.3045769264863338e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966016411781311]\n",
      " 4096 [D loss: 1.0143005965801422e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969735145568848]\n",
      " 4097 [D loss: 2.308054035893292e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967429041862488]\n",
      " 4098 [D loss: 1.8845913700715755e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967915415763855]\n",
      " 4099 [D loss: 2.768903186733951e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968986511230469]\n",
      " 4100 [D loss: 1.4645899000242935e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969196319580078]\n",
      " 4101 [D loss: 1.2309927797105047e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967638254165649]\n",
      " 4102 [D loss: 1.0111691153724678e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967823028564453]\n",
      " 4103 [D loss: 3.35825916408794e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968065023422241]\n",
      " 4104 [D loss: 1.062244564309367e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970196485519409]\n",
      " 4105 [D loss: 1.333319119112275e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969769716262817]\n",
      " 4106 [D loss: 1.8347393051953986e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970489740371704]\n",
      " 4107 [D loss: 1.5012683434179053e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969059228897095]\n",
      " 4108 [D loss: 1.3664749758390826e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970412254333496]\n",
      " 4109 [D loss: 6.532286533911247e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970947504043579]\n",
      " 4110 [D loss: 1.2547147889563348e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996936559677124]\n",
      " 4111 [D loss: 1.3302235402079532e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970533847808838]\n",
      " 4112 [D loss: 4.943707608617842e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969003200531006]\n",
      " 4113 [D loss: 9.767211395228514e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968730211257935]\n",
      " 4114 [D loss: 1.4406439277081518e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969478845596313]\n",
      " 4115 [D loss: 7.1081885835155845e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970700740814209]\n",
      " 4116 [D loss: 1.5831236623853329e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997019350528717]\n",
      " 4117 [D loss: 2.2906710910319816e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969461560249329]\n",
      " 4118 [D loss: 2.1181333522690693e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969979524612427]\n",
      " 4119 [D loss: 1.4137158359517343e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971020221710205]\n",
      " 4120 [D loss: 1.085465555661358e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970561265945435]\n",
      " 4121 [D loss: 1.67251710081473e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972009658813477]\n",
      " 4122 [D loss: 2.997326419063029e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971460103988647]\n",
      " 4123 [D loss: 1.2913656064483803e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969546794891357]\n",
      " 4124 [D loss: 1.1763579550461145e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971444606781006]\n",
      " 4125 [D loss: 1.5464712532775593e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971851110458374]\n",
      " 4126 [D loss: 2.999851403728826e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971287250518799]\n",
      " 4127 [D loss: 1.3876615412300453e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971514344215393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4128 [D loss: 1.0580525668046903e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970159530639648]\n",
      " 4129 [D loss: 1.940588845172897e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970457553863525]\n",
      " 4130 [D loss: 4.052005806443049e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967960119247437]\n",
      " 4131 [D loss: 9.99315830085834e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968949556350708]\n",
      " 4132 [D loss: 2.8650015337916557e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969884157180786]\n",
      " 4133 [D loss: 1.375228066535783e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969756007194519]\n",
      " 4134 [D loss: 1.061169655258709e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996901273727417]\n",
      " 4135 [D loss: 6.65342577121919e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971209764480591]\n",
      " 4136 [D loss: 3.6924800497217802e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969668984413147]\n",
      " 4137 [D loss: 2.514993639124441e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970330595970154]\n",
      " 4138 [D loss: 2.5960172933992e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972085952758789]\n",
      " 4139 [D loss: 1.1071729204559233e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970108866691589]\n",
      " 4140 [D loss: 2.389279870840255e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970342516899109]\n",
      " 4141 [D loss: 1.931684437295189e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972078800201416]\n",
      " 4142 [D loss: 1.652595415180258e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970930218696594]\n",
      " 4143 [D loss: 4.490098035603296e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968929290771484]\n",
      " 4144 [D loss: 2.4986325115605723e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968496561050415]\n",
      " 4145 [D loss: 5.008078915125225e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970058798789978]\n",
      " 4146 [D loss: 2.3309601601795293e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997003436088562]\n",
      " 4147 [D loss: 1.2147334018663969e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996889591217041]\n",
      " 4148 [D loss: 1.4711531548528e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969404935836792]\n",
      " 4149 [D loss: 4.052610165672377e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967555403709412]\n",
      " 4150 [D loss: 2.3945103748701513e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971083998680115]\n",
      " 4151 [D loss: 7.4735858106578235e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968613982200623]\n",
      " 4152 [D loss: 5.049988249083981e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969515800476074]\n",
      " 4153 [D loss: 2.093325974783511e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970493316650391]\n",
      " 4154 [D loss: 1.876668193290243e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972015619277954]\n",
      " 4155 [D loss: 8.276748303615022e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972829818725586]\n",
      " 4156 [D loss: 1.744508153933566e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997326135635376]\n",
      " 4157 [D loss: 1.1175329746038187e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971663355827332]\n",
      " 4158 [D loss: 1.397898586219526e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971825480461121]\n",
      " 4159 [D loss: 1.893449189083185e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972625970840454]\n",
      " 4160 [D loss: 2.3447862531611463e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972389340400696]\n",
      " 4161 [D loss: 1.3966543519927654e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971683025360107]\n",
      " 4162 [D loss: 1.4062723039387492e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971624612808228]\n",
      " 4163 [D loss: 1.685134861872939e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970324039459229]\n",
      " 4164 [D loss: 1.2431132745405193e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969569444656372]\n",
      " 4165 [D loss: 4.2859046516241506e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970426559448242]\n",
      " 4166 [D loss: 1.3587448393082013e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971436262130737]\n",
      " 4167 [D loss: 1.629857592888584e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970531463623047]\n",
      " 4168 [D loss: 1.2743892057187622e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970095157623291]\n",
      " 4169 [D loss: 1.5707711327195284e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996995210647583]\n",
      " 4170 [D loss: 2.1110301531734876e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969899654388428]\n",
      " 4171 [D loss: 1.7611973817110993e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969731569290161]\n",
      " 4172 [D loss: 4.741194516100222e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969683885574341]\n",
      " 4173 [D loss: 2.2948154310142854e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996907114982605]\n",
      " 4174 [D loss: 1.0795016578413197e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968523383140564]\n",
      " 4175 [D loss: 9.490057664152118e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968356490135193]\n",
      " 4176 [D loss: 2.467325657562469e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968883991241455]\n",
      " 4177 [D loss: 6.718884833389893e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969791173934937]\n",
      " 4178 [D loss: 2.473668700986309e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966562986373901]\n",
      " 4179 [D loss: 2.47165576183761e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970632791519165]\n",
      " 4180 [D loss: 1.5421520629388397e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970541596412659]\n",
      " 4181 [D loss: 2.929424044850748e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997037947177887]\n",
      " 4182 [D loss: 1.2123732631152961e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970972537994385]\n",
      " 4183 [D loss: 9.280927315558074e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971093535423279]\n",
      " 4184 [D loss: 2.1166920305404346e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969538450241089]\n",
      " 4185 [D loss: 1.9706349121406674e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971569776535034]\n",
      " 4186 [D loss: 1.2310023294048733e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971076846122742]\n",
      " 4187 [D loss: 3.752574684767751e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997126579284668]\n",
      " 4188 [D loss: 1.2976155403521261e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969679117202759]\n",
      " 4189 [D loss: 1.269968606720795e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968174695968628]\n",
      " 4190 [D loss: 4.207205620332388e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968961477279663]\n",
      " 4191 [D loss: 2.4895034584915265e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970040917396545]\n",
      " 4192 [D loss: 1.6004424878701684e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968259930610657]\n",
      " 4193 [D loss: 1.8739877987172804e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968743324279785]\n",
      " 4194 [D loss: 2.752570708253188e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968079924583435]\n",
      " 4195 [D loss: 1.8017574348050402e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969669580459595]\n",
      " 4196 [D loss: 1.6793593431430054e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970616698265076]\n",
      " 4197 [D loss: 1.1079948762926506e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968867897987366]\n",
      " 4198 [D loss: 1.2392349617584841e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969164729118347]\n",
      " 4199 [D loss: 3.4037848308798857e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969903230667114]\n",
      " 4200 [D loss: 1.0724988896981813e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969792366027832]\n",
      " 4201 [D loss: 1.4059287423151545e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969698190689087]\n",
      " 4202 [D loss: 1.5001231759015354e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969421625137329]\n",
      " 4203 [D loss: 2.5732792892085854e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968174695968628]\n",
      " 4204 [D loss: 1.7108201291193836e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970382452011108]\n",
      " 4205 [D loss: 1.6449060922241188e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968831539154053]\n",
      " 4206 [D loss: 7.192420980572933e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965224266052246]\n",
      " 4207 [D loss: 2.2973392788117053e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969918727874756]\n",
      " 4208 [D loss: 1.2359892934910022e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969041347503662]\n",
      " 4209 [D loss: 2.0475824840104906e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968976378440857]\n",
      " 4210 [D loss: 1.6799764352981583e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968451857566833]\n",
      " 4211 [D loss: 1.5846508176764473e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968937635421753]\n",
      " 4212 [D loss: 5.268403128866339e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970930814743042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4213 [D loss: 1.497880930401152e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971281886100769]\n",
      " 4214 [D loss: 2.3301436158362776e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971829652786255]\n",
      " 4215 [D loss: 1.067660491571587e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969847798347473]\n",
      " 4216 [D loss: 1.3029048204771243e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971303939819336]\n",
      " 4217 [D loss: 4.824390543944901e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971319437026978]\n",
      " 4218 [D loss: 1.2697487363766413e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971486330032349]\n",
      " 4219 [D loss: 1.0394824130344205e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970108270645142]\n",
      " 4220 [D loss: 1.8176212961407145e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971833825111389]\n",
      " 4221 [D loss: 2.8231618216523202e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971974492073059]\n",
      " 4222 [D loss: 1.2760056051774882e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970556497573853]\n",
      " 4223 [D loss: 3.97813073504949e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971151351928711]\n",
      " 4224 [D loss: 9.51839865592774e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970229864120483]\n",
      " 4225 [D loss: 1.276186594623141e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969366788864136]\n",
      " 4226 [D loss: 1.5149551018112106e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969443678855896]\n",
      " 4227 [D loss: 1.293592617912509e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970114231109619]\n",
      " 4228 [D loss: 1.0673567203411949e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970735311508179]\n",
      " 4229 [D loss: 4.180778887530323e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970105886459351]\n",
      " 4230 [D loss: 2.6720381356426515e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997032642364502]\n",
      " 4231 [D loss: 1.305041564592102e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969872236251831]\n",
      " 4232 [D loss: 4.5830274757463485e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970318675041199]\n",
      " 4233 [D loss: 7.47923195376643e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971117377281189]\n",
      " 4234 [D loss: 1.679339902693755e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970599412918091]\n",
      " 4235 [D loss: 1.184768393613922e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970203638076782]\n",
      " 4236 [D loss: 1.9889303075615317e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968593120574951]\n",
      " 4237 [D loss: 3.7360443911893526e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970206022262573]\n",
      " 4238 [D loss: 1.0829343182194862e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971715807914734]\n",
      " 4239 [D loss: 4.3018462747568265e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971519708633423]\n",
      " 4240 [D loss: 1.2989522701900569e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971673488616943]\n",
      " 4241 [D loss: 3.879948053508997e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969316124916077]\n",
      " 4242 [D loss: 1.8795454934661393e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970696568489075]\n",
      " 4243 [D loss: 1.329329279542435e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970473051071167]\n",
      " 4244 [D loss: 1.1433133977334364e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969939589500427]\n",
      " 4245 [D loss: 1.2710643204627559e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970993995666504]\n",
      " 4246 [D loss: 1.5433548696819344e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971073269844055]\n",
      " 4247 [D loss: 1.510371703261626e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969199895858765]\n",
      " 4248 [D loss: 9.830212093220325e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997025191783905]\n",
      " 4249 [D loss: 1.3134020946381497e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971113204956055]\n",
      " 4250 [D loss: 1.1226235983485822e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970005750656128]\n",
      " 4251 [D loss: 3.687350954351132e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.997132420539856]\n",
      " 4252 [D loss: 1.1552526757441228e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99686598777771]\n",
      " 4253 [D loss: 1.8231844478577841e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970147609710693]\n",
      " 4254 [D loss: 1.838053208302881e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971144199371338]\n",
      " 4255 [D loss: 4.896148311672732e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970628023147583]\n",
      " 4256 [D loss: 1.0167384061787743e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969028234481812]\n",
      " 4257 [D loss: 2.1927041871094843e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971551895141602]\n",
      " 4258 [D loss: 3.2380285119870678e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972010254859924]\n",
      " 4259 [D loss: 1.8880078869187855e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971598386764526]\n",
      " 4260 [D loss: 9.907965250022244e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996956467628479]\n",
      " 4261 [D loss: 1.222154082825e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971803426742554]\n",
      " 4262 [D loss: 1.449342789783259e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970203638076782]\n",
      " 4263 [D loss: 1.5979787804099033e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971653819084167]\n",
      " 4264 [D loss: 6.151275556476321e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970982670783997]\n",
      " 4265 [D loss: 1.5454359072464285e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972213506698608]\n",
      " 4266 [D loss: 2.780473323582555e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970988035202026]\n",
      " 4267 [D loss: 2.821929001584067e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969887733459473]\n",
      " 4268 [D loss: 6.441971436288441e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972259998321533]\n",
      " 4269 [D loss: 1.2058342690579593e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971518516540527]\n",
      " 4270 [D loss: 1.0109813956660219e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971119165420532]\n",
      " 4271 [D loss: 2.1805510641570436e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972773790359497]\n",
      " 4272 [D loss: 1.6469002730445936e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996996283531189]\n",
      " 4273 [D loss: 2.0570973902067635e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971121549606323]\n",
      " 4274 [D loss: 1.4853026186756324e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971291422843933]\n",
      " 4275 [D loss: 1.137018534791423e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972660541534424]\n",
      " 4276 [D loss: 2.545720235502813e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970243573188782]\n",
      " 4277 [D loss: 2.3170592612586915e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970321655273438]\n",
      " 4278 [D loss: 9.858623570835334e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971357583999634]\n",
      " 4279 [D loss: 4.092023118573707e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970114231109619]\n",
      " 4280 [D loss: 1.4131277339402004e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968862533569336]\n",
      " 4281 [D loss: 7.730226570856757e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970739483833313]\n",
      " 4282 [D loss: 1.3379096799326362e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969894289970398]\n",
      " 4283 [D loss: 1.9415520000620745e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996978223323822]\n",
      " 4284 [D loss: 6.259542260522721e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969173669815063]\n",
      " 4285 [D loss: 5.009872984373942e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970235824584961]\n",
      " 4286 [D loss: 2.354166554141557e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971780776977539]\n",
      " 4287 [D loss: 2.5746187020558864e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953939914703369]\n",
      " 4288 [D loss: 0.0009146988741122186, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938172698020935]\n",
      " 4289 [D loss: 0.00014043337432667613, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945772886276245]\n",
      " 4290 [D loss: 7.014975381025579e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9931889772415161]\n",
      " 4291 [D loss: 5.974552550469525e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942599534988403]\n",
      " 4292 [D loss: 2.6794964469445404e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945794939994812]\n",
      " 4293 [D loss: 4.733556124847382e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941468238830566]\n",
      " 4294 [D loss: 0.00010181147808907554, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941250085830688]\n",
      " 4295 [D loss: 1.5522213288932107e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935271739959717]\n",
      " 4296 [D loss: 1.8032749721896835e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935922622680664]\n",
      " 4297 [D loss: 1.3299348211148754e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935334920883179]\n",
      " 4298 [D loss: 6.652380761806853e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938439726829529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4299 [D loss: 2.196637069573626e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9926837682723999]\n",
      " 4300 [D loss: 1.2301897186262067e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935890436172485]\n",
      " 4301 [D loss: 2.0521854821708985e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9931961297988892]\n",
      " 4302 [D loss: 7.019848453637678e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938724040985107]\n",
      " 4303 [D loss: 3.371136699570343e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99320387840271]\n",
      " 4304 [D loss: 1.3652186680701561e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993801474571228]\n",
      " 4305 [D loss: 6.686384949716739e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938324689865112]\n",
      " 4306 [D loss: 0.00014229420048650354, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9931706190109253]\n",
      " 4307 [D loss: 1.394395985698793e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9928253889083862]\n",
      " 4308 [D loss: 4.527522378339199e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9934316873550415]\n",
      " 4309 [D loss: 6.161807505122852e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933125972747803]\n",
      " 4310 [D loss: 7.010543049545959e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9921640157699585]\n",
      " 4311 [D loss: 2.7010913981939666e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99314945936203]\n",
      " 4312 [D loss: 1.0514306268305518e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9918055534362793]\n",
      " 4313 [D loss: 1.7251435565412976e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9931615591049194]\n",
      " 4314 [D loss: 3.0261797292041592e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.992581844329834]\n",
      " 4315 [D loss: 7.579554949188605e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.992475688457489]\n",
      " 4316 [D loss: 1.323381729889661e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9928641319274902]\n",
      " 4317 [D loss: 1.3868030691810418e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9928221702575684]\n",
      " 4318 [D loss: 0.00042524017044343054, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9932053089141846]\n",
      " 4319 [D loss: 2.0314972061896697e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938472509384155]\n",
      " 4320 [D loss: 1.895841160148848e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949573278427124]\n",
      " 4321 [D loss: 5.4132860896061175e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949244856834412]\n",
      " 4322 [D loss: 7.538237696280703e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951798915863037]\n",
      " 4323 [D loss: 0.0010835406137630343, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993938684463501]\n",
      " 4324 [D loss: 5.340010829968378e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9924919605255127]\n",
      " 4325 [D loss: 7.377563451882452e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9896658658981323]\n",
      " 4326 [D loss: 0.0004514497995842248, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9896230101585388]\n",
      " 4327 [D loss: 1.2704129403573461e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9897036552429199]\n",
      " 4328 [D loss: 3.0417906600632705e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9916262626647949]\n",
      " 4329 [D loss: 1.1061024451919366e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9910755753517151]\n",
      " 4330 [D loss: 1.4827371160208713e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9904863834381104]\n",
      " 4331 [D loss: 8.854906627675518e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9914892911911011]\n",
      " 4332 [D loss: 7.3411820267210715e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9901366829872131]\n",
      " 4333 [D loss: 8.401657396461815e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9915271401405334]\n",
      " 4334 [D loss: 9.931382373906672e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9907753467559814]\n",
      " 4335 [D loss: 0.00011157723929500207, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9920376539230347]\n",
      " 4336 [D loss: 0.0014449121663346887, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946695566177368]\n",
      " 4337 [D loss: 3.1763408969709417e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962048530578613]\n",
      " 4338 [D loss: 1.2913563978145248e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971171617507935]\n",
      " 4339 [D loss: 2.919246753663174e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9972155094146729]\n",
      " 4340 [D loss: 4.276854815543629e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9976788759231567]\n",
      " 4341 [D loss: 7.429668471559125e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9980140924453735]\n",
      " 4342 [D loss: 9.296617236032034e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9981991648674011]\n",
      " 4343 [D loss: 7.448131214005116e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9980971217155457]\n",
      " 4344 [D loss: 9.66299467108911e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99812251329422]\n",
      " 4345 [D loss: 1.5700776430094265e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9982289671897888]\n",
      " 4346 [D loss: 0.01838071271777153, acc_real: 95.312500, acc_fake: 100.000000] [G loss: 0.9942308664321899]\n",
      " 4347 [D loss: 0.0005412007449194789, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9731960892677307]\n",
      " 4348 [D loss: 0.0015676032053306699, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9369669556617737]\n",
      " 4349 [D loss: 0.01111224852502346, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9286611080169678]\n",
      " 4350 [D loss: 0.028210720047354698, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.9255474805831909]\n",
      " 4351 [D loss: 0.00740185659378767, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9544662237167358]\n",
      " 4352 [D loss: 0.0010917815379798412, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9806573390960693]\n",
      " 4353 [D loss: 0.0005213016411289573, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9912643432617188]\n",
      " 4354 [D loss: 1.0503234079806134e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946019649505615]\n",
      " 4355 [D loss: 4.689207344199531e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959220886230469]\n",
      " 4356 [D loss: 5.396916549216257e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970885515213013]\n",
      " 4357 [D loss: 0.004307580646127462, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9966822862625122]\n",
      " 4358 [D loss: 1.1787054972955957e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969185590744019]\n",
      " 4359 [D loss: 5.108611730975099e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970566034317017]\n",
      " 4360 [D loss: 5.332217369868886e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971079230308533]\n",
      " 4361 [D loss: 1.8691675904847216e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964224100112915]\n",
      " 4362 [D loss: 2.6556131160759833e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970061779022217]\n",
      " 4363 [D loss: 4.194662324152887e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965798854827881]\n",
      " 4364 [D loss: 5.25763061887119e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9968516826629639]\n",
      " 4365 [D loss: 3.631483423305326e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963989853858948]\n",
      " 4366 [D loss: 2.434800990158692e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970552325248718]\n",
      " 4367 [D loss: 1.5554560377495363e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964790940284729]\n",
      " 4368 [D loss: 2.6276536573277554e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962736368179321]\n",
      " 4369 [D loss: 5.553920345846564e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967123866081238]\n",
      " 4370 [D loss: 7.808134796505328e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964370727539062]\n",
      " 4371 [D loss: 2.5824142539931927e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965325593948364]\n",
      " 4372 [D loss: 3.865123289870098e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965512752532959]\n",
      " 4373 [D loss: 8.075305231614038e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962301254272461]\n",
      " 4374 [D loss: 6.46097214485053e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962741136550903]\n",
      " 4375 [D loss: 6.8137655944155995e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960597157478333]\n",
      " 4376 [D loss: 7.631134394614492e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965753555297852]\n",
      " 4377 [D loss: 4.092177277925657e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965378046035767]\n",
      " 4378 [D loss: 6.766802471247502e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958118200302124]\n",
      " 4379 [D loss: 5.875257102161413e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965417981147766]\n",
      " 4380 [D loss: 6.500539711851161e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962136745452881]\n",
      " 4381 [D loss: 2.4249563921330264e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964220523834229]\n",
      " 4382 [D loss: 3.333226686663693e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996757984161377]\n",
      " 4383 [D loss: 5.646799309033668e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959139823913574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4384 [D loss: 2.896943442465272e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963521957397461]\n",
      " 4385 [D loss: 2.3067459551384673e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965470433235168]\n",
      " 4386 [D loss: 4.224308213451877e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969984292984009]\n",
      " 4387 [D loss: 4.3693680709111504e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959499835968018]\n",
      " 4388 [D loss: 2.472770574968308e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961036443710327]\n",
      " 4389 [D loss: 1.5099138181540184e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996310293674469]\n",
      " 4390 [D loss: 5.327397957444191e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996340274810791]\n",
      " 4391 [D loss: 3.859666321659461e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996639609336853]\n",
      " 4392 [D loss: 4.571847966872156e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960671663284302]\n",
      " 4393 [D loss: 4.2021574699901976e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959845542907715]\n",
      " 4394 [D loss: 6.305019724095473e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963014125823975]\n",
      " 4395 [D loss: 5.2714608500537e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959505200386047]\n",
      " 4396 [D loss: 3.2589086913503706e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962431192398071]\n",
      " 4397 [D loss: 5.143822363606887e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957134127616882]\n",
      " 4398 [D loss: 6.032084456819575e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959172606468201]\n",
      " 4399 [D loss: 3.422211648285156e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961885809898376]\n",
      " 4400 [D loss: 3.355231456225738e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960994720458984]\n",
      " 4401 [D loss: 5.64631818633643e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995701789855957]\n",
      " 4402 [D loss: 5.208999027672689e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958454370498657]\n",
      " 4403 [D loss: 4.472430191526655e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9962551593780518]\n",
      " 4404 [D loss: 5.24189226780436e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961026906967163]\n",
      " 4405 [D loss: 5.406867785495706e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966480731964111]\n",
      " 4406 [D loss: 3.8548769225599244e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955736398696899]\n",
      " 4407 [D loss: 5.331303327693604e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996005654335022]\n",
      " 4408 [D loss: 3.3912533581315074e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958059787750244]\n",
      " 4409 [D loss: 3.980374458478764e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964265823364258]\n",
      " 4410 [D loss: 7.3669398261699826e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963605403900146]\n",
      " 4411 [D loss: 4.883414021605859e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9956763982772827]\n",
      " 4412 [D loss: 5.7275028666481376e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958871603012085]\n",
      " 4413 [D loss: 3.6388958051247755e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966170191764832]\n",
      " 4414 [D loss: 4.385562533570919e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959928393363953]\n",
      " 4415 [D loss: 5.5643668019911274e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9961836934089661]\n",
      " 4416 [D loss: 6.526378001581179e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996110200881958]\n",
      " 4417 [D loss: 9.716835847939365e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963263273239136]\n",
      " 4418 [D loss: 7.390806786133908e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995816171169281]\n",
      " 4419 [D loss: 5.137684638611972e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955987334251404]\n",
      " 4420 [D loss: 6.811224920966197e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963216185569763]\n",
      " 4421 [D loss: 6.382772880897392e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959481954574585]\n",
      " 4422 [D loss: 5.571835572482087e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9964718818664551]\n",
      " 4423 [D loss: 0.002618353581055999, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9952512383460999]\n",
      " 4424 [D loss: 7.705656571488362e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947173595428467]\n",
      " 4425 [D loss: 1.0072450095321983e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9925726056098938]\n",
      " 4426 [D loss: 1.0769754226203077e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9913594722747803]\n",
      " 4427 [D loss: 2.288171526743099e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9878147840499878]\n",
      " 4428 [D loss: 2.251045953016728e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9864292144775391]\n",
      " 4429 [D loss: 4.4889035052619874e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9839364886283875]\n",
      " 4430 [D loss: 0.001106292475014925, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9816359281539917]\n",
      " 4431 [D loss: 0.00021079275757074356, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.988193929195404]\n",
      " 4432 [D loss: 1.395633262291085e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9905523061752319]\n",
      " 4433 [D loss: 9.986942313844338e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.991714358329773]\n",
      " 4434 [D loss: 9.916666385834105e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941934943199158]\n",
      " 4435 [D loss: 7.752792043902446e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9929314851760864]\n",
      " 4436 [D loss: 1.2735757991322316e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938657283782959]\n",
      " 4437 [D loss: 8.289764082292095e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9939768314361572]\n",
      " 4438 [D loss: 3.7511672417167574e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955350756645203]\n",
      " 4439 [D loss: 7.2125030783354305e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945283532142639]\n",
      " 4440 [D loss: 5.664609943778487e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952964782714844]\n",
      " 4441 [D loss: 5.2002387747052126e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950706958770752]\n",
      " 4442 [D loss: 6.910880983923562e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954463243484497]\n",
      " 4443 [D loss: 6.644624590990134e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954665899276733]\n",
      " 4444 [D loss: 8.900080501916818e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9956917762756348]\n",
      " 4445 [D loss: 2.7104399578092853e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948528409004211]\n",
      " 4446 [D loss: 5.168225925444858e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954221844673157]\n",
      " 4447 [D loss: 5.321557637216756e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953192472457886]\n",
      " 4448 [D loss: 5.560816589422757e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957413077354431]\n",
      " 4449 [D loss: 6.713822585879825e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958605766296387]\n",
      " 4450 [D loss: 2.323917669855291e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953385591506958]\n",
      " 4451 [D loss: 5.502995009010192e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951328635215759]\n",
      " 4452 [D loss: 4.2637516344257165e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957989454269409]\n",
      " 4453 [D loss: 3.4162892461608863e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955217838287354]\n",
      " 4454 [D loss: 4.881628683506278e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954668283462524]\n",
      " 4455 [D loss: 5.631187832477735e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953017234802246]\n",
      " 4456 [D loss: 6.474418569268892e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9956023097038269]\n",
      " 4457 [D loss: 5.195885933062527e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996006429195404]\n",
      " 4458 [D loss: 5.66347262065392e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949632883071899]\n",
      " 4459 [D loss: 7.413647836074233e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951974749565125]\n",
      " 4460 [D loss: 3.705479684867896e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954702854156494]\n",
      " 4461 [D loss: 5.889154181204503e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954341650009155]\n",
      " 4462 [D loss: 5.992096703266725e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995465874671936]\n",
      " 4463 [D loss: 2.8221545562701067e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995211124420166]\n",
      " 4464 [D loss: 4.3617615119728725e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949373006820679]\n",
      " 4465 [D loss: 6.735094757459592e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957485198974609]\n",
      " 4466 [D loss: 6.96442612024839e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955163598060608]\n",
      " 4467 [D loss: 4.59194416180253e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955151081085205]\n",
      " 4468 [D loss: 4.988675755157601e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954227209091187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4469 [D loss: 3.893509074259782e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995168924331665]\n",
      " 4470 [D loss: 8.090606570476666e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952065944671631]\n",
      " 4471 [D loss: 4.438069481693674e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950734376907349]\n",
      " 4472 [D loss: 3.6061039736523526e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951307773590088]\n",
      " 4473 [D loss: 5.210811650613323e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954425096511841]\n",
      " 4474 [D loss: 3.8093116927484516e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994787335395813]\n",
      " 4475 [D loss: 8.271358638012316e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955031275749207]\n",
      " 4476 [D loss: 5.071283339930233e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951142072677612]\n",
      " 4477 [D loss: 3.35917479787895e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955583214759827]\n",
      " 4478 [D loss: 6.645755547651788e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950834512710571]\n",
      " 4479 [D loss: 5.370879989641253e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958502054214478]\n",
      " 4480 [D loss: 5.146935563971056e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952021837234497]\n",
      " 4481 [D loss: 4.034874109493103e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952414035797119]\n",
      " 4482 [D loss: 3.709474185598083e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9958731532096863]\n",
      " 4483 [D loss: 5.331573902367381e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955475330352783]\n",
      " 4484 [D loss: 6.106637556513306e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954966306686401]\n",
      " 4485 [D loss: 6.080710591049865e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994512140750885]\n",
      " 4486 [D loss: 8.053116289374884e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952793121337891]\n",
      " 4487 [D loss: 5.001804311177693e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949821829795837]\n",
      " 4488 [D loss: 3.0811195301794214e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952356815338135]\n",
      " 4489 [D loss: 9.138859240920283e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953478574752808]\n",
      " 4490 [D loss: 6.68086568111903e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955476522445679]\n",
      " 4491 [D loss: 7.2818547778297216e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948083162307739]\n",
      " 4492 [D loss: 5.249637979432009e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995266854763031]\n",
      " 4493 [D loss: 8.079024155449588e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957559108734131]\n",
      " 4494 [D loss: 5.387845703808125e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950008392333984]\n",
      " 4495 [D loss: 5.9958315432595555e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949535131454468]\n",
      " 4496 [D loss: 8.736400559428148e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952565431594849]\n",
      " 4497 [D loss: 3.900603587680962e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955592155456543]\n",
      " 4498 [D loss: 4.757185251946794e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948292970657349]\n",
      " 4499 [D loss: 1.058530051523121e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953185319900513]\n",
      " 4500 [D loss: 6.386988388840109e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951920509338379]\n",
      " 4501 [D loss: 8.082278327492531e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951869249343872]\n",
      " 4502 [D loss: 3.4258064260939136e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949432611465454]\n",
      " 4503 [D loss: 5.956617769697914e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953231811523438]\n",
      " 4504 [D loss: 4.648965841624886e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949015974998474]\n",
      " 4505 [D loss: 3.018761162820738e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952532052993774]\n",
      " 4506 [D loss: 3.8279690670606215e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951419830322266]\n",
      " 4507 [D loss: 4.9000122999132145e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9956638813018799]\n",
      " 4508 [D loss: 1.551063724036794e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951274394989014]\n",
      " 4509 [D loss: 4.508527126745321e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954741597175598]\n",
      " 4510 [D loss: 9.9727849374176e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995182991027832]\n",
      " 4511 [D loss: 8.57636405271478e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954425692558289]\n",
      " 4512 [D loss: 1.8312694010091946e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949581623077393]\n",
      " 4513 [D loss: 1.3681019481737167e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950528740882874]\n",
      " 4514 [D loss: 6.827188371971715e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940979480743408]\n",
      " 4515 [D loss: 7.914290108601563e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949226379394531]\n",
      " 4516 [D loss: 8.207456630771048e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947474002838135]\n",
      " 4517 [D loss: 3.6850731248705415e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953088760375977]\n",
      " 4518 [D loss: 5.201726253289962e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950342178344727]\n",
      " 4519 [D loss: 1.0214124813501257e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9953193664550781]\n",
      " 4520 [D loss: 5.468634753924562e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951471090316772]\n",
      " 4521 [D loss: 1.0536152331042103e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947679042816162]\n",
      " 4522 [D loss: 1.0604371709632687e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946664571762085]\n",
      " 4523 [D loss: 5.094172138342401e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952647089958191]\n",
      " 4524 [D loss: 5.848461114510428e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949512481689453]\n",
      " 4525 [D loss: 6.357819074764848e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957847595214844]\n",
      " 4526 [D loss: 5.09753181177075e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942516088485718]\n",
      " 4527 [D loss: 3.68853511645284e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994778573513031]\n",
      " 4528 [D loss: 7.742111847619526e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947880506515503]\n",
      " 4529 [D loss: 8.92383286554832e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949843883514404]\n",
      " 4530 [D loss: 8.240561328420881e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950807094573975]\n",
      " 4531 [D loss: 1.3493155165633652e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954851865768433]\n",
      " 4532 [D loss: 1.1734503459592815e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9950400590896606]\n",
      " 4533 [D loss: 6.003174348734319e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994797945022583]\n",
      " 4534 [D loss: 8.058217645157129e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949434995651245]\n",
      " 4535 [D loss: 3.1016941193229286e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995000422000885]\n",
      " 4536 [D loss: 6.1027130868751556e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948626756668091]\n",
      " 4537 [D loss: 8.990273272502236e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944093227386475]\n",
      " 4538 [D loss: 8.52617813507095e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945253133773804]\n",
      " 4539 [D loss: 1.0555040717008524e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943329691886902]\n",
      " 4540 [D loss: 2.957994547614362e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9939247369766235]\n",
      " 4541 [D loss: 1.3022930033912417e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948010444641113]\n",
      " 4542 [D loss: 2.0415660401340574e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994385838508606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4543 [D loss: 1.5821804481674917e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942657351493835]\n",
      " 4544 [D loss: 7.98386554379249e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951895475387573]\n",
      " 4545 [D loss: 1.1364980309735984e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944338798522949]\n",
      " 4546 [D loss: 7.348333383561112e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944908618927002]\n",
      " 4547 [D loss: 9.849507478065789e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940717220306396]\n",
      " 4548 [D loss: 1.4063600247027352e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9949642419815063]\n",
      " 4549 [D loss: 1.3536186088458635e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941797256469727]\n",
      " 4550 [D loss: 1.1826607078546658e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945334792137146]\n",
      " 4551 [D loss: 7.19811350791133e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942593574523926]\n",
      " 4552 [D loss: 1.4639310393249616e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941864609718323]\n",
      " 4553 [D loss: 5.752118340751622e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943727850914001]\n",
      " 4554 [D loss: 5.238824087427929e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937993288040161]\n",
      " 4555 [D loss: 0.007825382985174656, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9945948719978333]\n",
      " 4556 [D loss: 1.095048719434999e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936459064483643]\n",
      " 4557 [D loss: 1.2400012565194629e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9930885434150696]\n",
      " 4558 [D loss: 1.988885924220085e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933505058288574]\n",
      " 4559 [D loss: 1.3112543456372805e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942026138305664]\n",
      " 4560 [D loss: 4.476567846722901e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9935981035232544]\n",
      " 4561 [D loss: 8.000950401765294e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993489682674408]\n",
      " 4562 [D loss: 8.876189895090647e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9939428567886353]\n",
      " 4563 [D loss: 9.928786312229931e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9919987916946411]\n",
      " 4564 [D loss: 2.1250511053949594e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940951466560364]\n",
      " 4565 [D loss: 2.490203405614011e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927139282226562]\n",
      " 4566 [D loss: 9.644993042456917e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9931933283805847]\n",
      " 4567 [D loss: 1.7682506950222887e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9939849972724915]\n",
      " 4568 [D loss: 1.0793959518196061e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938377141952515]\n",
      " 4569 [D loss: 1.5228504707920365e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937419891357422]\n",
      " 4570 [D loss: 1.274083842872642e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933229088783264]\n",
      " 4571 [D loss: 0.0027082611341029406, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.979607105255127]\n",
      " 4572 [D loss: 0.003541928483173251, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9509848356246948]\n",
      " 4573 [D loss: 0.012467488646507263, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.9571514129638672]\n",
      " 4574 [D loss: 0.00027209502877667546, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9688642024993896]\n",
      " 4575 [D loss: 0.0010326160117983818, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.979708194732666]\n",
      " 4576 [D loss: 0.00073343759868294, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9830319881439209]\n",
      " 4577 [D loss: 9.371949272463098e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9848781824111938]\n",
      " 4578 [D loss: 0.00626963097602129, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9832156896591187]\n",
      " 4579 [D loss: 0.0009846835164353251, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9858013987541199]\n",
      " 4580 [D loss: 6.320153624983504e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9877399206161499]\n",
      " 4581 [D loss: 0.00011204070324311033, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9886786341667175]\n",
      " 4582 [D loss: 4.8381876695202664e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9889140129089355]\n",
      " 4583 [D loss: 0.0005308989202603698, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.987655520439148]\n",
      " 4584 [D loss: 3.522529368638061e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9912141561508179]\n",
      " 4585 [D loss: 0.0004778133297804743, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9919042587280273]\n",
      " 4586 [D loss: 2.455589856253937e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.992851734161377]\n",
      " 4587 [D loss: 7.60798229748616e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940237998962402]\n",
      " 4588 [D loss: 0.00020286657672841102, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943448305130005]\n",
      " 4589 [D loss: 1.0027001735579688e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942430257797241]\n",
      " 4590 [D loss: 1.4700796782562975e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994235634803772]\n",
      " 4591 [D loss: 2.0209550712024793e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927296042442322]\n",
      " 4592 [D loss: 4.6599401684943587e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995295524597168]\n",
      " 4593 [D loss: 0.00613159267231822, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.987546443939209]\n",
      " 4594 [D loss: 0.000125467122416012, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9729083776473999]\n",
      " 4595 [D loss: 0.0003828621411230415, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9680240154266357]\n",
      " 4596 [D loss: 0.00825013779103756, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9606428742408752]\n",
      " 4597 [D loss: 0.0037760301493108273, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9696120023727417]\n",
      " 4598 [D loss: 0.00020324793877080083, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9849319458007812]\n",
      " 4599 [D loss: 6.676696648355573e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9907433390617371]\n",
      " 4600 [D loss: 2.629876689752564e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.991666853427887]\n",
      " 4601 [D loss: 0.0010130906011909246, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9921600818634033]\n",
      " 4602 [D loss: 0.0001542668032925576, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9898086786270142]\n",
      " 4603 [D loss: 4.517264824244194e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9876918792724609]\n",
      " 4604 [D loss: 4.4547861762112007e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9820191264152527]\n",
      " 4605 [D loss: 0.002423185622319579, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9867130517959595]\n",
      " 4606 [D loss: 0.00030895383679307997, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.987251877784729]\n",
      " 4607 [D loss: 0.0031142046209424734, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9926260709762573]\n",
      " 4608 [D loss: 1.5995623471098952e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.99498450756073]\n",
      " 4609 [D loss: 0.00020849307475145906, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995961606502533]\n",
      " 4610 [D loss: 2.3927561414893717e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9957910776138306]\n",
      " 4611 [D loss: 0.006565118208527565, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9859099388122559]\n",
      " 4612 [D loss: 0.0008825768018141389, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9536216259002686]\n",
      " 4613 [D loss: 0.016196344047784805, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.9330542683601379]\n",
      " 4614 [D loss: 0.0047758654691278934, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9612817764282227]\n",
      " 4615 [D loss: 0.0004966635024175048, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9817126393318176]\n",
      " 4616 [D loss: 6.521930481540039e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9888238310813904]\n",
      " 4617 [D loss: 4.9801408749772236e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9905743598937988]\n",
      " 4618 [D loss: 5.73675333725987e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952412843704224]\n",
      " 4619 [D loss: 0.006604609079658985, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9945775270462036]\n",
      " 4620 [D loss: 8.434371920884587e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9915313124656677]\n",
      " 4621 [D loss: 4.703972444985993e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9925714731216431]\n",
      " 4622 [D loss: 4.2456809751456603e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9893625974655151]\n",
      " 4623 [D loss: 3.479210863588378e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9893832206726074]\n",
      " 4624 [D loss: 3.777582969632931e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9866440296173096]\n",
      " 4625 [D loss: 0.0007570813177153468, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9908401966094971]\n",
      " 4626 [D loss: 5.766120375483297e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9903866052627563]\n",
      " 4627 [D loss: 4.32386550528463e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9922894239425659]\n",
      " 4628 [D loss: 0.00012678297935053706, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9926232099533081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4629 [D loss: 0.0001911529980134219, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9913365840911865]\n",
      " 4630 [D loss: 3.0172959668561816e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9906579256057739]\n",
      " 4631 [D loss: 0.0001381855399813503, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993200421333313]\n",
      " 4632 [D loss: 2.615077028167434e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993278980255127]\n",
      " 4633 [D loss: 1.4274804925662465e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9928674101829529]\n",
      " 4634 [D loss: 2.8356998882372864e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948084950447083]\n",
      " 4635 [D loss: 0.00010290655336575583, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936935901641846]\n",
      " 4636 [D loss: 1.2498365322244354e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941391944885254]\n",
      " 4637 [D loss: 1.445885300199734e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936147928237915]\n",
      " 4638 [D loss: 1.1470247045508586e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943066835403442]\n",
      " 4639 [D loss: 7.488125993404537e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942147135734558]\n",
      " 4640 [D loss: 1.0547433703322895e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937753677368164]\n",
      " 4641 [D loss: 0.00018074942636303604, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994767427444458]\n",
      " 4642 [D loss: 5.99390514253173e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933502078056335]\n",
      " 4643 [D loss: 5.923780918237753e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9911194443702698]\n",
      " 4644 [D loss: 2.5382614694535732e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9918781518936157]\n",
      " 4645 [D loss: 3.178223050781526e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.991746723651886]\n",
      " 4646 [D loss: 0.00011050137982238084, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.992871880531311]\n",
      " 4647 [D loss: 2.0544262952171266e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9918695688247681]\n",
      " 4648 [D loss: 2.5145400286419317e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927459955215454]\n",
      " 4649 [D loss: 4.1132105252472684e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9928197860717773]\n",
      " 4650 [D loss: 5.4446882131742314e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9931427836418152]\n",
      " 4651 [D loss: 1.2745305866701528e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9921925663948059]\n",
      " 4652 [D loss: 0.00021925329929217696, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9934048652648926]\n",
      " 4653 [D loss: 6.881417357362807e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9920365214347839]\n",
      " 4654 [D loss: 3.259478398831561e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9943447709083557]\n",
      " 4655 [D loss: 2.0007757484563626e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942464828491211]\n",
      " 4656 [D loss: 1.5733647160232067e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9944713711738586]\n",
      " 4657 [D loss: 0.0002824997645802796, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941984415054321]\n",
      " 4658 [D loss: 2.0749068426084705e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9930801391601562]\n",
      " 4659 [D loss: 0.0001114106344175525, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9917100071907043]\n",
      " 4660 [D loss: 2.1730074877268635e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9919615983963013]\n",
      " 4661 [D loss: 8.08284676168114e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9915266036987305]\n",
      " 4662 [D loss: 7.042643119348213e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9911537766456604]\n",
      " 4663 [D loss: 4.6775403461651877e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9928312301635742]\n",
      " 4664 [D loss: 1.9791463273577392e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.98984694480896]\n",
      " 4665 [D loss: 4.713178350357339e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9911072254180908]\n",
      " 4666 [D loss: 1.1035405805159826e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9907162189483643]\n",
      " 4667 [D loss: 0.00010041503992397338, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9922103881835938]\n",
      " 4668 [D loss: 6.815670531068463e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941993355751038]\n",
      " 4669 [D loss: 6.362894055200741e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9930030703544617]\n",
      " 4670 [D loss: 2.817888162098825e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937177300453186]\n",
      " 4671 [D loss: 6.82472309563309e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.992612361907959]\n",
      " 4672 [D loss: 5.2530474931700155e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933507442474365]\n",
      " 4673 [D loss: 1.803592203941662e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9929143190383911]\n",
      " 4674 [D loss: 1.3692512766283471e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9937930107116699]\n",
      " 4675 [D loss: 6.0289974499028176e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940752983093262]\n",
      " 4676 [D loss: 1.3083974408800714e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942924976348877]\n",
      " 4677 [D loss: 2.1468114937306382e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9936803579330444]\n",
      " 4678 [D loss: 2.335030512767844e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9939994812011719]\n",
      " 4679 [D loss: 5.8780413382919505e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9945881366729736]\n",
      " 4680 [D loss: 1.2855602108174935e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927650690078735]\n",
      " 4681 [D loss: 3.7801240978296846e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9927014112472534]\n",
      " 4682 [D loss: 9.667803169577383e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940388202667236]\n",
      " 4683 [D loss: 9.780163236428052e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9930011630058289]\n",
      " 4684 [D loss: 1.8540775272413157e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9911686182022095]\n",
      " 4685 [D loss: 0.00014652438403572887, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9912736415863037]\n",
      " 4686 [D loss: 0.00023140301345847547, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9931591749191284]\n",
      " 4687 [D loss: 0.0005555279203690588, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9933257102966309]\n",
      " 4688 [D loss: 6.967892113607377e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.995031476020813]\n",
      " 4689 [D loss: 0.00012001735740341246, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9966543912887573]\n",
      " 4690 [D loss: 3.616719595811446e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9970786571502686]\n",
      " 4691 [D loss: 1.4155208418742404e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971189498901367]\n",
      " 4692 [D loss: 5.065249752078671e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9976416826248169]\n",
      " 4693 [D loss: 0.00018641487986315042, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9969257116317749]\n",
      " 4694 [D loss: 1.3264828339742962e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954783916473389]\n",
      " 4695 [D loss: 7.220647239591926e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942207336425781]\n",
      " 4696 [D loss: 1.5012273252068553e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9924799203872681]\n",
      " 4697 [D loss: 5.5448716011596844e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9891337752342224]\n",
      " 4698 [D loss: 5.229687667451799e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9903428554534912]\n",
      " 4699 [D loss: 6.48227141937241e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9840615391731262]\n",
      " 4700 [D loss: 0.00017703020421322435, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9820312261581421]\n",
      " 4701 [D loss: 0.00026130472542718053, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9873509407043457]\n",
      " 4702 [D loss: 0.00103625503834337, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9907227158546448]\n",
      " 4703 [D loss: 0.00010637100058374926, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994640052318573]\n",
      " 4704 [D loss: 1.3102800039632712e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9965452551841736]\n",
      " 4705 [D loss: 0.0006556665757670999, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9941447377204895]\n",
      " 4706 [D loss: 1.985202106880024e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9907647967338562]\n",
      " 4707 [D loss: 9.971797226171475e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9882328510284424]\n",
      " 4708 [D loss: 0.00014195858966559172, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9844355583190918]\n",
      " 4709 [D loss: 0.006218068301677704, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9941074252128601]\n",
      " 4710 [D loss: 6.849514647910837e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9982231855392456]\n",
      " 4711 [D loss: 4.1066195421990415e-07, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9992061853408813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4712 [D loss: 0.1306360363960266, acc_real: 73.437500, acc_fake: 100.000000] [G loss: 0.9587619304656982]\n",
      " 4713 [D loss: 0.009727313183248043, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.8492739796638489]\n",
      " 4714 [D loss: 0.07521457970142365, acc_real: 100.000000, acc_fake: 84.375000] [G loss: 0.701744794845581]\n",
      " 4715 [D loss: 0.0744793564081192, acc_real: 100.000000, acc_fake: 76.562500] [G loss: 0.6959171295166016]\n",
      " 4716 [D loss: 0.0870189219713211, acc_real: 100.000000, acc_fake: 76.562500] [G loss: 0.8508508205413818]\n",
      " 4717 [D loss: 0.057006075978279114, acc_real: 100.000000, acc_fake: 87.500000] [G loss: 0.9535759687423706]\n",
      " 4718 [D loss: 0.008203890174627304, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9958606958389282]\n",
      " 4719 [D loss: 0.08883991837501526, acc_real: 87.500000, acc_fake: 93.750000] [G loss: 0.9550696611404419]\n",
      " 4720 [D loss: 0.02608301304280758, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.8233208656311035]\n",
      " 4721 [D loss: 0.05225076153874397, acc_real: 100.000000, acc_fake: 82.812500] [G loss: 0.7301916480064392]\n",
      " 4722 [D loss: 0.1347717046737671, acc_real: 100.000000, acc_fake: 65.625000] [G loss: 0.6107383966445923]\n",
      " 4723 [D loss: 0.11526481062173843, acc_real: 100.000000, acc_fake: 65.625000] [G loss: 0.7128528356552124]\n",
      " 4724 [D loss: 0.0529358834028244, acc_real: 100.000000, acc_fake: 85.937500] [G loss: 0.8127630949020386]\n",
      " 4725 [D loss: 0.016774587333202362, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.9475208520889282]\n",
      " 4726 [D loss: 8.634613186586648e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9891569018363953]\n",
      " 4727 [D loss: 6.594528713321779e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9975836277008057]\n",
      " 4728 [D loss: 0.023438161239027977, acc_real: 95.312500, acc_fake: 100.000000] [G loss: 0.9989950656890869]\n",
      " 4729 [D loss: 0.0937425047159195, acc_real: 81.250000, acc_fake: 100.000000] [G loss: 0.9993913173675537]\n",
      " 4730 [D loss: 0.07029420882463455, acc_real: 85.937500, acc_fake: 100.000000] [G loss: 0.9995468258857727]\n",
      " 4731 [D loss: 0.1328125298023224, acc_real: 73.437500, acc_fake: 100.000000] [G loss: 0.9996398687362671]\n",
      " 4732 [D loss: 0.09374633431434631, acc_real: 81.250000, acc_fake: 100.000000] [G loss: 0.999750018119812]\n",
      " 4733 [D loss: 0.11718752980232239, acc_real: 76.562500, acc_fake: 100.000000] [G loss: 0.9997615814208984]\n",
      " 4734 [D loss: 0.1875000149011612, acc_real: 62.500000, acc_fake: 100.000000] [G loss: 0.9997612237930298]\n",
      " 4735 [D loss: 0.14093919098377228, acc_real: 71.875000, acc_fake: 100.000000] [G loss: 0.9991629123687744]\n",
      " 4736 [D loss: 0.12533409893512726, acc_real: 75.000000, acc_fake: 100.000000] [G loss: 0.9835219383239746]\n",
      " 4737 [D loss: 0.04275207594037056, acc_real: 93.750000, acc_fake: 98.437500] [G loss: 0.8457719683647156]\n",
      " 4738 [D loss: 0.043851058930158615, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.7113730311393738]\n",
      " 4739 [D loss: 0.10058994591236115, acc_real: 100.000000, acc_fake: 75.000000] [G loss: 0.6798355579376221]\n",
      " 4740 [D loss: 0.13797979056835175, acc_real: 100.000000, acc_fake: 64.062500] [G loss: 0.6614060401916504]\n",
      " 4741 [D loss: 0.11229482293128967, acc_real: 100.000000, acc_fake: 70.312500] [G loss: 0.7037315368652344]\n",
      " 4742 [D loss: 0.06050954759120941, acc_real: 100.000000, acc_fake: 85.937500] [G loss: 0.8665479421615601]\n",
      " 4743 [D loss: 0.018856601789593697, acc_real: 98.437500, acc_fake: 96.875000] [G loss: 0.9686823487281799]\n",
      " 4744 [D loss: 0.007954449392855167, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9926435947418213]\n",
      " 4745 [D loss: 0.03140290081501007, acc_real: 93.750000, acc_fake: 100.000000] [G loss: 0.9971901774406433]\n",
      " 4746 [D loss: 0.0703137218952179, acc_real: 85.937500, acc_fake: 100.000000] [G loss: 0.9986271858215332]\n",
      " 4747 [D loss: 0.08256249874830246, acc_real: 82.812500, acc_fake: 100.000000] [G loss: 0.9985589981079102]\n",
      " 4748 [D loss: 0.06990376114845276, acc_real: 85.937500, acc_fake: 100.000000] [G loss: 0.9963555335998535]\n",
      " 4749 [D loss: 0.07032085955142975, acc_real: 85.937500, acc_fake: 100.000000] [G loss: 0.9912327527999878]\n",
      " 4750 [D loss: 0.08604307472705841, acc_real: 82.812500, acc_fake: 100.000000] [G loss: 0.9792680144309998]\n",
      " 4751 [D loss: 0.04964854568243027, acc_real: 93.750000, acc_fake: 96.875000] [G loss: 0.6232123374938965]\n",
      " 4752 [D loss: 0.1942196637392044, acc_real: 100.000000, acc_fake: 54.687500] [G loss: 0.46501028537750244]\n",
      " 4753 [D loss: 0.2334049642086029, acc_real: 100.000000, acc_fake: 46.875000] [G loss: 0.40449070930480957]\n",
      " 4754 [D loss: 0.2710955739021301, acc_real: 100.000000, acc_fake: 42.187500] [G loss: 0.36854732036590576]\n",
      " 4755 [D loss: 0.3407985270023346, acc_real: 100.000000, acc_fake: 26.562500] [G loss: 0.37077856063842773]\n",
      " 4756 [D loss: 0.2788621783256531, acc_real: 100.000000, acc_fake: 42.187500] [G loss: 0.31264710426330566]\n",
      " 4757 [D loss: 0.3249073624610901, acc_real: 100.000000, acc_fake: 32.812500] [G loss: 0.34275662899017334]\n",
      " 4758 [D loss: 0.3508768379688263, acc_real: 100.000000, acc_fake: 26.562500] [G loss: 0.3312959671020508]\n",
      " 4759 [D loss: 0.3616498112678528, acc_real: 100.000000, acc_fake: 26.562500] [G loss: 0.3474463224411011]\n",
      " 4760 [D loss: 0.3317355811595917, acc_real: 100.000000, acc_fake: 32.812500] [G loss: 0.2664923667907715]\n",
      " 4761 [D loss: 0.3689131736755371, acc_real: 100.000000, acc_fake: 23.437500] [G loss: 0.3137566149234772]\n",
      " 4762 [D loss: 0.32434725761413574, acc_real: 100.000000, acc_fake: 34.375000] [G loss: 0.2755489945411682]\n",
      " 4763 [D loss: 0.3263169825077057, acc_real: 100.000000, acc_fake: 32.812500] [G loss: 0.3072460889816284]\n",
      " 4764 [D loss: 0.3390924334526062, acc_real: 100.000000, acc_fake: 29.687500] [G loss: 0.3060455322265625]\n",
      " 4765 [D loss: 0.3852073550224304, acc_real: 100.000000, acc_fake: 20.312500] [G loss: 0.2259313464164734]\n",
      " 4766 [D loss: 0.32761287689208984, acc_real: 100.000000, acc_fake: 31.250000] [G loss: 0.27500393986701965]\n",
      " 4767 [D loss: 0.34839731454849243, acc_real: 100.000000, acc_fake: 28.125000] [G loss: 0.22593176364898682]\n",
      " 4768 [D loss: 0.3515722155570984, acc_real: 100.000000, acc_fake: 28.125000] [G loss: 0.2722025513648987]\n",
      " 4769 [D loss: 0.3338492214679718, acc_real: 100.000000, acc_fake: 31.250000] [G loss: 0.27579861879348755]\n",
      " 4770 [D loss: 0.3541416823863983, acc_real: 100.000000, acc_fake: 26.562500] [G loss: 0.25689372420310974]\n",
      " 4771 [D loss: 0.3329019844532013, acc_real: 100.000000, acc_fake: 29.687500] [G loss: 0.2683754563331604]\n",
      " 4772 [D loss: 0.3537702262401581, acc_real: 100.000000, acc_fake: 26.562500] [G loss: 0.2448570430278778]\n",
      " 4773 [D loss: 0.37349599599838257, acc_real: 100.000000, acc_fake: 23.437500] [G loss: 0.29778897762298584]\n",
      " 4774 [D loss: 0.3887494206428528, acc_real: 100.000000, acc_fake: 20.312500] [G loss: 0.26928073167800903]\n",
      " 4775 [D loss: 0.3466634750366211, acc_real: 100.000000, acc_fake: 28.125000] [G loss: 0.24874639511108398]\n",
      " 4776 [D loss: 0.37322595715522766, acc_real: 100.000000, acc_fake: 21.875000] [G loss: 0.25112736225128174]\n",
      " 4777 [D loss: 0.38694441318511963, acc_real: 100.000000, acc_fake: 20.312500] [G loss: 0.2449984848499298]\n",
      " 4778 [D loss: 0.40219342708587646, acc_real: 100.000000, acc_fake: 15.625000] [G loss: 0.22652900218963623]\n",
      " 4779 [D loss: 0.38392049074172974, acc_real: 100.000000, acc_fake: 20.312500] [G loss: 0.23838578164577484]\n",
      " 4780 [D loss: 0.3514087200164795, acc_real: 100.000000, acc_fake: 25.000000] [G loss: 0.23617172241210938]\n",
      " 4781 [D loss: 0.31688985228538513, acc_real: 100.000000, acc_fake: 34.375000] [G loss: 0.279784619808197]\n",
      " 4782 [D loss: 0.3129841983318329, acc_real: 100.000000, acc_fake: 34.375000] [G loss: 0.22414331138134003]\n",
      " 4783 [D loss: 0.3375965356826782, acc_real: 100.000000, acc_fake: 26.562500] [G loss: 0.20494118332862854]\n",
      " 4784 [D loss: 0.34982848167419434, acc_real: 100.000000, acc_fake: 21.875000] [G loss: 0.2500446140766144]\n",
      " 4785 [D loss: 0.337325781583786, acc_real: 100.000000, acc_fake: 23.437500] [G loss: 0.21770307421684265]\n",
      " 4786 [D loss: 0.30105990171432495, acc_real: 100.000000, acc_fake: 25.000000] [G loss: 0.26469773054122925]\n",
      " 4787 [D loss: 0.18389369547367096, acc_real: 100.000000, acc_fake: 35.937500] [G loss: 0.39681902527809143]\n",
      " 4788 [D loss: 0.05232034623622894, acc_real: 100.000000, acc_fake: 81.250000] [G loss: 0.6662746667861938]\n",
      " 4789 [D loss: 0.0053115468472242355, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.931378960609436]\n",
      " 4790 [D loss: 0.00035178055986762047, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9830648899078369]\n",
      " 4791 [D loss: 0.01564793288707733, acc_real: 96.875000, acc_fake: 100.000000] [G loss: 0.99283766746521]\n",
      " 4792 [D loss: 0.07812940329313278, acc_real: 84.375000, acc_fake: 100.000000] [G loss: 0.9980230331420898]\n",
      " 4793 [D loss: 0.10936831682920456, acc_real: 78.125000, acc_fake: 100.000000] [G loss: 0.9991726875305176]\n",
      " 4794 [D loss: 0.15382519364356995, acc_real: 68.750000, acc_fake: 100.000000] [G loss: 0.9991072416305542]\n",
      " 4795 [D loss: 0.15637345612049103, acc_real: 68.750000, acc_fake: 100.000000] [G loss: 0.9993325471878052]\n",
      " 4796 [D loss: 0.14061489701271057, acc_real: 71.875000, acc_fake: 100.000000] [G loss: 0.9992837905883789]\n",
      " 4797 [D loss: 0.14843764901161194, acc_real: 70.312500, acc_fake: 100.000000] [G loss: 0.9993797540664673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4798 [D loss: 0.12500010430812836, acc_real: 75.000000, acc_fake: 100.000000] [G loss: 0.999245285987854]\n",
      " 4799 [D loss: 0.19140762090682983, acc_real: 60.937500, acc_fake: 100.000000] [G loss: 0.9988850951194763]\n",
      " 4800 [D loss: 0.17579685151576996, acc_real: 64.062500, acc_fake: 100.000000] [G loss: 0.996039867401123]\n",
      " 4801 [D loss: 0.08202221244573593, acc_real: 82.812500, acc_fake: 100.000000] [G loss: 0.8028205633163452]\n",
      " 4802 [D loss: 0.143709197640419, acc_real: 100.000000, acc_fake: 48.437500] [G loss: 0.34866955876350403]\n",
      " 4803 [D loss: 0.31918275356292725, acc_real: 100.000000, acc_fake: 26.562500] [G loss: 0.254376620054245]\n",
      " 4804 [D loss: 0.37719783186912537, acc_real: 100.000000, acc_fake: 20.312500] [G loss: 0.19202125072479248]\n",
      " 4805 [D loss: 0.3931123912334442, acc_real: 100.000000, acc_fake: 20.312500] [G loss: 0.19021126627922058]\n",
      " 4806 [D loss: 0.3718090355396271, acc_real: 100.000000, acc_fake: 23.437500] [G loss: 0.20945514738559723]\n",
      " 4807 [D loss: 0.41210490465164185, acc_real: 100.000000, acc_fake: 15.625000] [G loss: 0.22049462795257568]\n",
      " 4808 [D loss: 0.4168377220630646, acc_real: 100.000000, acc_fake: 14.062500] [G loss: 0.18367063999176025]\n",
      " 4809 [D loss: 0.36755210161209106, acc_real: 100.000000, acc_fake: 25.000000] [G loss: 0.20942097902297974]\n",
      " 4810 [D loss: 0.3828623294830322, acc_real: 100.000000, acc_fake: 21.875000] [G loss: 0.19388671219348907]\n",
      " 4811 [D loss: 0.42231273651123047, acc_real: 100.000000, acc_fake: 14.062500] [G loss: 0.2056698054075241]\n",
      " 4812 [D loss: 0.3745335340499878, acc_real: 100.000000, acc_fake: 23.437500] [G loss: 0.1954040825366974]\n",
      " 4813 [D loss: 0.4198092818260193, acc_real: 100.000000, acc_fake: 14.062500] [G loss: 0.2037293165922165]\n",
      " 4814 [D loss: 0.3674432039260864, acc_real: 100.000000, acc_fake: 25.000000] [G loss: 0.14304739236831665]\n",
      " 4815 [D loss: 0.4160357117652893, acc_real: 100.000000, acc_fake: 12.500000] [G loss: 0.2225954830646515]\n",
      " 4816 [D loss: 0.36452242732048035, acc_real: 100.000000, acc_fake: 26.562500] [G loss: 0.17785659432411194]\n",
      " 4817 [D loss: 0.4022616147994995, acc_real: 100.000000, acc_fake: 18.750000] [G loss: 0.1980627477169037]\n",
      " 4818 [D loss: 0.4375932216644287, acc_real: 100.000000, acc_fake: 10.937500] [G loss: 0.15174049139022827]\n",
      " 4819 [D loss: 0.3888638913631439, acc_real: 100.000000, acc_fake: 17.187500] [G loss: 0.19754914939403534]\n",
      " 4820 [D loss: 0.3363463282585144, acc_real: 100.000000, acc_fake: 26.562500] [G loss: 0.2065756916999817]\n",
      " 4821 [D loss: 0.3726552128791809, acc_real: 100.000000, acc_fake: 20.312500] [G loss: 0.15249624848365784]\n",
      " 4822 [D loss: 0.29825007915496826, acc_real: 100.000000, acc_fake: 29.687500] [G loss: 0.22550101578235626]\n",
      " 4823 [D loss: 0.2461707592010498, acc_real: 100.000000, acc_fake: 25.000000] [G loss: 0.3017650246620178]\n",
      " 4824 [D loss: 0.10661652684211731, acc_real: 100.000000, acc_fake: 71.875000] [G loss: 0.5517129302024841]\n",
      " 4825 [D loss: 0.03168278932571411, acc_real: 100.000000, acc_fake: 89.062500] [G loss: 0.8558229207992554]\n",
      " 4826 [D loss: 0.001530931913293898, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9554926753044128]\n",
      " 4827 [D loss: 0.015499808825552464, acc_real: 96.875000, acc_fake: 100.000000] [G loss: 0.982867956161499]\n",
      " 4828 [D loss: 0.00785062275826931, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9976250529289246]\n",
      " 4829 [D loss: 0.0312529131770134, acc_real: 93.750000, acc_fake: 100.000000] [G loss: 0.9990081787109375]\n",
      " 4830 [D loss: 0.03012118488550186, acc_real: 93.750000, acc_fake: 100.000000] [G loss: 0.9978621006011963]\n",
      " 4831 [D loss: 0.015702061355113983, acc_real: 96.875000, acc_fake: 100.000000] [G loss: 0.994432806968689]\n",
      " 4832 [D loss: 0.005779508035629988, acc_real: 96.875000, acc_fake: 100.000000] [G loss: 0.9868736267089844]\n",
      " 4833 [D loss: 0.00025023584021255374, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9460017085075378]\n",
      " 4834 [D loss: 0.0013575771590694785, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9226750731468201]\n",
      " 4835 [D loss: 0.004038977436721325, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.896126389503479]\n",
      " 4836 [D loss: 0.00290777999907732, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.821571946144104]\n",
      " 4837 [D loss: 0.024139896035194397, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.7946898937225342]\n",
      " 4838 [D loss: 0.029562989249825478, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.7934256792068481]\n",
      " 4839 [D loss: 0.03689543157815933, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.8198680877685547]\n",
      " 4840 [D loss: 0.026154324412345886, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.871484637260437]\n",
      " 4841 [D loss: 0.020828930661082268, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.9277350902557373]\n",
      " 4842 [D loss: 0.006814304273575544, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9436758756637573]\n",
      " 4843 [D loss: 0.002469551283866167, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9736456871032715]\n",
      " 4844 [D loss: 0.0015804572030901909, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9736131429672241]\n",
      " 4845 [D loss: 0.0034503648057579994, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9872607588768005]\n",
      " 4846 [D loss: 0.0008180956356227398, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9929841756820679]\n",
      " 4847 [D loss: 0.00022931824787519872, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993182361125946]\n",
      " 4848 [D loss: 5.848309228895232e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.996997594833374]\n",
      " 4849 [D loss: 6.230453436728567e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9971791505813599]\n",
      " 4850 [D loss: 4.299238298699493e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9977184534072876]\n",
      " 4851 [D loss: 0.0032650213688611984, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9977098703384399]\n",
      " 4852 [D loss: 0.002412043511867523, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9971224665641785]\n",
      " 4853 [D loss: 6.198493792908266e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9973382949829102]\n",
      " 4854 [D loss: 5.325668098521419e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9967133402824402]\n",
      " 4855 [D loss: 0.0001580060343258083, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9959142208099365]\n",
      " 4856 [D loss: 0.00020383123774081469, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9973382949829102]\n",
      " 4857 [D loss: 1.389646149618784e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9963045120239258]\n",
      " 4858 [D loss: 1.647462158871349e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9913561344146729]\n",
      " 4859 [D loss: 0.00011839174112537876, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952700138092041]\n",
      " 4860 [D loss: 1.6804417100502178e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9955511093139648]\n",
      " 4861 [D loss: 3.3963948226301e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9951779246330261]\n",
      " 4862 [D loss: 0.0001393074926454574, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9960106015205383]\n",
      " 4863 [D loss: 0.00807152409106493, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9940521717071533]\n",
      " 4864 [D loss: 2.0366280296002515e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9908211827278137]\n",
      " 4865 [D loss: 1.9320623323437758e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9922189712524414]\n",
      " 4866 [D loss: 7.344845653278753e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9948210120201111]\n",
      " 4867 [D loss: 2.254513674415648e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.993269681930542]\n",
      " 4868 [D loss: 7.967062265379354e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.991621196269989]\n",
      " 4869 [D loss: 6.054634286556393e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9888321161270142]\n",
      " 4870 [D loss: 8.462453115498647e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9896078109741211]\n",
      " 4871 [D loss: 2.9536924557760358e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9827774167060852]\n",
      " 4872 [D loss: 0.00047630665358155966, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9863762855529785]\n",
      " 4873 [D loss: 0.00019965933461207896, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.984904408454895]\n",
      " 4874 [D loss: 0.0005046525038778782, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9802971482276917]\n",
      " 4875 [D loss: 9.573147690389305e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.974280595779419]\n",
      " 4876 [D loss: 0.0014602866722270846, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9807915687561035]\n",
      " 4877 [D loss: 0.0005922257550992072, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9793220162391663]\n",
      " 4878 [D loss: 2.3232119929161854e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9878816604614258]\n",
      " 4879 [D loss: 0.0024541595485061407, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9755236506462097]\n",
      " 4880 [D loss: 0.0016139857470989227, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.989870548248291]\n",
      " 4881 [D loss: 7.539615762652829e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9873368144035339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4882 [D loss: 0.0001457975449739024, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9881104826927185]\n",
      " 4883 [D loss: 0.031639501452445984, acc_real: 93.750000, acc_fake: 98.437500] [G loss: 0.7592477798461914]\n",
      " 4884 [D loss: 0.12301656603813171, acc_real: 100.000000, acc_fake: 68.750000] [G loss: 0.5121851563453674]\n",
      " 4885 [D loss: 0.24365875124931335, acc_real: 100.000000, acc_fake: 39.062500] [G loss: 0.3540785312652588]\n",
      " 4886 [D loss: 0.2821153402328491, acc_real: 100.000000, acc_fake: 31.250000] [G loss: 0.30494385957717896]\n",
      " 4887 [D loss: 0.33201172947883606, acc_real: 100.000000, acc_fake: 28.125000] [G loss: 0.32257285714149475]\n",
      " 4888 [D loss: 0.2792356610298157, acc_real: 100.000000, acc_fake: 37.500000] [G loss: 0.25252729654312134]\n",
      " 4889 [D loss: 0.31382274627685547, acc_real: 100.000000, acc_fake: 28.125000] [G loss: 0.31437668204307556]\n",
      " 4890 [D loss: 0.29214245080947876, acc_real: 100.000000, acc_fake: 31.250000] [G loss: 0.30091309547424316]\n",
      " 4891 [D loss: 0.27159327268600464, acc_real: 100.000000, acc_fake: 35.937500] [G loss: 0.3091527223587036]\n",
      " 4892 [D loss: 0.22295185923576355, acc_real: 100.000000, acc_fake: 45.312500] [G loss: 0.36861228942871094]\n",
      " 4893 [D loss: 0.1859556883573532, acc_real: 100.000000, acc_fake: 59.375000] [G loss: 0.4512164890766144]\n",
      " 4894 [D loss: 0.18573424220085144, acc_real: 100.000000, acc_fake: 59.375000] [G loss: 0.49252450466156006]\n",
      " 4895 [D loss: 0.14997598528862, acc_real: 100.000000, acc_fake: 64.062500] [G loss: 0.6350077986717224]\n",
      " 4896 [D loss: 0.09467543661594391, acc_real: 100.000000, acc_fake: 78.125000] [G loss: 0.7040368318557739]\n",
      " 4897 [D loss: 0.03618617728352547, acc_real: 98.437500, acc_fake: 90.625000] [G loss: 0.7852488160133362]\n",
      " 4898 [D loss: 0.04019158333539963, acc_real: 93.750000, acc_fake: 96.875000] [G loss: 0.8325902819633484]\n",
      " 4899 [D loss: 0.051751405000686646, acc_real: 92.187500, acc_fake: 96.875000] [G loss: 0.8665124177932739]\n",
      " 4900 [D loss: 0.03902639076113701, acc_real: 93.750000, acc_fake: 100.000000] [G loss: 0.9397373199462891]\n",
      " 4901 [D loss: 0.03949442505836487, acc_real: 92.187500, acc_fake: 100.000000] [G loss: 0.960450291633606]\n",
      " 4902 [D loss: 0.07541421800851822, acc_real: 85.937500, acc_fake: 98.437500] [G loss: 0.9435029625892639]\n",
      " 4903 [D loss: 0.033424247056245804, acc_real: 93.750000, acc_fake: 98.437500] [G loss: 0.9476525783538818]\n",
      " 4904 [D loss: 0.05427756905555725, acc_real: 90.625000, acc_fake: 98.437500] [G loss: 0.9291764497756958]\n",
      " 4905 [D loss: 0.10197914391756058, acc_real: 79.687500, acc_fake: 100.000000] [G loss: 0.9406263828277588]\n",
      " 4906 [D loss: 0.08594745397567749, acc_real: 87.500000, acc_fake: 93.750000] [G loss: 0.8711103796958923]\n",
      " 4907 [D loss: 0.05105388164520264, acc_real: 98.437500, acc_fake: 89.062500] [G loss: 0.8118656277656555]\n",
      " 4908 [D loss: 0.08345592767000198, acc_real: 95.312500, acc_fake: 85.937500] [G loss: 0.7913944721221924]\n",
      " 4909 [D loss: 0.0679154098033905, acc_real: 96.875000, acc_fake: 89.062500] [G loss: 0.7703319787979126]\n",
      " 4910 [D loss: 0.06823684275150299, acc_real: 100.000000, acc_fake: 84.375000] [G loss: 0.7912557125091553]\n",
      " 4911 [D loss: 0.10618683695793152, acc_real: 100.000000, acc_fake: 73.437500] [G loss: 0.8072712421417236]\n",
      " 4912 [D loss: 0.08062700182199478, acc_real: 96.875000, acc_fake: 81.250000] [G loss: 0.8449168801307678]\n",
      " 4913 [D loss: 0.04723510146141052, acc_real: 98.437500, acc_fake: 87.500000] [G loss: 0.8864109516143799]\n",
      " 4914 [D loss: 0.07373284548521042, acc_real: 96.875000, acc_fake: 85.937500] [G loss: 0.9137688875198364]\n",
      " 4915 [D loss: 0.043626800179481506, acc_real: 92.187500, acc_fake: 100.000000] [G loss: 0.9639255404472351]\n",
      " 4916 [D loss: 0.054824914783239365, acc_real: 92.187500, acc_fake: 95.312500] [G loss: 0.9115581512451172]\n",
      " 4917 [D loss: 0.06289949268102646, acc_real: 98.437500, acc_fake: 84.375000] [G loss: 0.8224475383758545]\n",
      " 4918 [D loss: 0.07231203466653824, acc_real: 100.000000, acc_fake: 79.687500] [G loss: 0.7986403703689575]\n",
      " 4919 [D loss: 0.07535051554441452, acc_real: 100.000000, acc_fake: 78.125000] [G loss: 0.7801216840744019]\n",
      " 4920 [D loss: 0.056206971406936646, acc_real: 100.000000, acc_fake: 85.937500] [G loss: 0.777190089225769]\n",
      " 4921 [D loss: 0.04994560778141022, acc_real: 100.000000, acc_fake: 85.937500] [G loss: 0.8247183561325073]\n",
      " 4922 [D loss: 0.06292839348316193, acc_real: 100.000000, acc_fake: 82.812500] [G loss: 0.8806593418121338]\n",
      " 4923 [D loss: 0.03914150595664978, acc_real: 100.000000, acc_fake: 89.062500] [G loss: 0.8789072036743164]\n",
      " 4924 [D loss: 0.03717219829559326, acc_real: 100.000000, acc_fake: 89.062500] [G loss: 0.9349424242973328]\n",
      " 4925 [D loss: 0.014174601063132286, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.9450737833976746]\n",
      " 4926 [D loss: 0.0022226173896342516, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9826569557189941]\n",
      " 4927 [D loss: 0.009095153771340847, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9826536178588867]\n",
      " 4928 [D loss: 0.009099499322474003, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9889119267463684]\n",
      " 4929 [D loss: 0.0031980019994080067, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9954988956451416]\n",
      " 4930 [D loss: 0.022106880322098732, acc_real: 96.875000, acc_fake: 98.437500] [G loss: 0.9907721877098083]\n",
      " 4931 [D loss: 0.00981720257550478, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9967420101165771]\n",
      " 4932 [D loss: 0.027687108144164085, acc_real: 95.312500, acc_fake: 98.437500] [G loss: 0.9496045708656311]\n",
      " 4933 [D loss: 0.022149724885821342, acc_real: 100.000000, acc_fake: 93.750000] [G loss: 0.8909708261489868]\n",
      " 4934 [D loss: 0.03475745767354965, acc_real: 100.000000, acc_fake: 92.187500] [G loss: 0.7977957725524902]\n",
      " 4935 [D loss: 0.06641645729541779, acc_real: 100.000000, acc_fake: 81.250000] [G loss: 0.7473050355911255]\n",
      " 4936 [D loss: 0.09742511808872223, acc_real: 100.000000, acc_fake: 75.000000] [G loss: 0.7119582891464233]\n",
      " 4937 [D loss: 0.10254955291748047, acc_real: 100.000000, acc_fake: 70.312500] [G loss: 0.710002064704895]\n",
      " 4938 [D loss: 0.11360183358192444, acc_real: 100.000000, acc_fake: 68.750000] [G loss: 0.7200487852096558]\n",
      " 4939 [D loss: 0.12340019643306732, acc_real: 100.000000, acc_fake: 64.062500] [G loss: 0.7453489303588867]\n",
      " 4940 [D loss: 0.03536968305706978, acc_real: 100.000000, acc_fake: 90.625000] [G loss: 0.785819947719574]\n",
      " 4941 [D loss: 0.0029019187204539776, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.8624136447906494]\n",
      " 4942 [D loss: 0.013987807556986809, acc_real: 100.000000, acc_fake: 95.312500] [G loss: 0.9148736000061035]\n",
      " 4943 [D loss: 0.0005534084630198777, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9593600630760193]\n",
      " 4944 [D loss: 0.003064169082790613, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9670177102088928]\n",
      " 4945 [D loss: 0.0004862711066380143, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9759178161621094]\n",
      " 4946 [D loss: 0.00025914557045325637, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9816559553146362]\n",
      " 4947 [D loss: 0.00014653477410320193, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9773008823394775]\n",
      " 4948 [D loss: 5.0969039875781164e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9882804155349731]\n",
      " 4949 [D loss: 8.461425022687763e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.984419584274292]\n",
      " 4950 [D loss: 0.00022225180873647332, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.985528290271759]\n",
      " 4951 [D loss: 6.4368978200946e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9855940341949463]\n",
      " 4952 [D loss: 0.00121117546223104, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9891842007637024]\n",
      " 4953 [D loss: 0.005769751965999603, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9926065802574158]\n",
      " 4954 [D loss: 0.0003356024099048227, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9837369918823242]\n",
      " 4955 [D loss: 3.203505912097171e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9891981482505798]\n",
      " 4956 [D loss: 0.0005184190231375396, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9912684559822083]\n",
      " 4957 [D loss: 0.0007746696937829256, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9924683570861816]\n",
      " 4958 [D loss: 3.636927067418583e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9931979179382324]\n",
      " 4959 [D loss: 2.0649637008318678e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9946827292442322]\n",
      " 4960 [D loss: 0.00018369758618064225, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.994189977645874]\n",
      " 4961 [D loss: 1.9324099412187934e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9932655096054077]\n",
      " 4962 [D loss: 1.4269357961893547e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9942445755004883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4963 [D loss: 1.8994382116943598e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9915311336517334]\n",
      " 4964 [D loss: 4.0687300497666e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9905007481575012]\n",
      " 4965 [D loss: 1.7533031950733857e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9947231411933899]\n",
      " 4966 [D loss: 0.005522322840988636, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9965449571609497]\n",
      " 4967 [D loss: 0.013897828757762909, acc_real: 96.875000, acc_fake: 100.000000] [G loss: 0.9937169551849365]\n",
      " 4968 [D loss: 0.006890044081956148, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9948722124099731]\n",
      " 4969 [D loss: 0.006740693002939224, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9921370148658752]\n",
      " 4970 [D loss: 0.006438461132347584, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9897720813751221]\n",
      " 4971 [D loss: 3.1735075026517734e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9918215274810791]\n",
      " 4972 [D loss: 0.00021238303452264518, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9938568472862244]\n",
      " 4973 [D loss: 1.0527688573347405e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9852983951568604]\n",
      " 4974 [D loss: 0.00011203029134776443, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9900943040847778]\n",
      " 4975 [D loss: 0.00010318438580725342, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9839863181114197]\n",
      " 4976 [D loss: 0.013419164344668388, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.987298309803009]\n",
      " 4977 [D loss: 0.00010170369932893664, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9923979043960571]\n",
      " 4978 [D loss: 6.262520037125796e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9820210933685303]\n",
      " 4979 [D loss: 0.0006329487659968436, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9862031936645508]\n",
      " 4980 [D loss: 0.00024680697242729366, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9891433715820312]\n",
      " 4981 [D loss: 0.0005283732898533344, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9854079484939575]\n",
      " 4982 [D loss: 0.0007836190634407103, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9896392822265625]\n",
      " 4983 [D loss: 9.696278721094131e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9865002632141113]\n",
      " 4984 [D loss: 0.012568606995046139, acc_real: 100.000000, acc_fake: 96.875000] [G loss: 0.9895164966583252]\n",
      " 4985 [D loss: 3.5813071008306e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9919735789299011]\n",
      " 4986 [D loss: 0.00011846535198856145, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9932262897491455]\n",
      " 4987 [D loss: 7.40007744752802e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.987899661064148]\n",
      " 4988 [D loss: 1.1496640581754036e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9902349710464478]\n",
      " 4989 [D loss: 0.0027597839944064617, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.9923458695411682]\n",
      " 4990 [D loss: 9.674650755187031e-06, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9886090755462646]\n",
      " 4991 [D loss: 0.007215884514153004, acc_real: 98.437500, acc_fake: 100.000000] [G loss: 0.992987871170044]\n",
      " 4992 [D loss: 0.00010131862654816359, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9924068450927734]\n",
      " 4993 [D loss: 1.7446895071770996e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9898542165756226]\n",
      " 4994 [D loss: 6.491241219919175e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9904084801673889]\n",
      " 4995 [D loss: 0.0021961573511362076, acc_real: 100.000000, acc_fake: 98.437500] [G loss: 0.9927591681480408]\n",
      " 4996 [D loss: 0.00017723423661664128, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9869739413261414]\n",
      " 4997 [D loss: 0.0005186774069443345, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9889932870864868]\n",
      " 4998 [D loss: 1.703294037724845e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9940068125724792]\n",
      " 4999 [D loss: 5.623983815894462e-05, acc_real: 100.000000, acc_fake: 100.000000] [G loss: 0.9952685832977295]\n"
     ]
    }
   ],
   "source": [
    "gan = GANNumerical()\n",
    "gan.train(x_train, epochs=500, batch_size=16, save_model_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating New Data with GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 8)                 48        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 209\n",
      "Trainable params: 209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,077\n",
      "Trainable params: 981\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Data successfully saved to av\n"
     ]
    }
   ],
   "source": [
    "gan = GANNumerical()\n",
    "gan.load_model(version='4500')#version='v1'\n",
    "\n",
    "noise = np.random.normal(0, 1, (1000, 10))\n",
    "new_x = gan.generator.predict(noise)\n",
    "\n",
    "save_data(new_x, file_name = 'av')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.68801  ,  13.036514 ,  12.850748 ,  13.123866 ,  64.60991  ],\n",
       "       [  3.9416118,   6.3042984,  10.447666 ,   4.900977 , -93.915504 ],\n",
       "       [ -3.9672208,   4.543545 ,  -7.2935047,  -5.8123903, 117.29474  ],\n",
       "       ...,\n",
       "       [ 18.643938 ,  16.289673 ,  19.571909 ,  21.800507 ,  83.199005 ],\n",
       "       [ -2.6599102,   6.768923 ,   1.1811445,  -2.2925344,  10.352486 ],\n",
       "       [ 99.54113  ,  67.71621  ,  98.64039  , 117.01799  , 211.63351  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x63b705518>,\n",
       " <matplotlib.lines.Line2D at 0x63b705668>,\n",
       " <matplotlib.lines.Line2D at 0x63b7057b8>,\n",
       " <matplotlib.lines.Line2D at 0x63b705908>,\n",
       " <matplotlib.lines.Line2D at 0x63b705a58>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x63834d390>,\n",
       " <matplotlib.lines.Line2D at 0x63834d4e0>,\n",
       " <matplotlib.lines.Line2D at 0x63834d630>,\n",
       " <matplotlib.lines.Line2D at 0x63834d780>,\n",
       " <matplotlib.lines.Line2D at 0x63834d8d0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Original and Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for original dataset\n",
      "X1 Mean: 3943.0637353433845\n",
      "X1 Standard Deviation: 3963.4771089261694\n",
      "\n",
      "X2 Mean: 3939.515572305974\n",
      "X2 Standard Deviation: 3963.894401372093\n",
      "\n",
      "Statistics for generated dataset\n",
      "X1 Mean: 19.966136932373047\n",
      "X1 Standard Deviation: 18.86258888244629\n",
      "\n",
      "X2 Mean: 17.88066291809082\n",
      "X2 Standard Deviation: 12.044404029846191\n",
      "\n",
      "Number of duplicated data: 0\n"
     ]
    }
   ],
   "source": [
    "org_x = load_data('datanp')\n",
    "org_x1 = []\n",
    "org_x2 = []\n",
    "\n",
    "for xs in org_x:\n",
    "    org_x1.append(xs[0])\n",
    "    org_x2.append(xs[1])\n",
    "\n",
    "print('Statistics for original dataset')\n",
    "print('X1 Mean: {}\\nX1 Standard Deviation: {}\\n'.format(np.mean(org_x1), np.std(org_x1)))\n",
    "print('X2 Mean: {}\\nX2 Standard Deviation: {}\\n'.format(np.mean(org_x2), np.std(org_x2)))\n",
    "\n",
    "gen_x = load_data('av')\n",
    "gen_x1 = []\n",
    "gen_x2 = []\n",
    "\n",
    "for xs in gen_x:\n",
    "    gen_x1.append(xs[0])\n",
    "    gen_x2.append(xs[1])\n",
    "    \n",
    "print('Statistics for generated dataset')\n",
    "print('X1 Mean: {}\\nX1 Standard Deviation: {}\\n'.format(np.mean(gen_x1), np.std(gen_x1)))\n",
    "print('X2 Mean: {}\\nX2 Standard Deviation: {}\\n'.format(np.mean(gen_x2), np.std(gen_x2)))\n",
    "\n",
    "# Check if there are duplicate values of x1 and x2 between original and generated dataset\n",
    "\n",
    "dup_count = 0\n",
    "for idx_gen in range(0, len(gen_x)):\n",
    "    for idx_org in range(0, len(org_x)):\n",
    "        if gen_x1[idx_gen] == org_x1[idx_org] and gen_x2[idx_gen] == org_x2[idx_org]:\n",
    "            dup_count += 1\n",
    "            \n",
    "print('Number of duplicated data: {}'.format(dup_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Data + Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Numerical Data + Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002815729211789675\n",
      "1.0085201902544954\n",
      "5.011537331160059\n",
      "2.003526473070175\n",
      "7.045430071758438\n",
      "3.664700804324905\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "mu_x1 = 0\n",
    "sd_x1 = 1\n",
    "\n",
    "x1 = np.random.normal(mu_x1, sd_x1, 10000)\n",
    "\n",
    "print(np.mean(x1))\n",
    "print(np.std(x1))\n",
    "\n",
    "mu_x2 = 5\n",
    "sd_x2 = 2\n",
    "\n",
    "x2 = np.random.normal(mu_x2, sd_x2, 10000)\n",
    "\n",
    "print(np.mean(x2))\n",
    "print(np.std(x2))\n",
    "\n",
    "y_train = np.empty_like(x1)\n",
    "\n",
    "epsilon = np.random.normal(0, 1, 10000)\n",
    "for idx in range (len(y_train)):\n",
    "    y_train[idx] = 2*x1[idx]*x1[idx] + x2[idx] + epsilon[idx]\n",
    "\n",
    "x_train = np.stack((x1, x2))\n",
    "x_train = np.transpose(x_train)\n",
    "\n",
    "print(np.mean(y_train))\n",
    "print(np.std(y_train))\n",
    "print(y_train.shape)\n",
    "\n",
    "# save_data(x_train, file_name='numerical-label-train-x')\n",
    "# save_data(y_train, file_name='numerical-label-train-y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.026197153228951595\n",
      "0.9852285568294048\n",
      "5.02363987897211\n",
      "1.997252339126335\n",
      "6.9787391285606954\n",
      "3.561402703425572\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "mu_x1 = 0\n",
    "sd_x1 = 1\n",
    "\n",
    "x1 = np.random.normal(mu_x1, sd_x1, 5000)\n",
    "\n",
    "print(np.mean(x1))\n",
    "print(np.std(x1))\n",
    "\n",
    "mu_x2 = 5\n",
    "sd_x2 = 2\n",
    "\n",
    "x2 = np.random.normal(mu_x2, sd_x2, 5000)\n",
    "\n",
    "print(np.mean(x2))\n",
    "print(np.std(x2))\n",
    "\n",
    "y_test = np.empty_like(x1)\n",
    "\n",
    "epsilon = np.random.normal(0, 1, 5000)\n",
    "for idx in range (len(y_test)):\n",
    "    y_test[idx] = 2*x1[idx]*x1[idx] + x2[idx] + epsilon[idx]\n",
    "\n",
    "x_test = np.stack((x1, x2))\n",
    "x_test = np.transpose(x_test)\n",
    "\n",
    "print(np.mean(y_test))\n",
    "print(np.std(y_test))\n",
    "print(y_test.shape)\n",
    "\n",
    "# save_data(x_test, file_name='numerical-label-test-x')\n",
    "# save_data(y_test, file_name='numerical-label-test-y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier for Numerical Data + Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierNumerical():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_shape = (2,)\n",
    "        self.classifier = self.build_classifier()\n",
    "        \n",
    "        self.classifier.compile(optimizer='adam',\n",
    "                                loss='mean_squared_error',\n",
    "                                metrics=['mean_squared_error'])\n",
    "    \n",
    "    def save_model(self, version=None):\n",
    "        if version is None:\n",
    "            self.classifier.save('gan/models/toy-numerical-classifier.h5')\n",
    "        else:\n",
    "            self.classifier.save('gan/models/toy-numerical-classifier-{}.h5'.format(version))\n",
    "            \n",
    "    \n",
    "    def load_model(self, version=None):\n",
    "        if version is None:\n",
    "            self.classifier = load_model('gan/models/toy-numerical-classifier.h5')\n",
    "        else:\n",
    "            self.classifier = load_model('gan/models/toy-numerical-classifier-{}.h5'.format(version))\n",
    "\n",
    "            \n",
    "    def build_classifier(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32,\n",
    "                        activation='relu',\n",
    "                        input_shape=self.data_shape))\n",
    "        model.add(Dense(16,\n",
    "                        activation='tanh'))\n",
    "        model.add(Dense(8,\n",
    "                        activation='relu'))\n",
    "        model.add(Dense(4,\n",
    "                        activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, x_train, y_train, epoch_limit=10001, batch_size=128, version=''):\n",
    "        \n",
    "        history = self.classifier.fit(x_train, y_train,\n",
    "                                     batch_size=batch_size,\n",
    "                                     epochs=epoch_limit,\n",
    "                                     verbose=1)\n",
    "\n",
    "        self.save_model(version)\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        score = self.classifier.evaluate(x_test, y_test,\n",
    "                                         verbose=1)\n",
    "        print('Test Loss = {}, Test MSE = {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 801\n",
      "Trainable params: 801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_train = load_data('numerical-label-train-x')\n",
    "y_train = load_data('numerical-label-train-y')\n",
    "\n",
    "classifier = ClassifierNumerical()\n",
    "# classifier.train(x_train, y_train, epoch_limit=101, version='v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN for Numerical Data + Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANNumericalLabel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_shape = (3,)\n",
    "        self.noise_shape = (10,)\n",
    "\n",
    "        # Manually tune down learning rate to avoid oscillation\n",
    "        optimizer = Adam(lr=0.005, beta_1=0.5)\n",
    "\n",
    "        # -------------\n",
    "        # Discriminator\n",
    "        # -------------\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mean_squared_error',\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "        # ---------\n",
    "        # Generator\n",
    "        # ---------\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='mean_squared_error',\n",
    "                               optimizer=optimizer)\n",
    "        # --------\n",
    "        # Combined\n",
    "        # --------\n",
    "        # The combined model is created by stacking generator and discriminator.\n",
    "        # Noise ---Generator--> Generated Data ---Discriminator--> Validity\n",
    "        \n",
    "        z = Input(shape=self.noise_shape)\n",
    "        data = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        validity = self.discriminator(data)\n",
    "\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='mean_squared_error',\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "        \n",
    "    def save_model(self, version=None):\n",
    "        if version is None:\n",
    "            self.discriminator.save('gan/models/toy-numerical-label-discriminator.h5')\n",
    "            self.generator.save('gan/models/toy-numerical-label-generator.h5')\n",
    "            self.combined.save('gan/models/toy-numerical-label-combined.h5')\n",
    "        else:\n",
    "            self.discriminator.save('gan/models/toy-numerical-label-discriminator-{}.h5'.format(version))\n",
    "            self.generator.save('gan/models/toy-numerical-label-generator-{}.h5'.format(version))\n",
    "            self.combined.save('gan/models/toy-numerical-label-combined-{}.h5'.format(version))\n",
    "        \n",
    "        \n",
    "    def load_model(self, version=None): \n",
    "        if version is None:\n",
    "            self.discriminator = load_model('gan/models/toy-numerical-label-discriminator.h5')\n",
    "            self.generator = load_model('gan/models/toy-numerical-label-generator.h5')\n",
    "            self.combined = load_model('gan/models/toy-numerical-label-combined.h5')\n",
    "        else:\n",
    "            self.discriminator = load_model('gan/models/toy-numerical-label-discriminator-{}.h5'.format(version))\n",
    "            self.generator = load_model('gan/models/toy-numerical-label-generator-{}.h5'.format(version))\n",
    "            self.combined = load_model('gan/models/toy-numerical-label-combined-{}.h5'.format(version))\n",
    "            \n",
    "        \n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(4, input_shape=self.data_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        # Discriminator takes an image as an input and outputs its validity\n",
    "        data = Input(shape=self.data_shape)\n",
    "        validity = model(data)\n",
    "\n",
    "        return Model(data, validity)\n",
    "\n",
    "        \n",
    "    def build_generator(self):\n",
    "        # BatchNormalization maintains the mean activation close to 0\n",
    "        # and the activation standard deviation close to 1\n",
    "        model = Sequential()\n",
    "        model.add(Dense(8, input_shape=self.noise_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(4))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(3))\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        # Generator takes noise as an input and outputs an image\n",
    "        noise = Input(shape=self.noise_shape)\n",
    "        data = model(noise)\n",
    "\n",
    "        return Model(noise, data)\n",
    "    \n",
    "    \n",
    "    def train(self, train_data, epochs=50001, batch_size=128, save_model_interval=10000):\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # -------------------\n",
    "            # Train Discriminator\n",
    "            # -------------------\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, train_data.shape[0], half_batch)\n",
    "            data = train_data[idx]\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            noise = np.random.normal(0, 1, (half_batch, 10))\n",
    "            gen_data = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(data, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_data, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------\n",
    "            # Train Generator\n",
    "            # ---------------\n",
    "            noise = np.random.normal(0, 1, (batch_size, 10))\n",
    "\n",
    "            # The generator wants to fool the discriminator, hence trained with valid label (1)\n",
    "            # valid_y = np.array([1] * batch_size)\n",
    "            valid_y = np.ones((batch_size, 1))\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "\n",
    "            # Print progress\n",
    "            print (\"{:5d} [D loss: {}, acc_real: {:2f}, acc_fake: {:2f}] [G loss: {}]\".format(epoch, d_loss[0], 100*d_loss_real[1], 100*d_loss_fake[1], g_loss))\n",
    "\n",
    "            with open('gan/logs/toy-numerical-label-gan.log', 'a') as log_file:\n",
    "                log_file.write('{},{}\\n'.format(d_loss[0], g_loss))\n",
    "            \n",
    "            # Save models at save_interval\n",
    "            if epoch != 0 and epoch % save_model_interval == 0:\n",
    "                self.save_model(version=str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 65\n",
      "Trainable params: 65\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 187\n",
      "Trainable params: 163\n",
      "Non-trainable params: 24\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_train = load_data('numerical-label-train-x')\n",
    "y_train = load_data('numerical-label-train-y')\n",
    "\n",
    "expanded_y_train = np.expand_dims(y_train, axis=1)\n",
    "merged_train_data = np.concatenate((x_train, expanded_y_train), axis=1)\n",
    "\n",
    "gan = GANNumericalLabel()\n",
    "# gan.train(merged_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating New Numerical Data + Label with GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 65\n",
      "Trainable params: 65\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 187\n",
      "Trainable params: 163\n",
      "Non-trainable params: 24\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rionaldichandraseta/miniconda3/envs/gan-keras/lib/python3.6/site-packages/keras/engine/saving.py:304: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "gan = GANNumericalLabel()\n",
    "gan.load_model(version='v1')\n",
    "\n",
    "noise = np.random.normal(0, 1, (1000, 10))\n",
    "new_numerical_label_data = gan.generator.predict(noise)\n",
    "\n",
    "# save_data(new_numerical_label_data, file_name = 'numerical-label-generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Original and Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for original dataset\n",
      "X1 Mean: -0.009102463775850814\n",
      "X1 Standard Deviation: 0.9998558420656466\n",
      "\n",
      "X2 Mean: 5.006042971999659\n",
      "X2 Standard Deviation: 1.9744843293539256\n",
      "\n",
      "Y  Mean: 7.012985457040965\n",
      "Y  Standard Deviation: 3.5722964056680815\n",
      "\n",
      "Statistics for generated dataset\n",
      "X1 Mean: 0.0005220718448981643\n",
      "X1 Standard Deviation: 0.9610234498977661\n",
      "\n",
      "X2 Mean: 5.525527477264404\n",
      "X2 Standard Deviation: 1.6596297025680542\n",
      "\n",
      "Y  Mean: 7.321664333343506\n",
      "Y  Standard Deviation: 3.0317885875701904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "org_x = load_data('numerical-label-train-x')\n",
    "org_y = load_data('numerical-label-train-y')\n",
    "org_x1 = []\n",
    "org_x2 = []\n",
    "\n",
    "for xs in org_x:\n",
    "    org_x1.append(xs[0])\n",
    "    org_x2.append(xs[1])\n",
    "\n",
    "print('Statistics for original dataset')\n",
    "print('X1 Mean: {}\\nX1 Standard Deviation: {}\\n'.format(np.mean(org_x1), np.std(org_x1)))\n",
    "print('X2 Mean: {}\\nX2 Standard Deviation: {}\\n'.format(np.mean(org_x2), np.std(org_x2)))\n",
    "print('Y  Mean: {}\\nY  Standard Deviation: {}\\n'.format(np.mean(org_y), np.std(org_y)))\n",
    "\n",
    "gen_data = load_data('numerical-label-generated')\n",
    "gen_x1 = []\n",
    "gen_x2 = []\n",
    "gen_y = []\n",
    "\n",
    "for data in gen_data:\n",
    "    gen_x1.append(data[0])\n",
    "    gen_x2.append(data[1])\n",
    "    gen_y.append(data[2])\n",
    "    \n",
    "print('Statistics for generated dataset')\n",
    "print('X1 Mean: {}\\nX1 Standard Deviation: {}\\n'.format(np.mean(gen_x1), np.std(gen_x1)))\n",
    "print('X2 Mean: {}\\nX2 Standard Deviation: {}\\n'.format(np.mean(gen_x2), np.std(gen_x2)))\n",
    "print('Y  Mean: {}\\nY  Standard Deviation: {}\\n'.format(np.mean(gen_y), np.std(gen_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training New Classifier with GAN-generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 801\n",
      "Trainable params: 801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/101\n",
      "10000/10000 [==============================] - 2s 177us/step - loss: 35.6012 - mean_squared_error: 35.6012\n",
      "Epoch 2/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 10.7387 - mean_squared_error: 10.7387\n",
      "Epoch 3/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 8.8549 - mean_squared_error: 8.8549\n",
      "Epoch 4/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 5.9132 - mean_squared_error: 5.9132\n",
      "Epoch 5/101\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 3.4543 - mean_squared_error: 3.4543\n",
      "Epoch 6/101\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 2.5556 - mean_squared_error: 2.5556\n",
      "Epoch 7/101\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 2.1655 - mean_squared_error: 2.1655\n",
      "Epoch 8/101\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 1.9150 - mean_squared_error: 1.9150\n",
      "Epoch 9/101\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 1.7373 - mean_squared_error: 1.7373\n",
      "Epoch 10/101\n",
      "10000/10000 [==============================] - 0s 21us/step - loss: 1.6018 - mean_squared_error: 1.6018\n",
      "Epoch 11/101\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 1.4833 - mean_squared_error: 1.4833\n",
      "Epoch 12/101\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 1.3958 - mean_squared_error: 1.3958\n",
      "Epoch 13/101\n",
      "10000/10000 [==============================] - 0s 17us/step - loss: 1.3480 - mean_squared_error: 1.3480\n",
      "Epoch 14/101\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 1.2811 - mean_squared_error: 1.2811\n",
      "Epoch 15/101\n",
      "10000/10000 [==============================] - 0s 18us/step - loss: 1.2635 - mean_squared_error: 1.2635\n",
      "Epoch 16/101\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.2249 - mean_squared_error: 1.2249\n",
      "Epoch 17/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1765 - mean_squared_error: 1.1765\n",
      "Epoch 18/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1763 - mean_squared_error: 1.1763\n",
      "Epoch 19/101\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.1526 - mean_squared_error: 1.1526\n",
      "Epoch 20/101\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.1607 - mean_squared_error: 1.1607\n",
      "Epoch 21/101\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 1.1360 - mean_squared_error: 1.1360\n",
      "Epoch 22/101\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.1208 - mean_squared_error: 1.1208\n",
      "Epoch 23/101\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.1140 - mean_squared_error: 1.1140\n",
      "Epoch 24/101\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.1040 - mean_squared_error: 1.1040\n",
      "Epoch 25/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1028 - mean_squared_error: 1.1028\n",
      "Epoch 26/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1112 - mean_squared_error: 1.1112\n",
      "Epoch 27/101\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.1280 - mean_squared_error: 1.1280\n",
      "Epoch 28/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1033 - mean_squared_error: 1.1033\n",
      "Epoch 29/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0895 - mean_squared_error: 1.0895\n",
      "Epoch 30/101\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0953 - mean_squared_error: 1.0953\n",
      "Epoch 31/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0934 - mean_squared_error: 1.0934\n",
      "Epoch 32/101\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0861 - mean_squared_error: 1.0861\n",
      "Epoch 33/101\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0745 - mean_squared_error: 1.0745\n",
      "Epoch 34/101\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0820 - mean_squared_error: 1.0820\n",
      "Epoch 35/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0805 - mean_squared_error: 1.0805\n",
      "Epoch 36/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0847 - mean_squared_error: 1.0847\n",
      "Epoch 37/101\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0859 - mean_squared_error: 1.0859\n",
      "Epoch 38/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0755 - mean_squared_error: 1.0755\n",
      "Epoch 39/101\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0779 - mean_squared_error: 1.0779\n",
      "Epoch 40/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0743 - mean_squared_error: 1.0743\n",
      "Epoch 41/101\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.0810 - mean_squared_error: 1.0810\n",
      "Epoch 42/101\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0891 - mean_squared_error: 1.0891\n",
      "Epoch 43/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0888 - mean_squared_error: 1.0888\n",
      "Epoch 44/101\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0681 - mean_squared_error: 1.0681\n",
      "Epoch 45/101\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0594 - mean_squared_error: 1.0594\n",
      "Epoch 46/101\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 1.0720 - mean_squared_error: 1.0720\n",
      "Epoch 47/101\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 1.0800 - mean_squared_error: 1.0800\n",
      "Epoch 48/101\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 1.0685 - mean_squared_error: 1.0685\n",
      "Epoch 49/101\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 1.0664 - mean_squared_error: 1.0664\n",
      "Epoch 50/101\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 1.0615 - mean_squared_error: 1.0615\n",
      "Epoch 51/101\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 1.0750 - mean_squared_error: 1.0750\n",
      "Epoch 52/101\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 1.0695 - mean_squared_error: 1.0695\n",
      "Epoch 53/101\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.0637 - mean_squared_error: 1.0637\n",
      "Epoch 54/101\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 1.0656 - mean_squared_error: 1.0656\n",
      "Epoch 55/101\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 1.0646 - mean_squared_error: 1.0646\n",
      "Epoch 56/101\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 1.0619 - mean_squared_error: 1.0619\n",
      "Epoch 57/101\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 1.0815 - mean_squared_error: 1.0815\n",
      "Epoch 58/101\n",
      "10000/10000 [==============================] - 0s 16us/step - loss: 1.0543 - mean_squared_error: 1.0543\n",
      "Epoch 59/101\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 1.0602 - mean_squared_error: 1.0602\n",
      "Epoch 60/101\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 1.0717 - mean_squared_error: 1.0717\n",
      "Epoch 61/101\n",
      "10000/10000 [==============================] - 0s 17us/step - loss: 1.0651 - mean_squared_error: 1.0651\n",
      "Epoch 62/101\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 1.0522 - mean_squared_error: 1.0522\n",
      "Epoch 63/101\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0488 - mean_squared_error: 1.0488\n",
      "Epoch 64/101\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 1.0648 - mean_squared_error: 1.0648\n",
      "Epoch 65/101\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0618 - mean_squared_error: 1.0618\n",
      "Epoch 66/101\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0640 - mean_squared_error: 1.0640\n",
      "Epoch 67/101\n",
      "10000/10000 [==============================] - 0s 16us/step - loss: 1.0516 - mean_squared_error: 1.0516\n",
      "Epoch 68/101\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 1.0551 - mean_squared_error: 1.0551\n",
      "Epoch 69/101\n",
      "10000/10000 [==============================] - 0s 18us/step - loss: 1.0677 - mean_squared_error: 1.0677\n",
      "Epoch 70/101\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 1.0512 - mean_squared_error: 1.0512\n",
      "Epoch 71/101\n",
      "10000/10000 [==============================] - 0s 16us/step - loss: 1.0525 - mean_squared_error: 1.0525\n",
      "Epoch 72/101\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 1.0486 - mean_squared_error: 1.0486\n",
      "Epoch 73/101\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 1.0555 - mean_squared_error: 1.0555\n",
      "Epoch 74/101\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 1.0657 - mean_squared_error: 1.0657\n",
      "Epoch 75/101\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 1.0728 - mean_squared_error: 1.0728\n",
      "Epoch 76/101\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 1.0513 - mean_squared_error: 1.0513\n",
      "Epoch 77/101\n",
      "10000/10000 [==============================] - 0s 16us/step - loss: 1.0574 - mean_squared_error: 1.0574\n",
      "Epoch 78/101\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 1.0501 - mean_squared_error: 1.0501\n",
      "Epoch 79/101\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 1.0647 - mean_squared_error: 1.0647\n",
      "Epoch 80/101\n",
      "10000/10000 [==============================] - 0s 19us/step - loss: 1.0670 - mean_squared_error: 1.0670\n",
      "Epoch 81/101\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 1.0529 - mean_squared_error: 1.0529\n",
      "Epoch 82/101\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 1.0434 - mean_squared_error: 1.0434\n",
      "Epoch 83/101\n",
      "10000/10000 [==============================] - 0s 16us/step - loss: 1.0645 - mean_squared_error: 1.0645\n",
      "Epoch 84/101\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 1.0540 - mean_squared_error: 1.0540\n",
      "Epoch 85/101\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 1.0513 - mean_squared_error: 1.0513\n",
      "Epoch 86/101\n",
      "10000/10000 [==============================] - 0s 21us/step - loss: 1.0593 - mean_squared_error: 1.0593\n",
      "Epoch 87/101\n",
      "10000/10000 [==============================] - 0s 18us/step - loss: 1.0460 - mean_squared_error: 1.0460\n",
      "Epoch 88/101\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 1.0484 - mean_squared_error: 1.0484\n",
      "Epoch 89/101\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 1.0553 - mean_squared_error: 1.0553\n",
      "Epoch 90/101\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 1.0452 - mean_squared_error: 1.0452\n",
      "Epoch 91/101\n",
      "10000/10000 [==============================] - 0s 18us/step - loss: 1.0530 - mean_squared_error: 1.0530\n",
      "Epoch 92/101\n",
      "10000/10000 [==============================] - 0s 19us/step - loss: 1.0459 - mean_squared_error: 1.0459\n",
      "Epoch 93/101\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 1.0657 - mean_squared_error: 1.0657\n",
      "Epoch 94/101\n",
      "10000/10000 [==============================] - 0s 17us/step - loss: 1.0491 - mean_squared_error: 1.0491\n",
      "Epoch 95/101\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 1.0465 - mean_squared_error: 1.0465\n",
      "Epoch 96/101\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0482 - mean_squared_error: 1.0482\n",
      "Epoch 97/101\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 1.0416 - mean_squared_error: 1.0416\n",
      "Epoch 98/101\n",
      "10000/10000 [==============================] - 0s 17us/step - loss: 1.0430 - mean_squared_error: 1.0430\n",
      "Epoch 99/101\n",
      "10000/10000 [==============================] - 0s 21us/step - loss: 1.0529 - mean_squared_error: 1.0529\n",
      "Epoch 100/101\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 1.0594 - mean_squared_error: 1.0594\n",
      "Epoch 101/101\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.0521 - mean_squared_error: 1.0521\n"
     ]
    }
   ],
   "source": [
    "# Merge original and generated dataset\n",
    "\n",
    "np_org_x1 = np.asarray(org_x1)\n",
    "np_org_x2 = np.asarray(org_x2)\n",
    "np_org_y = np.asarray(org_y)\n",
    "\n",
    "np_gen_x1 = np.asarray(gen_x1)\n",
    "np_gen_x2 = np.asarray(gen_x2)\n",
    "np_gen_y = np.asarray(gen_y)\n",
    "\n",
    "combined_x1 = np.concatenate((np_org_x1, np_gen_x1))\n",
    "combined_x2 = np.concatenate((np_org_x2, np_gen_x2))\n",
    "combined_y = np.concatenate((org_y, gen_y))\n",
    "\n",
    "combined_x1 = np.expand_dims(combined_x1, axis=1)\n",
    "combined_x2 = np.expand_dims(combined_x2, axis=1)\n",
    "combined_y = np.expand_dims(combined_y, axis=1)\n",
    "combined_training_data = np.concatenate((combined_x1, combined_x2, combined_y), axis=1)\n",
    "\n",
    "# Train new classifier\n",
    "\n",
    "new_classifier = ClassifierNumerical()\n",
    "new_classifier.train(x_train, y_train, epoch_limit=101, version='with-gan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Classifier with and without GAN-generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 801\n",
      "Trainable params: 801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5000/5000 [==============================] - 1s 131us/step\n",
      "Test Loss = 1.0211943440437317, Test MSE = 1.0211943440437317\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 801\n",
      "Trainable params: 801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5000/5000 [==============================] - 0s 85us/step\n",
      "Test Loss = 1.010679329586029, Test MSE = 1.010679329586029\n"
     ]
    }
   ],
   "source": [
    "x_test = load_data('numerical-label-test-x')\n",
    "y_test = load_data('numerical-label-test-y')\n",
    "\n",
    "classifier = ClassifierNumerical()\n",
    "classifier.load_model('v1')\n",
    "classifier.evaluate(x_test, y_test)\n",
    "\n",
    "new_classifier = ClassifierNumerical()\n",
    "new_classifier.load_model('with-gan')\n",
    "new_classifier.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation score **improves slightly** by using the GAN-generated dataset as training data, the error dropped around 0.033."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating One-Hot Encoded Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_categorical_data(mean, std, n_categories, n_data):\n",
    "    \n",
    "    distribution = np.random.normal(mean, std, n_data)\n",
    "    \n",
    "    # Around 99.7% datalies between 3 times standard deviation\n",
    "    lower_bound = mean - (3 * std)\n",
    "    upper_bound = mean + (3 * std)\n",
    "    \n",
    "    data_range = upper_bound - lower_bound\n",
    "    category_range = data_range / n_categories\n",
    "    \n",
    "    categorical_data = []\n",
    "    \n",
    "    for data in distribution:\n",
    "        category = int((data - lower_bound) / category_range)\n",
    "        if category < 0:\n",
    "            category = 0\n",
    "        if category >= n_categories:\n",
    "            category = n_categories - 1\n",
    "            \n",
    "        ohe_data = []\n",
    "        \n",
    "        for i in range (n_categories):\n",
    "            if i == category:\n",
    "                ohe_data.append(1)\n",
    "            else:\n",
    "                ohe_data.append(0)\n",
    "            \n",
    "        categorical_data.append(ohe_data)\n",
    "    \n",
    "    categorical_data = np.asarray(categorical_data)\n",
    "    \n",
    "    return categorical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[339, 2422, 4460, 2398, 381]\n",
      "[5018, 4982, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "feature1 = generate_categorical_data(0, 1, 5, 10000)\n",
    "feature2 = generate_categorical_data(5, 2, 2, 10000)\n",
    "\n",
    "combined_feature = np.concatenate((feature1, feature2), axis=1)\n",
    "# save_data(combined_feature, file_name='categorical-original')\n",
    "\n",
    "\n",
    "split_feature_1, split_feature_2 = np.split(combined_feature, [5], axis=1)\n",
    "\n",
    "count = [0, 0, 0, 0, 0]\n",
    "for c in split_feature_1:\n",
    "    for idx, i in enumerate(c):\n",
    "        if i == 1:\n",
    "            count[idx] += 1\n",
    "            \n",
    "print(count)\n",
    "\n",
    "count = [0, 0, 0, 0, 0]\n",
    "for c in split_feature_2:\n",
    "    for idx, i in enumerate(c):\n",
    "        if i == 1:\n",
    "            count[idx] += 1\n",
    "            \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN for Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANCategorical():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_shape = (7,)\n",
    "        self.noise_shape = (20,)\n",
    "\n",
    "        # Manually tune down learning rate to avoid oscillation\n",
    "        optimizer = Adam(lr=0.005, beta_1=0.5)\n",
    "\n",
    "        # -------------\n",
    "        # Discriminator\n",
    "        # -------------\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mean_squared_error',\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "        \n",
    "        # ---------\n",
    "        # Generator\n",
    "        # ---------\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='mean_squared_error',\n",
    "                               optimizer=optimizer)\n",
    "        \n",
    "\n",
    "        # --------\n",
    "        # Combined\n",
    "        # --------\n",
    "        # The combined model is created by stacking generator and discriminator.\n",
    "        # Noise ---Generator--> Generated Data ---Discriminator--> Validity\n",
    "\n",
    "        # From Generator\n",
    "        z = Input(shape=self.noise_shape)\n",
    "        data = self.generator(z)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        validity = self.discriminator(data)\n",
    "\n",
    "        self.combined = Model(inputs=z, outputs=validity)\n",
    "        self.combined.compile(loss='mean_squared_error',\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "        \n",
    "    def save_model(self, version=None):\n",
    "        if version is None:\n",
    "            self.discriminator.save('gan/models/toy-categorical-discriminator.h5')\n",
    "            self.generator.save('gan/models/toy-categorical-generator.h5')\n",
    "            self.combined.save('gan/models/toy-categorical-combined.h5')\n",
    "        else:\n",
    "            self.discriminator.save('gan/models/toy-categorical-discriminator-{}.h5'.format(version))\n",
    "            self.generator.save('gan/models/toy-categorical-generator-{}.h5'.format(version))\n",
    "            self.combined.save('gan/models/toy-categorical-combined-{}.h5'.format(version))\n",
    "        \n",
    "        \n",
    "    def load_model(self, version=None): \n",
    "        if version is None:\n",
    "            self.discriminator = load_model('gan/models/toy-categorical-discriminator.h5')\n",
    "            self.generator = load_model('gan/models/toy-categorical-generator.h5')\n",
    "            self.combined = load_model('gan/models/toy-categorical-combined.h5')\n",
    "        else:\n",
    "            self.discriminator = load_model('gan/models/toy-categorical-discriminator-{}.h5'.format(version))\n",
    "            self.generator = load_model('gan/models/toy-categorical-generator-{}.h5'.format(version))\n",
    "            self.combined = load_model('gan/models/toy-categorical-combined-{}.h5'.format(version))\n",
    "            \n",
    "        \n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_shape=self.data_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(6))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        # Discriminator takes an image as an input and outputs its validity\n",
    "        data = Input(shape=self.data_shape)\n",
    "        validity = model(data)\n",
    "\n",
    "        return Model(data, validity)\n",
    "\n",
    "        \n",
    "    def build_generator(self):\n",
    "        # BatchNormalization maintains the mean activation close to 0\n",
    "        # and the activation standard deviation close to 1\n",
    "        noise = Input(shape=self.noise_shape)\n",
    "        \n",
    "        hidden_1 = Dense(18)(noise)\n",
    "        hidden_1 = LeakyReLU(alpha=0.2)(hidden_1)\n",
    "        hidden_1 = BatchNormalization(momentum=0.8)(hidden_1)\n",
    "        \n",
    "        hidden_2 = Dense(14)(hidden_1)\n",
    "        hidden_2 = LeakyReLU(alpha=0.2)(hidden_2)\n",
    "        hidden_2 = BatchNormalization(momentum=0.8)(hidden_2)\n",
    "        \n",
    "        branch_1_hidden_1 = Dense(12)(hidden_2)\n",
    "        branch_1_hidden_1 = LeakyReLU(alpha=0.2)(branch_1_hidden_1)\n",
    "        branch_1_hidden_1 = BatchNormalization(momentum=0.8)(branch_1_hidden_1)\n",
    "        \n",
    "        branch_1_hidden_2 = Dense(7)(branch_1_hidden_1)\n",
    "        branch_1_hidden_2 = LeakyReLU(alpha=0.2)(branch_1_hidden_2)\n",
    "        branch_1_hidden_2 = BatchNormalization(momentum=0.8)(branch_1_hidden_2)\n",
    "        \n",
    "        branch_1_output = Dense(5, activation='softmax')(branch_1_hidden_2)\n",
    "        \n",
    "        branch_2_hidden_1 = Dense(8)(hidden_2)\n",
    "        branch_2_hidden_1 = LeakyReLU(alpha=0.2)(branch_2_hidden_1)\n",
    "        branch_2_hidden_1 = BatchNormalization(momentum=0.8)(branch_2_hidden_1)\n",
    "        \n",
    "        branch_2_hidden_2 = Dense(4)(branch_2_hidden_1)\n",
    "        branch_2_hidden_2 = LeakyReLU(alpha=0.2)(branch_2_hidden_2)\n",
    "        branch_2_hidden_2 = BatchNormalization(momentum=0.8)(branch_2_hidden_2)\n",
    "        \n",
    "        branch_2_output = Dense(2, activation='softmax')(branch_2_hidden_2)\n",
    "        \n",
    "        merged_output = concatenate([branch_1_output, branch_2_output])\n",
    "        \n",
    "        return Model(inputs=noise, outputs=merged_output)\n",
    "    \n",
    "    \n",
    "    def train(self, train_data, epochs=50001, batch_size=128, save_model_interval=10000):\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # -------------------\n",
    "            # Train Discriminator\n",
    "            # -------------------\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, train_data.shape[0], half_batch)\n",
    "            data = train_data[idx]\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            noise = np.random.normal(0, 1, (half_batch, 20))\n",
    "            gen_data = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(data, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_data, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------\n",
    "            # Train Generator\n",
    "            # ---------------\n",
    "            noise = np.random.normal(0, 1, (batch_size, 20))\n",
    "\n",
    "            # The generator wants to fool the discriminator, hence trained with valid label (1)\n",
    "            # valid_y = np.array([1] * batch_size)\n",
    "            valid_y = np.ones((batch_size, 1))\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "\n",
    "            # Print progress\n",
    "            print (\"{:5d} [D loss: {}, acc_real: {:2f}, acc_fake: {:2f}] [G loss: {}]\".format(epoch, d_loss[0], 100*d_loss_real[1], 100*d_loss_fake[1], g_loss))\n",
    "\n",
    "            with open('gan/logs/toy-categorical-gan.log', 'a') as log_file:\n",
    "                log_file.write('{},{}\\n'.format(d_loss[0], g_loss))\n",
    "            \n",
    "            # Save models at save_interval\n",
    "            if epoch != 0 and epoch % save_model_interval == 0:\n",
    "                self.save_model(version=str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 12)                96        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 181\n",
      "Trainable params: 181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "categorical_data = load_data('categorical-original')\n",
    "\n",
    "gan = GANCategorical()\n",
    "# gan.train(categorical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating New Categorical Data with GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_56 (Dense)             (None, 12)                96        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 181\n",
      "Trainable params: 181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rionaldichandraseta/miniconda3/envs/gan-keras/lib/python3.6/site-packages/keras/engine/saving.py:304: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 257, 453, 236, 14]\n",
      "[548, 428, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "noise = np.random.normal(0, 1, (1000, 20))\n",
    "\n",
    "gan = GANCategorical()\n",
    "gan.load_model('v1')\n",
    "\n",
    "generated_feature = gan.generator.predict(noise)\n",
    "\n",
    "split_generated_feature_1, split_generated_feature_2 = np.split(generated_feature, [5], axis=1)\n",
    "\n",
    "count = [0, 0, 0, 0, 0]\n",
    "for c in split_generated_feature_1:\n",
    "    for idx, i in enumerate(c):\n",
    "        if i == 1:\n",
    "            count[idx] += 1\n",
    "            \n",
    "print(count)\n",
    "\n",
    "count = [0, 0, 0, 0, 0]\n",
    "for c in split_generated_feature_2:\n",
    "    for idx, i in enumerate(c):\n",
    "        if i == 1:\n",
    "            count[idx] += 1\n",
    "            \n",
    "print(count)\n",
    "\n",
    "generated_feature = generated_feature.astype(int)\n",
    "# save_data(generated_feature, file_name='categorical-generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data + Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Categorical Data + Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n",
      "(10000, 2)\n",
      "[2747, 5509, 1744]\n"
     ]
    }
   ],
   "source": [
    "# Generate label for the categorical data\n",
    "\n",
    "categorical_data = load_data('categorical-original')\n",
    "\n",
    "feature_1, feature_2 = np.split(categorical_data, [5], axis=1)\n",
    "\n",
    "print(feature_1.shape)\n",
    "print(feature_2.shape)\n",
    "\n",
    "epsilon = np.random.normal(0, 1, 10000)\n",
    "label = []\n",
    "for idx in range(feature_1.shape[0]):\n",
    "    value = 0\n",
    "    for idx_f1 in range(feature_1.shape[1]):\n",
    "        if feature_1[idx][idx_f1] == 1:\n",
    "            value += idx_f1 * 3\n",
    "    \n",
    "    for idx_f2 in range(feature_2.shape[1]):\n",
    "        if feature_2[idx][idx_f2] == 1:\n",
    "            value -= idx_f2 * 5\n",
    "            \n",
    "    value += epsilon[idx]\n",
    "    \n",
    "    if value < 1:\n",
    "        label.append([1,0,0])\n",
    "    elif 1 <= value <= 7:\n",
    "        label.append([0,1,0])\n",
    "    else:\n",
    "        label.append([0,0,1])\n",
    "        \n",
    "label = np.asarray(label)\n",
    "\n",
    "count = [0,0,0]\n",
    "for l in label:\n",
    "    for idx, i in enumerate(l):\n",
    "        if i == 1:\n",
    "            count[idx] += 1\n",
    "            \n",
    "print(count)\n",
    "\n",
    "# save_data(categorical_data, file_name='categorical-label-train-x')\n",
    "# save_data(label, file_name='categorical-label-train-y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decided to skip on the GAN and classifier for Categorical + Label and head straight to Mixed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed (Categorical + Numerical + Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Mixed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3110, 4541, 2349]\n",
      "Data successfully saved to gan/gen-data/mix-label-train-x\n",
      "Data successfully saved to gan/gen-data/mix-label-train-y\n",
      "[1559, 2235, 1206]\n",
      "Data successfully saved to gan/gen-data/mix-label-test-x\n",
      "Data successfully saved to gan/gen-data/mix-label-test-y\n"
     ]
    }
   ],
   "source": [
    "# Generate training data for mixed dataset\n",
    "n_data_train = 10000\n",
    "\n",
    "train_categorical_1 = generate_categorical_data(0, 1, 5, n_data_train)\n",
    "train_categorical_2 = generate_categorical_data(5, 2, 2, n_data_train)\n",
    "\n",
    "train_numerical_1 = np.random.normal(0, 1, n_data_train)\n",
    "train_numerical_2 = np.random.normal(5, 2, n_data_train)\n",
    "\n",
    "epsilon = np.random.normal(0, 1, n_data_train)\n",
    "train_label = []\n",
    "for idx in range(n_data_train):\n",
    "    value = 0\n",
    "    for idx_c1 in range(train_categorical_1.shape[1]):\n",
    "        if train_categorical_1[idx][idx_c1] == 1:\n",
    "            value += idx_c1 * 3\n",
    "    \n",
    "    for idx_c2 in range(train_categorical_2.shape[1]):\n",
    "        if train_categorical_2[idx][idx_c2] == 1:\n",
    "            value -= idx_f2 * 5\n",
    "            \n",
    "    value += train_numerical_1[idx] * 4\n",
    "    value += train_numerical_2[idx] / 2\n",
    "            \n",
    "    value += epsilon[idx]\n",
    "    \n",
    "    if value < 1:\n",
    "        train_label.append([1,0,0])\n",
    "    elif 1 <= value <= 7:\n",
    "        train_label.append([0,1,0])\n",
    "    else:\n",
    "        train_label.append([0,0,1])\n",
    "        \n",
    "train_label = np.asarray(train_label)\n",
    "\n",
    "count = [0,0,0]\n",
    "for label in train_label:\n",
    "    for idx, i in enumerate(label):\n",
    "        if i == 1:\n",
    "            count[idx] += 1\n",
    "            \n",
    "print(count)\n",
    "\n",
    "train_numerical_1 = np.expand_dims(train_numerical_1, axis=1)\n",
    "train_numerical_2 = np.expand_dims(train_numerical_2, axis=1)\n",
    "\n",
    "train_data = np.concatenate((train_categorical_1, train_categorical_2, train_numerical_1, train_numerical_2), axis=1)\n",
    "\n",
    "save_data(train_data, file_name='mix-label-train-x')\n",
    "save_data(train_label, file_name='mix-label-train-y')\n",
    "\n",
    "# Generate test data for mixed dataset\n",
    "n_data_test = 5000\n",
    "\n",
    "test_categorical_1 = generate_categorical_data(0, 1, 5, n_data_test)\n",
    "test_categorical_2 = generate_categorical_data(5, 2, 2, n_data_test)\n",
    "\n",
    "test_numerical_1 = np.random.normal(0, 1, n_data_test)\n",
    "test_numerical_2 = np.random.normal(5, 2, n_data_test)\n",
    "\n",
    "epsilon = np.random.normal(0, 1, n_data_test)\n",
    "test_label = []\n",
    "for idx in range(n_data_test):\n",
    "    value = 0\n",
    "    for idx_c1 in range(test_categorical_1.shape[1]):\n",
    "        if test_categorical_1[idx][idx_c1] == 1:\n",
    "            value += idx_c1 * 3\n",
    "    \n",
    "    for idx_c2 in range(test_categorical_2.shape[1]):\n",
    "        if test_categorical_2[idx][idx_c2] == 1:\n",
    "            value -= idx_f2 * 5\n",
    "            \n",
    "    value += test_numerical_1[idx] * 4\n",
    "    value += test_numerical_2[idx] / 2\n",
    "            \n",
    "    value += epsilon[idx]\n",
    "    \n",
    "    if value < 1:\n",
    "        test_label.append([1,0,0])\n",
    "    elif 1 <= value <= 7:\n",
    "        test_label.append([0,1,0])\n",
    "    else:\n",
    "        test_label.append([0,0,1])\n",
    "        \n",
    "test_label = np.asarray(test_label)\n",
    "\n",
    "count = [0,0,0]\n",
    "for label in test_label:\n",
    "    for idx, i in enumerate(label):\n",
    "        if i == 1:\n",
    "            count[idx] += 1\n",
    "            \n",
    "print(count)\n",
    "\n",
    "test_numerical_1 = np.expand_dims(test_numerical_1, axis=1)\n",
    "test_numerical_2 = np.expand_dims(test_numerical_2, axis=1)\n",
    "\n",
    "test_data = np.concatenate((test_categorical_1, test_categorical_2, test_numerical_1, test_numerical_2), axis=1)\n",
    "save_data(test_data, file_name='mix-label-test-x')\n",
    "save_data(test_label, file_name='mix-label-test-y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier for Mixed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierMix():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_shape = (9,)\n",
    "        self.classifier = self.build_classifier()\n",
    "        \n",
    "        self.classifier.compile(optimizer='adam',\n",
    "                                loss='categorical_crossentropy',\n",
    "                                metrics=['accuracy'])\n",
    "    \n",
    "    def save_model(self, version=None):\n",
    "        if version is None:\n",
    "            self.classifier.save('gan/models/toy-mix-classifier.h5')\n",
    "        else:\n",
    "            self.classifier.save('gan/models/toy-mix-classifier-{}.h5'.format(version))\n",
    "            \n",
    "    \n",
    "    def load_model(self, version=None):\n",
    "        if version is None:\n",
    "            self.classifier = load_model('gan/models/toy-mix-classifier.h5')\n",
    "        else:\n",
    "            self.classifier = load_model('gan/models/toy-mix-classifier-{}.h5'.format(version))\n",
    "\n",
    "            \n",
    "    def build_classifier(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, \n",
    "                        activation='relu',\n",
    "                        input_shape=self.input_shape))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(64,\n",
    "                        activation='tanh'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(32,\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(16,\n",
    "                        activation='tanh'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, x_train, y_train, epoch_limit=10001, batch_size=128, version=''):\n",
    "        \n",
    "        history = self.classifier.fit(x_train, y_train,\n",
    "                                     batch_size=batch_size,\n",
    "                                     epochs=epoch_limit,\n",
    "                                     verbose=1)\n",
    "\n",
    "        self.save_model(version)\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        score = self.classifier.evaluate(x_test, y_test,\n",
    "                                         verbose=1)\n",
    "        print('Test Loss = {}, Test Accuracy = {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_138 (Dense)            (None, 128)               1280      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 12,195\n",
      "Trainable params: 12,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_train = load_data('mix-label-train-x')\n",
    "y_train = load_data('mix-label-train-y')\n",
    "\n",
    "classifier_mix = ClassifierMix()\n",
    "# classifier_mix.train(x_train, y_train, epoch_limit=1000, version='v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN for Mixed Data + Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMixWithLabel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_shape = (12,)\n",
    "            # Categorical 5\n",
    "            # Categorical 2\n",
    "            # Numerical 1\n",
    "            # Numerical 1\n",
    "            # Label 3\n",
    "        self.noise_shape = (20,)\n",
    "\n",
    "        # Manually tune down learning rate to avoid oscillation\n",
    "        optimizer = Adam(lr=0.005, beta_1=0.5)\n",
    "\n",
    "        # -------------\n",
    "        # Discriminator\n",
    "        # -------------\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mean_squared_error',\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "        \n",
    "        # ---------\n",
    "        # Generator\n",
    "        # ---------\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='mean_squared_error',\n",
    "                               optimizer=optimizer)\n",
    "        \n",
    "\n",
    "        # --------\n",
    "        # Combined\n",
    "        # --------\n",
    "        # The combined model is created by stacking generator and discriminator.\n",
    "        # Noise ---Generator--> Generated Data ---Discriminator--> Validity\n",
    "\n",
    "        # From Generator\n",
    "        z = Input(shape=self.noise_shape)\n",
    "        data = self.generator(z)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        validity = self.discriminator(data)\n",
    "\n",
    "        self.combined = Model(inputs=z, outputs=validity)\n",
    "        self.combined.compile(loss='mean_squared_error',\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "        \n",
    "    def save_model(self, version=None):\n",
    "        if version is None:\n",
    "            self.discriminator.save('gan/models/toy-mix-label-discriminator.h5')\n",
    "            self.generator.save('gan/models/toy-mix-label-generator.h5')\n",
    "            self.combined.save('gan/models/toy-mix-label-combined.h5')\n",
    "        else:\n",
    "            self.discriminator.save('gan/models/toy-mix-label-discriminator-{}.h5'.format(version))\n",
    "            self.generator.save('gan/models/toy-mix-label-generator-{}.h5'.format(version))\n",
    "            self.combined.save('gan/models/toy-mix-label-combined-{}.h5'.format(version))\n",
    "        \n",
    "        \n",
    "    def load_model(self, version=None): \n",
    "        if version is None:\n",
    "            self.discriminator = load_model('gan/models/toy-mix-label-discriminator.h5')\n",
    "            self.generator = load_model('gan/models/toy-mix-label-generator.h5')\n",
    "            self.combined = load_model('gan/models/toy-mix-label-combined.h5')\n",
    "        else:\n",
    "            self.discriminator = load_model('gan/models/toy-mix-label-discriminator-{}.h5'.format(version))\n",
    "            self.generator = load_model('gan/models/toy-mix-label-generator-{}.h5'.format(version))\n",
    "            self.combined = load_model('gan/models/toy-mix-label-combined-{}.h5'.format(version))\n",
    "            \n",
    "        \n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_shape=self.data_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(6))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        # Discriminator takes an image as an input and outputs its validity\n",
    "        data = Input(shape=self.data_shape)\n",
    "        validity = model(data)\n",
    "\n",
    "        return Model(data, validity)\n",
    "    \n",
    "        \n",
    "    def build_generator(self):\n",
    "        # BatchNormalization maintains the mean activation close to 0\n",
    "        # and the activation standard deviation close to 1\n",
    "        noise = Input(shape=self.noise_shape)\n",
    "        \n",
    "        hidden_1 = Dense(18)(noise)\n",
    "        hidden_1 = LeakyReLU(alpha=0.2)(hidden_1)\n",
    "        hidden_1 = BatchNormalization(momentum=0.8)(hidden_1)\n",
    "        \n",
    "        hidden_2 = Dense(16)(hidden_1)\n",
    "        hidden_2 = LeakyReLU(alpha=0.2)(hidden_2)\n",
    "        hidden_2 = BatchNormalization(momentum=0.8)(hidden_2)\n",
    "            \n",
    "        # Branch 1 - Categorical (5)\n",
    "        branch_1_hidden_1 = Dense(12)(hidden_2)\n",
    "        branch_1_hidden_1 = LeakyReLU(alpha=0.2)(branch_1_hidden_1)\n",
    "        branch_1_hidden_1 = BatchNormalization(momentum=0.8)(branch_1_hidden_1)\n",
    "        \n",
    "        branch_1_hidden_2 = Dense(7)(branch_1_hidden_1)\n",
    "        branch_1_hidden_2 = LeakyReLU(alpha=0.2)(branch_1_hidden_2)\n",
    "        branch_1_hidden_2 = BatchNormalization(momentum=0.8)(branch_1_hidden_2)\n",
    "        \n",
    "        branch_1_output = Dense(5, activation='softmax')(branch_1_hidden_2)\n",
    "        \n",
    "        # Branch 2 - Categorical (2)\n",
    "        branch_2_hidden_1 = Dense(8)(hidden_2)\n",
    "        branch_2_hidden_1 = LeakyReLU(alpha=0.2)(branch_2_hidden_1)\n",
    "        branch_2_hidden_1 = BatchNormalization(momentum=0.8)(branch_2_hidden_1)\n",
    "\n",
    "        branch_2_hidden_2 = Dense(4)(branch_2_hidden_1)\n",
    "        branch_2_hidden_2 = LeakyReLU(alpha=0.2)(branch_2_hidden_2)\n",
    "        branch_2_hidden_2 = BatchNormalization(momentum=0.8)(branch_2_hidden_2)\n",
    "        \n",
    "        branch_2_output = Dense(2, activation='softmax')(branch_2_hidden_2)\n",
    "        \n",
    "        # Branch 3 - Numerical\n",
    "        branch_3_hidden_1 = Dense(4)(hidden_2)\n",
    "        branch_3_hidden_1 = LeakyReLU(alpha=0.2)(branch_3_hidden_1)\n",
    "        branch_3_hidden_1 = BatchNormalization(momentum=0.8)(branch_3_hidden_1)\n",
    "        \n",
    "        branch_3_output = Dense(1)(branch_3_hidden_1)\n",
    "        \n",
    "        # Branch 4 - Numerical\n",
    "        branch_4_hidden_1 = Dense(4)(hidden_2)\n",
    "        branch_4_hidden_1 = LeakyReLU(alpha=0.2)(branch_4_hidden_1)\n",
    "        branch_4_hidden_1 = BatchNormalization(momentum=0.8)(branch_4_hidden_1)\n",
    "        \n",
    "        branch_4_output = Dense(1)(branch_4_hidden_1)\n",
    "        \n",
    "        # Branch 5 - Label (3)\n",
    "        branch_5_hidden_1 = Dense(10)(hidden_2)\n",
    "        branch_5_hidden_1 = LeakyReLU(alpha=0.2)(branch_5_hidden_1)\n",
    "        branch_5_hidden_1 = BatchNormalization(momentum=0.8)(branch_5_hidden_1)\n",
    "        \n",
    "        branch_5_hidden_2 = Dense(6)(branch_5_hidden_1)\n",
    "        branch_5_hidden_2 = LeakyReLU(alpha=0.2)(branch_5_hidden_2)\n",
    "        branch_5_hidden_2 = BatchNormalization(momentum=0.8)(branch_5_hidden_2)\n",
    "        \n",
    "        branch_5_output = Dense(3, activation='softmax')(branch_5_hidden_2)\n",
    "        \n",
    "        merged_output = concatenate([branch_1_output, branch_2_output, branch_3_output, branch_4_output, branch_5_output])\n",
    "        \n",
    "        return Model(inputs=noise, outputs=merged_output)\n",
    "    \n",
    "    \n",
    "    def train(self, train_data, epochs=50001, batch_size=128, save_model_interval=10000):\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # -------------------\n",
    "            # Train Discriminator\n",
    "            # -------------------\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, train_data.shape[0], half_batch)\n",
    "            data = train_data[idx]\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            noise = np.random.normal(0, 1, (half_batch, 20))\n",
    "            gen_data = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(data, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_data, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------\n",
    "            # Train Generator\n",
    "            # ---------------\n",
    "            noise = np.random.normal(0, 1, (batch_size, 20))\n",
    "\n",
    "            # The generator wants to fool the discriminator, hence trained with valid label (1)\n",
    "            # valid_y = np.array([1] * batch_size)\n",
    "            valid_y = np.ones((batch_size, 1))\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "\n",
    "            # Print progress\n",
    "            print (\"{:5d} [D loss: {}, acc_real: {:2f}, acc_fake: {:2f}] [G loss: {}]\".format(epoch, d_loss[0], 100*d_loss_real[1], 100*d_loss_fake[1], g_loss))\n",
    "\n",
    "            with open('gan/logs/toy-mix-label-gan.log', 'a') as log_file:\n",
    "                log_file.write('{},{}\\n'.format(d_loss[0], g_loss))\n",
    "            \n",
    "            # Save models at save_interval\n",
    "            if epoch != 0 and epoch % save_model_interval == 0:\n",
    "                self.save_model(version=str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_72 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 241\n",
      "Trainable params: 241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_train = load_data('mix-label-train-x')\n",
    "y_train = load_data('mix-label-train-y')\n",
    "\n",
    "train_data = np.concatenate((x_train, y_train), axis=1)\n",
    "\n",
    "gan = GANMixWithLabel()\n",
    "# gan.train(train_data, epochs=100001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating New Mixed Data with GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_143 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)   (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 241\n",
      "Trainable params: 241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rionaldichandraseta/miniconda3/envs/gan-keras/lib/python3.6/site-packages/keras/engine/saving.py:304: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "gan_mix = GANMixWithLabel()\n",
    "gan_mix.load_model('v1')\n",
    "\n",
    "noise = np.random.normal(0, 1, (1000, 20))\n",
    "\n",
    "generated_data = gan_mix.generator.predict(noise)\n",
    "# save_data(generated_data, file_name='mix-label-generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = load_data('mix-label-generated')\n",
    "\n",
    "categorical_1, categorical_2, numerical_1, numerical_2, label = np.split(generated_data, [5, 7, 8, 9], axis=1)\n",
    "\n",
    "categorical_1 = np.round(categorical_1)\n",
    "categorical_2 = np.round(categorical_2)\n",
    "label = np.round(label)\n",
    "\n",
    "categorical_1 = categorical_1.astype(int)\n",
    "categorical_2 = categorical_2.astype(int)\n",
    "label = label.astype(int)\n",
    "\n",
    "generated_data_normalized = np.concatenate((categorical_1, categorical_2, numerical_1, numerical_2, label), axis=1)\n",
    "\n",
    "# save_data(generated_data_normalized, file_name='mix-label-generated-normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training New Classifier with GAN-Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_301 (Dense)            (None, 128)               1280      \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_302 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_303 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_304 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_305 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 12,195\n",
      "Trainable params: 12,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "org_x_train = load_data('mix-label-train-x')\n",
    "org_y_train = load_data('mix-label-train-y')\n",
    "\n",
    "generated_data = load_data('mix-label-generated-normalized')\n",
    "\n",
    "gen_x_train, gen_y_train = np.split(generated_data, [9], axis=1)\n",
    "\n",
    "x_train = np.concatenate((org_x_train, gen_x_train), axis=0)\n",
    "y_train = np.concatenate((org_y_train, gen_y_train), axis=0)\n",
    "\n",
    "classifier_mix = ClassifierMix()\n",
    "# classifier_mix.train(x_train, y_train, epoch_limit=1000, version='with-gan-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Classifier with and without GAN-generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_306 (Dense)            (None, 128)               1280      \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_307 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_308 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_309 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_310 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 12,195\n",
      "Trainable params: 12,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5000/5000 [==============================] - 7s 1ms/step\n",
      "Test Loss = 0.2712072402238846, Test Accuracy = 0.8832\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_311 (Dense)            (None, 128)               1280      \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_312 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_313 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_314 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_315 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 12,195\n",
      "Trainable params: 12,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5000/5000 [==============================] - 7s 1ms/step\n",
      "Test Loss = 0.2898571259021759, Test Accuracy = 0.8836\n"
     ]
    }
   ],
   "source": [
    "x_test = load_data('mix-label-test-x')\n",
    "y_test = load_data('mix-label-test-y')\n",
    "\n",
    "classifier = ClassifierMix()\n",
    "classifier.load_model('v1')\n",
    "classifier.evaluate(x_test, y_test)\n",
    "\n",
    "new_classifier = ClassifierMix()\n",
    "new_classifier.load_model('with-gan-v1')\n",
    "new_classifier.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the model increased from 88.32% to 88.36%, an increase of 2 more correct samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN for Mixed Data without Generating Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMix():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_shape = (9,)\n",
    "            # Categorical 5\n",
    "            # Categorical 2\n",
    "            # Numerical 1\n",
    "            # Numerical 1\n",
    "        self.noise_shape = (20,)\n",
    "\n",
    "        # Manually tune down learning rate to avoid oscillation\n",
    "        optimizer = Adam(lr=0.005, beta_1=0.5)\n",
    "\n",
    "        # -------------\n",
    "        # Discriminator\n",
    "        # -------------\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mean_squared_error',\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "        \n",
    "        # ---------\n",
    "        # Generator\n",
    "        # ---------\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='mean_squared_error',\n",
    "                               optimizer=optimizer)\n",
    "        \n",
    "\n",
    "        # --------\n",
    "        # Combined\n",
    "        # --------\n",
    "        # The combined model is created by stacking generator and discriminator.\n",
    "        # Noise ---Generator--> Generated Data ---Discriminator--> Validity\n",
    "\n",
    "        # From Generator\n",
    "        z = Input(shape=self.noise_shape)\n",
    "        data = self.generator(z)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        validity = self.discriminator(data)\n",
    "\n",
    "        self.combined = Model(inputs=z, outputs=validity)\n",
    "        self.combined.compile(loss='mean_squared_error',\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "        \n",
    "    def save_model(self, version=None):\n",
    "        if version is None:\n",
    "            self.discriminator.save('gan/models/toy-mix-discriminator.h5')\n",
    "            self.generator.save('gan/models/toy-mix-generator.h5')\n",
    "            self.combined.save('gan/models/toy-mix-combined.h5')\n",
    "        else:\n",
    "            self.discriminator.save('gan/models/toy-mix-discriminator-{}.h5'.format(version))\n",
    "            self.generator.save('gan/models/toy-mix-generator-{}.h5'.format(version))\n",
    "            self.combined.save('gan/models/toy-mix-combined-{}.h5'.format(version))\n",
    "        \n",
    "        \n",
    "    def load_model(self, version=None): \n",
    "        if version is None:\n",
    "            self.discriminator = load_model('gan/models/toy-mix-discriminator.h5')\n",
    "            self.generator = load_model('gan/models/toy-mix-generator.h5')\n",
    "            self.combined = load_model('gan/models/toy-mix-combined.h5')\n",
    "        else:\n",
    "            self.discriminator = load_model('gan/models/toy-mix-discriminator-{}.h5'.format(version))\n",
    "            self.generator = load_model('gan/models/toy-mix-generator-{}.h5'.format(version))\n",
    "            self.combined = load_model('gan/models/toy-mix-combined-{}.h5'.format(version))\n",
    "            \n",
    "        \n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_shape=self.data_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(6))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        # Discriminator takes an image as an input and outputs its validity\n",
    "        data = Input(shape=self.data_shape)\n",
    "        validity = model(data)\n",
    "\n",
    "        return Model(data, validity)\n",
    "    \n",
    "        \n",
    "    def build_generator(self):\n",
    "        # BatchNormalization maintains the mean activation close to 0\n",
    "        # and the activation standard deviation close to 1\n",
    "        noise = Input(shape=self.noise_shape)\n",
    "        \n",
    "        hidden_1 = Dense(18)(noise)\n",
    "        hidden_1 = LeakyReLU(alpha=0.2)(hidden_1)\n",
    "        hidden_1 = BatchNormalization(momentum=0.8)(hidden_1)\n",
    "        \n",
    "        hidden_2 = Dense(16)(hidden_1)\n",
    "        hidden_2 = LeakyReLU(alpha=0.2)(hidden_2)\n",
    "        hidden_2 = BatchNormalization(momentum=0.8)(hidden_2)\n",
    "            \n",
    "        # Branch 1 - Categorical (5)\n",
    "        branch_1_hidden_1 = Dense(12)(hidden_2)\n",
    "        branch_1_hidden_1 = LeakyReLU(alpha=0.2)(branch_1_hidden_1)\n",
    "        branch_1_hidden_1 = BatchNormalization(momentum=0.8)(branch_1_hidden_1)\n",
    "        \n",
    "        branch_1_hidden_2 = Dense(7)(branch_1_hidden_1)\n",
    "        branch_1_hidden_2 = LeakyReLU(alpha=0.2)(branch_1_hidden_2)\n",
    "        branch_1_hidden_2 = BatchNormalization(momentum=0.8)(branch_1_hidden_2)\n",
    "        \n",
    "        branch_1_output = Dense(5, activation='softmax')(branch_1_hidden_2)\n",
    "        \n",
    "        # Branch 2 - Categorical (2)\n",
    "        branch_2_hidden_1 = Dense(8)(hidden_2)\n",
    "        branch_2_hidden_1 = LeakyReLU(alpha=0.2)(branch_2_hidden_1)\n",
    "        branch_2_hidden_1 = BatchNormalization(momentum=0.8)(branch_2_hidden_1)\n",
    "\n",
    "        branch_2_hidden_2 = Dense(4)(branch_2_hidden_1)\n",
    "        branch_2_hidden_2 = LeakyReLU(alpha=0.2)(branch_2_hidden_2)\n",
    "        branch_2_hidden_2 = BatchNormalization(momentum=0.8)(branch_2_hidden_2)\n",
    "        \n",
    "        branch_2_output = Dense(2, activation='softmax')(branch_2_hidden_2)\n",
    "        \n",
    "        # Branch 3 - Numerical\n",
    "        branch_3_hidden_1 = Dense(4)(hidden_2)\n",
    "        branch_3_hidden_1 = LeakyReLU(alpha=0.2)(branch_3_hidden_1)\n",
    "        branch_3_hidden_1 = BatchNormalization(momentum=0.8)(branch_3_hidden_1)\n",
    "        \n",
    "        branch_3_output = Dense(1)(branch_3_hidden_1)\n",
    "        \n",
    "        # Branch 4 - Numerical\n",
    "        branch_4_hidden_1 = Dense(4)(hidden_2)\n",
    "        branch_4_hidden_1 = LeakyReLU(alpha=0.2)(branch_4_hidden_1)\n",
    "        branch_4_hidden_1 = BatchNormalization(momentum=0.8)(branch_4_hidden_1)\n",
    "        \n",
    "        branch_4_output = Dense(1)(branch_4_hidden_1)\n",
    "        \n",
    "        merged_output = concatenate([branch_1_output, branch_2_output, branch_3_output, branch_4_output])\n",
    "        \n",
    "        return Model(inputs=noise, outputs=merged_output)\n",
    "    \n",
    "    \n",
    "    def train(self, train_data, epochs=50001, batch_size=128, save_model_interval=10000):\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # -------------------\n",
    "            # Train Discriminator\n",
    "            # -------------------\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, train_data.shape[0], half_batch)\n",
    "            data = train_data[idx]\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            noise = np.random.normal(0, 1, (half_batch, 20))\n",
    "            gen_data = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(data, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_data, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------\n",
    "            # Train Generator\n",
    "            # ---------------\n",
    "            noise = np.random.normal(0, 1, (batch_size, 20))\n",
    "\n",
    "            # The generator wants to fool the discriminator, hence trained with valid label (1)\n",
    "            # valid_y = np.array([1] * batch_size)\n",
    "            valid_y = np.ones((batch_size, 1))\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "\n",
    "            # Print progress\n",
    "            print (\"{:5d} [D loss: {}, acc_real: {:2f}, acc_fake: {:2f}] [G loss: {}]\".format(epoch, d_loss[0], 100*d_loss_real[1], 100*d_loss_fake[1], g_loss))\n",
    "\n",
    "            with open('gan/logs/toy-mix-gan.log', 'a') as log_file:\n",
    "                log_file.write('{},{}\\n'.format(d_loss[0], g_loss))\n",
    "            \n",
    "            # Save models at save_interval\n",
    "            if epoch != 0 and epoch % save_model_interval == 0:\n",
    "                self.save_model(version=str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_191 (Dense)            (None, 12)                120       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_99 (LeakyReLU)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_100 (LeakyReLU)  (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 205\n",
      "Trainable params: 205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_train = load_data('mix-label-train-x')\n",
    "\n",
    "gan = GANMix()\n",
    "# gan.train(x_train, epochs=100001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating New Mixed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_261 (Dense)            (None, 12)                120       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_139 (LeakyReLU)  (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_140 (LeakyReLU)  (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_263 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 205\n",
      "Trainable params: 205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rionaldichandraseta/miniconda3/envs/gan-keras/lib/python3.6/site-packages/keras/engine/saving.py:304: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to gan/gen-data/mix-generated\n"
     ]
    }
   ],
   "source": [
    "gan = GANMix()\n",
    "gan.load_model('v1')\n",
    "\n",
    "noise = np.random.normal(0, 1, (3000, 20))\n",
    "\n",
    "generated_data = gan.generator.predict(noise)\n",
    "# save_data(generated_data, file_name='mix-generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to gan/gen-data/mix-generated-normalized\n"
     ]
    }
   ],
   "source": [
    "generated_data = load_data('mix-generated')\n",
    "\n",
    "categorical_1, categorical_2, numerical_1, numerical_2 = np.split(generated_data, [5, 7, 8], axis=1)\n",
    "\n",
    "categorical_1 = np.round(categorical_1)\n",
    "categorical_2 = np.round(categorical_2)\n",
    "\n",
    "categorical_1 = categorical_1.astype(int)\n",
    "categorical_2 = categorical_2.astype(int)\n",
    "\n",
    "generated_data_normalized = np.concatenate((categorical_1, categorical_2, numerical_1, numerical_2), axis=1)\n",
    "# save_data(generated_data_normalized, file_name='mix-generated-normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Labelling with Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_281 (Dense)            (None, 128)               1280      \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_282 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_283 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_284 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_285 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 12,195\n",
      "Trainable params: 12,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Data successfully saved to gan/gen-data/mix-generated-prediction\n"
     ]
    }
   ],
   "source": [
    "x_train = load_data('mix-generated-normalized')\n",
    "\n",
    "classifier = ClassifierMix()\n",
    "classifier.load_model('v1')\n",
    "\n",
    "predictions = classifier.classifier.predict(x_train)\n",
    "\n",
    "# save_data(predictions, 'mix-generated-prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data with Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 1620 generated data that falls below the threshold of 0.999\n",
      "Label distribution for data above threshold: [499. 428. 453.]\n",
      "Filtered data label distribution: [333, 334, 333]\n",
      "Data successfully saved to gan/gen-data/mix-generated-x-above-threshold\n",
      "Data successfully saved to gan/gen-data/mix-generated-y-above-threshold\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.999\n",
    "\n",
    "generated_data = load_data('mix-generated-normalized')\n",
    "predictions = load_data('mix-generated-prediction')\n",
    "predictions_dummy = np.where(predictions > threshold, 1, 0)\n",
    "\n",
    "n_class = 3\n",
    "\n",
    "below_threshold_indices = []\n",
    "below_threshold_count = 0\n",
    "for i in range(len(predictions_dummy)):\n",
    "    all_zero = True\n",
    "    for j in range(n_class):\n",
    "        if all_zero:\n",
    "            all_zero = predictions_clone[i][j] == 0\n",
    "        else:\n",
    "            break\n",
    "    if all_zero:\n",
    "        below_threshold_indices.append(i)\n",
    "        below_threshold_count += 1\n",
    "\n",
    "print('Detected {} generated data that falls below the threshold of {}'.format(below_threshold_count, threshold))\n",
    "\n",
    "above_threshold_indices = []\n",
    "above_threshold_count = np.zeros(n_class)\n",
    "for idx, prediction in enumerate(predictions):\n",
    "    if idx not in below_threshold_indices:\n",
    "        above_threshold_indices.append(idx)\n",
    "        above_threshold_count[np.argmax(prediction)] += 1\n",
    "        \n",
    "print('Label distribution for data above threshold: {}'.format(above_threshold_count))\n",
    "\n",
    "data_limit = [333, 334, 333]\n",
    "\n",
    "filtered_generated_data = np.empty((1000,9))\n",
    "normalized_label = np.empty((1000,3))\n",
    "\n",
    "count = np.zeros(n_class)\n",
    "it = 0\n",
    "for idx in above_threshold_indices:\n",
    "    data_label = np.argmax(predictions[idx])\n",
    "    if count[data_label] >= data_limit[data_label]:\n",
    "        pass\n",
    "    else:\n",
    "        filtered_generated_data[it] = generated_data[idx]\n",
    "        normalized_label[it] = predictions[idx]\n",
    "        count[data_label] += 1\n",
    "        it += 1\n",
    "\n",
    "normalized_label = np.round(normalized_label)\n",
    "\n",
    "\n",
    "# Check labelled data distribution\n",
    "\n",
    "count = [0,0,0]\n",
    "for y in normalized_label:\n",
    "    for j in range(3):\n",
    "        if y[j] == 1:\n",
    "            count[j] += 1\n",
    "\n",
    "print('Filtered data label distribution: {}'.format(count))\n",
    "\n",
    "# save_data(filtered_generated_data, 'mix-generated-x-above-threshold')\n",
    "# save_data(normalized_label, 'mix-generated-y-above-threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training New Classifier with GAN Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_316 (Dense)            (None, 128)               1280      \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_317 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_318 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_319 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_320 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 12,195\n",
      "Trainable params: 12,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "org_x_train = load_data('mix-label-train-x')\n",
    "org_y_train = load_data('mix-label-train-y')\n",
    "\n",
    "gen_x_train = load_data('mix-generated-x-above-threshold')\n",
    "gen_y_train = load_data('mix-generated-y-above-threshold')\n",
    "\n",
    "x_train = np.concatenate((org_x_train, gen_x_train), axis=0)\n",
    "y_train = np.concatenate((org_y_train, gen_y_train), axis=0)\n",
    "\n",
    "classifier_mix = ClassifierMix()\n",
    "# classifier_mix.train(x_train, y_train, epoch_limit=1000, version='with-gan-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Classifier with and without GAN-generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_291 (Dense)            (None, 128)               1280      \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_292 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_293 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_294 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_295 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 12,195\n",
      "Trainable params: 12,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5000/5000 [==============================] - 6s 1ms/step\n",
      "Test Loss = 0.2712072402238846, Test Accuracy = 0.8832\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_296 (Dense)            (None, 128)               1280      \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_297 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_298 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_299 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_300 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 12,195\n",
      "Trainable params: 12,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5000/5000 [==============================] - 6s 1ms/step\n",
      "Test Loss = 0.274448974275589, Test Accuracy = 0.8866\n"
     ]
    }
   ],
   "source": [
    "x_test = load_data('mix-label-test-x')\n",
    "y_test = load_data('mix-label-test-y')\n",
    "\n",
    "classifier = ClassifierMix()\n",
    "classifier.load_model('v1')\n",
    "classifier.evaluate(x_test, y_test)\n",
    "\n",
    "new_classifier = ClassifierMix()\n",
    "new_classifier.load_model('with-gan-v2')\n",
    "new_classifier.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the model increased from 88.32% to 88.66%, an increase of 12 more correct samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
